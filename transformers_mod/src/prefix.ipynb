{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from transformers.models.t5 import T5Tokenizer, T5ForConditionalGeneration\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from rouge_score import rouge_scorer\n",
    "import pickle\n",
    "import random\n",
    "random.seed(42)\n",
    "import wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet(Dataset):\n",
    "  \"\"\"\n",
    "  Creating a custom dataset for reading the dataset and \n",
    "  loading it into the dataloader to pass it to the neural network for finetuning the model\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, dataframe, tokenizer, source_len, target_len, source_text, target_text):\n",
    "    self.tokenizer = tokenizer\n",
    "    self.data = dataframe\n",
    "    self.source_len = source_len\n",
    "    self.summ_len = target_len\n",
    "    self.target_text = self.data[target_text]\n",
    "    self.source_text = self.data[source_text]\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.target_text)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    source_text = str(self.source_text[index])\n",
    "    target_text = str(self.target_text[index])\n",
    "\n",
    "    #cleaning data so as to ensure data is in string type\n",
    "    source_text = ' '.join(source_text.split())\n",
    "    target_text = ' '.join(target_text.split())\n",
    "\n",
    "    source = self.tokenizer.batch_encode_plus([source_text], max_length= self.source_len, pad_to_max_length=True, truncation=True, padding=\"max_length\", return_tensors='pt')\n",
    "    target = self.tokenizer.batch_encode_plus([target_text], max_length= self.summ_len, pad_to_max_length=True, truncation=True, padding=\"max_length\", return_tensors='pt')\n",
    "\n",
    "    source_ids = source['input_ids'].squeeze()\n",
    "    source_mask = source['attention_mask'].squeeze()\n",
    "    source_mask = torch.ones(100 + self.source_len)  # UPDATE FOR PREFIX FINE TUNING\n",
    "    target_ids = target['input_ids'].squeeze()\n",
    "    target_mask = target['attention_mask'].squeeze()\n",
    "\n",
    "    return {\n",
    "        'source_ids': source_ids.to(dtype=torch.long), \n",
    "        'source_mask': source_mask.to(dtype=torch.long), \n",
    "        'target_ids': target_ids.to(dtype=torch.long),\n",
    "        'target_ids_y': target_ids.to(dtype=torch.long)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "test_pairs = load(open('/home/kjros2/cs546/project/2021cs546project/data/text_pairs.pkl', 'rb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ninput_ids = tokenizer.encode(\"hello world this is a neural test. Can you hear me?\", return_tensors=\\'pt\\').to(device)\\nattention_mask = torch.ones((1, input_ids.size()[1] + 50)).to(device)\\noutputs = model.generate(\\n    input_ids=input_ids,\\n    max_length=64,\\n    do_sample=True,\\n    top_k=10,\\n    attention_mask=attention_mask,\\n    num_return_sequences=3)\\n\\nfor i in range(3):\\n    print(f\\'sample {i + 1}: {tokenizer.decode(outputs[i], skip_special_tokens=True)}\\')\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "input_ids = tokenizer.encode(\"hello world this is a neural test. Can you hear me?\", return_tensors='pt').to(device)\n",
    "attention_mask = torch.ones((1, input_ids.size()[1] + 50)).to(device)\n",
    "outputs = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_length=64,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    attention_mask=attention_mask,\n",
    "    num_return_sequences=3)\n",
    "\n",
    "for i in range(3):\n",
    "    print(f'sample {i + 1}: {tokenizer.decode(outputs[i], skip_special_tokens=True)}')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
    "\n",
    "  \"\"\"\n",
    "  Function to be called for training with the parameters passed from main function\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  model.train()\n",
    "\n",
    "  for _,data in enumerate(loader, 0):\n",
    "    y = data['target_ids'].to(device, dtype = torch.long)\n",
    "    y_ids = y[:, :-1].contiguous()\n",
    "    lm_labels = y[:, 1:].clone().detach()\n",
    "    lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
    "    ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "    mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "    outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, labels=lm_labels)\n",
    "    loss = outputs[0]\n",
    "\n",
    "    if _%100==0:\n",
    "      print(str(epoch), str(_), str(loss))\n",
    "      wandb.log({'epoch': epoch, 'train_loss': float(loss)})\n",
    "      \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(epoch, fold_idx, tokenizer, model, device, loader):\n",
    "\n",
    "  \"\"\"\n",
    "  Function to be called for validation with the parameters passed from main function\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  model.eval()\n",
    "\n",
    "  with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    for _,data in enumerate(loader, 0):\n",
    "        y = data['target_ids'].to(device, dtype = torch.long)\n",
    "        y_ids = y[:, :-1].contiguous()\n",
    "        lm_labels = y[:, 1:].clone().detach()\n",
    "        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
    "        ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "        mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, labels=lm_labels)\n",
    "        total_loss += float(outputs[0])\n",
    "    total_loss /= len(loader)\n",
    "    print({'epoch': epoch, 'fold_idx': fold_idx, 'val_loss': total_loss})\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(tokenizer, model, device, loader):\n",
    "\n",
    "  \"\"\"\n",
    "  Function to evaluate model for predictions\n",
    "\n",
    "  \"\"\"\n",
    "  model.eval()\n",
    "  \n",
    "  predictions = []\n",
    "  actuals = []\n",
    "  with torch.no_grad():\n",
    "      for _, data in enumerate(loader, 0):\n",
    "          y = data['target_ids'].to(device, dtype = torch.long)\n",
    "          ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "          mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "          generated_ids = model.generate(\n",
    "              input_ids = ids,\n",
    "              attention_mask = mask, \n",
    "              max_length=150, \n",
    "              num_beams=2,\n",
    "              repetition_penalty=2.5, \n",
    "              length_penalty=1.0, \n",
    "              early_stopping=True\n",
    "              )\n",
    "          preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "          target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
    "\n",
    "          predictions.extend(preds)\n",
    "          actuals.extend(target)\n",
    "  return predictions, actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(preds, acts, typ):\n",
    "  scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=False)\n",
    "  \n",
    "  rouge1 = {'precision': [], 'recall': [], 'fmeasure': []}\n",
    "  rouge2 = {'precision': [], 'recall': [], 'fmeasure': []}\n",
    "  rougeL = {'precision': [], 'recall': [], 'fmeasure': []}\n",
    "\n",
    "\n",
    "  for pred,act in zip(preds, acts):\n",
    "      scores = scorer.score(act, pred)\n",
    "      r1_pre, r1_re, r1_fm = scores['rouge1']\n",
    "      r2_pre, r2_re, r2_fm = scores['rouge2']\n",
    "      rL_pre, rL_re, rL_fm = scores['rougeL']\n",
    "      rouge1['precision'].append(r1_pre)\n",
    "      rouge1['recall'].append(r1_re)\n",
    "      rouge1['fmeasure'].append(r1_fm)\n",
    "      rouge2['precision'].append(r2_pre)\n",
    "      rouge2['recall'].append(r2_re)\n",
    "      rouge2['fmeasure'].append(r2_fm)\n",
    "      rougeL['precision'].append(rL_pre)\n",
    "      rougeL['recall'].append(rL_re)\n",
    "      rougeL['fmeasure'].append(rL_fm)\n",
    "  # for sig testing, uncomment for running\n",
    "  #return rouge1, rouge2, rougeL\n",
    "\n",
    "  num_samples = len(preds)\n",
    "  for key in rouge1:\n",
    "      rouge1[key] = sum(rouge1[key]) /  num_samples\n",
    "      #wandb.log({typ + '_rouge1_' + key: rouge1[key]})\n",
    "      #print({'rouge1_' + key: rouge1[key]})\n",
    "  for key in rouge2:\n",
    "      rouge2[key] = sum(rouge2[key]) / num_samples\n",
    "      #wandb.log({typ + '_rouge2_' + key: rouge2[key]})\n",
    "      #print({'rouge2_' + key: rouge2[key]})\n",
    "  for key in rougeL:\n",
    "      rougeL[key] = sum(rougeL[key]) / num_samples\n",
    "      #wandb.log({typ + '_rougeL_' + key: rougeL[key]})\n",
    "      #print({'rougeL_' + key: rougeL[key]})\n",
    "\n",
    "  return rouge1, rouge2, rougeL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def T5Trainer(test_pairs, val_idx, source_text, target_text, model_params, output_dir=\"./outputs/\", mode=\"tune\" ):\n",
    "  \n",
    "  \"\"\"\n",
    "  T5 trainer\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  # Set random seeds and deterministic pytorch for reproducibility\n",
    "  torch.manual_seed(model_params[\"SEED\"]) # pytorch random seed\n",
    "  np.random.seed(model_params[\"SEED\"]) # numpy random seed\n",
    "  torch.backends.cudnn.deterministic = True\n",
    "\n",
    "  # logging\n",
    "  print(f\"\"\"[Model]: Loading {model_params[\"MODEL\"]}...\\n\"\"\")\n",
    "\n",
    "  # tokenzier for encoding the text\n",
    "  tokenizer = T5Tokenizer.from_pretrained(model_params[\"MODEL\"])\n",
    "\n",
    "  # Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary. \n",
    "  # Further this model is sent to device (GPU/TPU) for using the hardware.\n",
    "  model = T5ForConditionalGeneration.from_pretrained(model_params[\"MODEL\"])\n",
    "  model = model.to(device)\n",
    "\n",
    "  # for prefix tuning (comment out for regular fine tuning)\n",
    "  model._modules.requires_grad=False\n",
    "  model._modules['encoder'].embedding.requires_grad=True\n",
    "  model._modules['encoder'].linear1.requires_grad=True\n",
    "  model._modules['encoder'].linear2.requires_grad=True\n",
    "  #model._modules['encoder'].linear3.requires_grad=True\n",
    "  \n",
    "  # Defining the optimizer that will be used to tune the weights of the network in the training session. \n",
    "  optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                               lr=model_params[\"LEARNING_RATE\"])\n",
    "\n",
    "  \n",
    "  # logging\n",
    "  print(f\"[Data]: Reading data...\\n\")\n",
    "\n",
    "  shuff_lec_names = list(test_pairs.keys())\n",
    "  random.shuffle(shuff_lec_names)\n",
    "\n",
    "\n",
    "  # perform cross validation with a test set of the last 5\n",
    "  test_questions = []\n",
    "  test_lecture_text = []\n",
    "  for lecture in shuff_lec_names[85:]:\n",
    "    for question_unit in test_pairs[lecture]:\n",
    "        test_questions.append(question_unit['text'])\n",
    "        test_lecture_text.append(question_unit['lecture_text'])\n",
    "  test_dataset = pd.DataFrame({'lecture': test_lecture_text, 'question': test_questions})\n",
    "\n",
    "  val_questions = []\n",
    "  val_lecture_text = []\n",
    "  for lecture in shuff_lec_names[val_idx:val_idx+10]:\n",
    "    for question_unit in test_pairs[lecture]:\n",
    "        val_questions.append(question_unit['text'])\n",
    "        val_lecture_text.append(question_unit['lecture_text'])\n",
    "  val_dataset = pd.DataFrame({'lecture': val_lecture_text, 'question': val_questions})\n",
    "\n",
    "  train_questions = []\n",
    "  train_lecture_text = []\n",
    "  for lecture in shuff_lec_names[0:val_idx] + shuff_lec_names[val_idx+10:85]:\n",
    "    for question_unit in test_pairs[lecture]:\n",
    "        train_questions.append(question_unit['text'])\n",
    "        train_lecture_text.append(question_unit['lecture_text'])\n",
    "  train_dataset = pd.DataFrame({'lecture': train_lecture_text, 'question': train_questions})\n",
    "\n",
    "  if mode == \"test\":\n",
    "      # merge val and train data sets\n",
    "      train_questions = train_questions + val_questions\n",
    "      train_lecture_text = train_lecture_text + val_lecture_text\n",
    "      train_dataset = pd.DataFrame({'lecture': train_lecture_text, 'question': train_questions})\n",
    "\n",
    "  #print(f\"FULL Dataset: {dataframe.shape}\")\n",
    "  print(f\"TRAIN Dataset: {train_dataset.shape}\")\n",
    "  print(f\"VAL Dataset: {val_dataset.shape}\")\n",
    "  print(f\"TEST Dataset: {test_dataset.shape}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "  # Creating the Training and Validation dataset for further creation of Dataloader\n",
    "  training_set = DataSet(train_dataset, tokenizer, model_params[\"MAX_SOURCE_TEXT_LENGTH\"], model_params[\"MAX_TARGET_TEXT_LENGTH\"], source_text, target_text)\n",
    "  val_set = DataSet(val_dataset, tokenizer, model_params[\"MAX_SOURCE_TEXT_LENGTH\"], model_params[\"MAX_TARGET_TEXT_LENGTH\"], source_text, target_text)\n",
    "  test_set = DataSet(test_dataset, tokenizer, model_params[\"MAX_SOURCE_TEXT_LENGTH\"], model_params[\"MAX_TARGET_TEXT_LENGTH\"], source_text, target_text)\n",
    "\n",
    "\n",
    "  # Defining the parameters for creation of dataloaders\n",
    "  train_params = {\n",
    "      'batch_size': model_params[\"TRAIN_BATCH_SIZE\"],\n",
    "      'shuffle': True,\n",
    "      'num_workers': 0\n",
    "      }\n",
    "\n",
    "\n",
    "  val_params = {\n",
    "      'batch_size': model_params[\"VALID_BATCH_SIZE\"],\n",
    "      'shuffle': False,\n",
    "      'num_workers': 0\n",
    "      }\n",
    "\n",
    "  test_params = {\n",
    "      'batch_size': model_params[\"TEST_BATCH_SIZE\"],\n",
    "      'shuffle': False,\n",
    "      'num_workers': 0\n",
    "      }\n",
    "\n",
    "\n",
    "  # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n",
    "  training_loader = DataLoader(training_set, **train_params)\n",
    "  val_loader = DataLoader(val_set, **val_params)\n",
    "  test_loader = DataLoader(test_set, **test_params)\n",
    "\n",
    "\n",
    "\n",
    "  # Training loop\n",
    "  print(f'[Initiating Fine Tuning]...\\n')\n",
    "\n",
    "  prev_loss = []\n",
    "  prev_rouge1f = []\n",
    "  for epoch in range(model_params[\"TRAIN_EPOCHS\"]):\n",
    "      train(epoch, tokenizer, model, device, training_loader, optimizer)\n",
    "      if mode == \"test\":\n",
    "          continue\n",
    "      loss = val(epoch, val_idx, tokenizer, model, device, val_loader)\n",
    "      preds, acts = test(tokenizer, model, device, val_loader)\n",
    "      wandb.log({'epoch': epoch, 'val_loss': loss})\n",
    "      rouge1_f = score(preds, acts, 'val')\n",
    "\n",
    "      prev_loss.append(loss)\n",
    "      prev_rouge1f.append(rouge1_f)\n",
    "\n",
    "      #if len(prev_loss) > 2 and prev_loss[-3] < prev_loss[-2] and prev_loss[-2] < prev_loss[-1]:\n",
    "      #    break\n",
    "      if len(prev_rouge1f) > 2 and prev_rouge1f[-1] < prev_rouge1f[-2] and prev_rouge1f[-2] < prev_rouge1f[-3]:\n",
    "          break\n",
    "\n",
    "  \n",
    "  \n",
    "  if mode == \"test\":\n",
    "      preds, acts = test(tokenizer, model, device, test_loader)\n",
    "      score(preds, acts, 'test')\n",
    "      pickle.dump(preds, open('preds.pkl', 'wb'))\n",
    "      pickle.dump(acts, open('acts.pkl', 'wb'))\n",
    "  del model\n",
    "  return max(prev_rouge1f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# castorini/doc2query-t5-base-msmarco\n",
    "# t5-base \n",
    "mode = \"test\"\n",
    "model_params={\n",
    "    \"MODEL\":\"castorini/doc2query-t5-base-msmarco\",\n",
    "    \"TRAIN_BATCH_SIZE\":1,          # training batch size\n",
    "    \"VALID_BATCH_SIZE\":1,          # validation batch size\n",
    "    \"TEST_BATCH_SIZE\":1,           # validation batch size\n",
    "    \"TRAIN_EPOCHS\":7,              # number of training epochs\n",
    "    \"VAL_EPOCHS\":1,                # number of validation epochs\n",
    "    \"LEARNING_RATE\":1e-4,          # learning rate\n",
    "    \"MAX_SOURCE_TEXT_LENGTH\":512,  # max length of source text\n",
    "    \"MAX_TARGET_TEXT_LENGTH\":50,   # max length of target text\n",
    "    \"SEED\": 42,                    # set seed for reproducibility \n",
    "    \"type\": 'prefix with embedding layer (post review recovering morning-pond-100)',\n",
    "    \"mode\": mode,\n",
    "    \"input_dim\": \"768*20\",\n",
    "    \"hidden\": \"800\",\n",
    "    \"output_dim\": \"768*100\"\n",
    "}\n",
    "wandb.init(project=\"prefix-questions\", entity=\"kevinros\", config=model_params)\n",
    "if mode == \"tune\":\n",
    "    all_f1s = []\n",
    "    for fold_idx in [0,10,20]:\n",
    "        wandb.log({'fold_idx': fold_idx})\n",
    "        f1 = T5Trainer(test_pairs=test_pairs, val_idx=fold_idx,\n",
    "            source_text=\"lecture\", target_text=\"question\", model_params=model_params, output_dir=\"outputs\")\n",
    "        all_f1s.append(f1)\n",
    "    avg_f1 = sum(all_f1s) / 3\n",
    "    wandb.log({'selection_crit': avg_f1})\n",
    "elif mode == \"test\":\n",
    "    T5Trainer(test_pairs=test_pairs, val_idx=0,\n",
    "            source_text=\"lecture\", target_text=\"question\", model_params=model_params, output_dir=\"outputs\", mode=mode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "acts = pickle.load(open('runs/docTTTTTquery/acts.pkl', 'rb'))\n",
    "t5_base_ft_pred = pickle.load(open('runs/t5-base_ft/preds.pkl', 'rb'))\n",
    "t5_base_prefix_pred = pickle.load(open('runs/t5-base_prefix/preds.pkl', 'rb'))\n",
    "\n",
    "\n",
    "docTTTTTquery_pred = pickle.load(open('runs/docTTTTTquery/preds.pkl', 'rb'))\n",
    "docTTTTTquery_ft_pred = pickle.load(open('runs/docTTTTTquery_ft/preds.pkl', 'rb'))\n",
    "#docTTTTTquery_prefix_pred = pickle.load(open('runs/docTTTTTquery_prefix/preds.pkl', 'rb'))\n",
    "# old run was different than reported in paper, so had to rerun and update\n",
    "docTTTTTquery_prefix_pred = pickle.load(open('runs/docTTTTTquery_prefix_v2/preds.pkl', 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docTTTTTquery_prefix_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wilcoxon\n",
    "import seaborn as sb\n",
    "t5_base_ft_pred_scores = score(t5_base_ft_pred, acts, 'test')\n",
    "t5_base_prefix_pred_scores = score(t5_base_prefix_pred, acts, 'test')\n",
    "\n",
    "\n",
    "docTTTTTquery_pred_scores = score(docTTTTTquery_pred, acts, 'test')\n",
    "docTTTTTquery_ft_pred_scores = score(docTTTTTquery_ft_pred, acts, 'test')\n",
    "docTTTTTquery_prefix_pred_scores = score(docTTTTTquery_prefix_pred, acts, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rq1: is the precision of the t5 models lower than the docTTTTTquery models?\n",
    "\n",
    "# yes in t5-base FT to docTTTTTquery\n",
    "print(wilcoxon(t5_base_ft_pred_scores[0]['precision'], docTTTTTquery_pred_scores[0]['precision'], alternative='less'))\n",
    "print(wilcoxon(t5_base_ft_pred_scores[1]['precision'], docTTTTTquery_pred_scores[1]['precision'], alternative='less'))\n",
    "print(wilcoxon(t5_base_ft_pred_scores[2]['precision'], docTTTTTquery_pred_scores[2]['precision'], alternative='less'))\n",
    "print('=')\n",
    "\n",
    "\n",
    "# yes in t5-base FT to docTTTTTquery FT\n",
    "print(wilcoxon(t5_base_ft_pred_scores[0]['precision'], docTTTTTquery_ft_pred_scores[0]['precision'], alternative='less'))\n",
    "print(wilcoxon(t5_base_ft_pred_scores[1]['precision'], docTTTTTquery_ft_pred_scores[1]['precision'], alternative='less'))\n",
    "print(wilcoxon(t5_base_ft_pred_scores[2]['precision'], docTTTTTquery_ft_pred_scores[2]['precision'], alternative='less'))\n",
    "print('=')\n",
    "\n",
    "# ? in t5-base prefix to docTTTTTquery prefix\n",
    "print(wilcoxon(t5_base_prefix_pred_scores[0]['precision'], docTTTTTquery_prefix_pred_scores[0]['precision'], alternative='less'))\n",
    "print(wilcoxon(t5_base_prefix_pred_scores[1]['precision'], docTTTTTquery_prefix_pred_scores[1]['precision'], alternative='less'))\n",
    "print(wilcoxon(t5_base_prefix_pred_scores[2]['precision'], docTTTTTquery_prefix_pred_scores[2]['precision'], alternative='less'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rq2: effect of continuous prefix tuning\n",
    "\n",
    "# t5-base case\n",
    "print(wilcoxon(t5_base_ft_pred_scores[0]['fmeasure'], t5_base_prefix_pred_scores[0]['fmeasure'], alternative='less'))\n",
    "print(wilcoxon(t5_base_ft_pred_scores[1]['fmeasure'], t5_base_prefix_pred_scores[1]['fmeasure'], alternative='less'))\n",
    "print(wilcoxon(t5_base_ft_pred_scores[2]['fmeasure'], t5_base_prefix_pred_scores[2]['fmeasure'], alternative='less'))\n",
    "print('=')\n",
    "\n",
    "# docTTTTTquery case\n",
    "print(wilcoxon(docTTTTTquery_ft_pred_scores[0]['fmeasure'], docTTTTTquery_prefix_pred_scores[0]['fmeasure'], alternative='less'))\n",
    "print(wilcoxon(docTTTTTquery_ft_pred_scores[1]['fmeasure'], docTTTTTquery_prefix_pred_scores[1]['fmeasure'], alternative='less'))\n",
    "print(wilcoxon(docTTTTTquery_ft_pred_scores[2]['fmeasure'], docTTTTTquery_prefix_pred_scores[2]['fmeasure'], alternative='less'))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

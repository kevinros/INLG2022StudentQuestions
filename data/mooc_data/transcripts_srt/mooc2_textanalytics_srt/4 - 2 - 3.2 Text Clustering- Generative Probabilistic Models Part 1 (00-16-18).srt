1
00:00:00,217 --> 00:00:06,963
[SOUND]
This

2
00:00:06,963 --> 00:00:11,480
lecture is about generating probabilistic
models for text clustering.

3
00:00:13,860 --> 00:00:17,730
In this lecture, we're going to continue
discussing text clustering, and

4
00:00:17,730 --> 00:00:21,740
we're going to introduce
generating probabilistic models

5
00:00:21,740 --> 00:00:25,770
as a way to do text clustering.

6
00:00:25,770 --> 00:00:30,720
So this is the overall plan for
covering text clustering.

7
00:00:30,720 --> 00:00:34,893
In the previous lecture, we have talked
about what is text clustering and

8
00:00:34,893 --> 00:00:37,062
why text clustering is interesting.

9
00:00:37,062 --> 00:00:40,607
In this lecture, we're going to talk
about how to do text clustering.

10
00:00:40,607 --> 00:00:44,707
In general, as you see on this slide,
there are two kinds of approaches.

11
00:00:44,707 --> 00:00:49,660
One is generating probabilistic models,
which is the topic of this lecture.

12
00:00:49,660 --> 00:00:52,259
And later, we'll also discuss
similarity-based approaches.

13
00:00:53,840 --> 00:00:58,530
So to talk about generating models for
text clustering,

14
00:00:58,530 --> 00:01:03,460
it would be useful to revisit
the topic mining problem

15
00:01:03,460 --> 00:01:08,900
using topic models,
because the two problems are very similar.

16
00:01:08,900 --> 00:01:15,089
This is a slide that you have seen
earlier in the lecture on topic model.

17
00:01:15,089 --> 00:01:19,827
Here we show that we have input
of a text collection C and

18
00:01:19,827 --> 00:01:23,334
a number of topics k, and vocabulary V.

19
00:01:23,334 --> 00:01:27,760
And we hope to generate
as output two things.

20
00:01:27,760 --> 00:01:31,478
One is a set of topics
denoted by Theta i's,

21
00:01:31,478 --> 00:01:35,750
each is awarded distribution and
the other is pi i j.

22
00:01:35,750 --> 00:01:42,120
These are the probabilities that
each document covers each topic.

23
00:01:42,120 --> 00:01:47,190
So this is a topic coverage and
it's also visualized here on this slide.

24
00:01:47,190 --> 00:01:51,279
You can see that this is what we
can get by using a topic model.

25
00:01:51,279 --> 00:01:58,101
Now, the main difference between this and
the text clustering problem is that here,

26
00:01:58,101 --> 00:02:02,758
a document is assumed to
possibly cover multiple topics.

27
00:02:02,758 --> 00:02:07,946
And indeed, in general,
a document will be covering

28
00:02:07,946 --> 00:02:12,802
more than one topic with
nonzero probabilities.

29
00:02:12,802 --> 00:02:17,624
In text clustering, however,
we only allow a document

30
00:02:17,624 --> 00:02:22,460
to cover one topic,
if we assume one topic is a cluster.

31
00:02:24,270 --> 00:02:30,610
So that means if we change the problem
definition just slightly by

32
00:02:30,610 --> 00:02:35,800
assuming that each document that can only
be generated by using precisely one topic.

33
00:02:37,200 --> 00:02:41,987
Then we'll have a definition of
the clustering problem as you'll hear.

34
00:02:41,987 --> 00:02:44,431
So here the output is changed so

35
00:02:44,431 --> 00:02:49,703
that we no longer have the detailed
coverage distributions pi i j.

36
00:02:49,703 --> 00:02:55,084
But instead, we're going to have
a cluster assignment decisions, Ci.

37
00:02:55,084 --> 00:03:02,015
And Ci is a decision for the document i.

38
00:03:02,015 --> 00:03:06,943
And C sub i is going to take a value
from 1 through k to indicate one of

39
00:03:06,943 --> 00:03:08,180
the k clusters.

40
00:03:09,230 --> 00:03:15,766
And basically tells us that
d i is in which cluster.

41
00:03:15,766 --> 00:03:21,772
As illustrated here, we no longer have
multiple topics covered in each document.

42
00:03:21,772 --> 00:03:23,862
It is precisely one topic.

43
00:03:23,862 --> 00:03:27,330
Although which topic is still uncertain.

44
00:03:27,330 --> 00:03:28,950
There is also a connection with

45
00:03:29,980 --> 00:03:34,800
the problem of mining one topic
that we discussed earlier.

46
00:03:34,800 --> 00:03:38,980
So here again,
it's a slide that you have seen before and

47
00:03:38,980 --> 00:03:43,130
here we hope to estimate a topic model or

48
00:03:43,130 --> 00:03:47,145
distribution based on
precisely one document.

49
00:03:47,145 --> 00:03:51,065
And that's when we assume that this
document, it covers precisely one topic.

50
00:03:52,955 --> 00:03:55,585
But we can also consider some
variations of the problem.

51
00:03:55,585 --> 00:03:59,000
For example,
we can consider there are N documents,

52
00:03:59,000 --> 00:04:04,410
each covers a different topic, so
that's N documents, and topics.

53
00:04:04,410 --> 00:04:07,232
Of course, in this case,
these documents are independent,

54
00:04:07,232 --> 00:04:09,090
and these topics are also independent.

55
00:04:09,090 --> 00:04:13,811
But, we can further allow these
documents with share topics, and

56
00:04:13,811 --> 00:04:18,869
we can also assume that we are going
to assume there are fewer topics than

57
00:04:18,869 --> 00:04:23,862
the number of documents, so
these documents must share some topics.

58
00:04:23,862 --> 00:04:27,243
And if we have N documents
that share k topics,

59
00:04:27,243 --> 00:04:32,290
then we'll again have precisely
the document clustering problem.

60
00:04:34,350 --> 00:04:37,430
So because of these connections,
naturally we can think about how to

61
00:04:37,430 --> 00:04:41,310
use a probabilistically generative model
to solve the problem of text clustering.

62
00:04:43,450 --> 00:04:47,640
So the question now is what generative
model can be used to do clustering?

63
00:04:49,700 --> 00:04:54,960
As in all cases of designing a generative
model, we hope the generative model would

64
00:04:54,960 --> 00:05:00,008
adopt the output that we hope to generate
or the structure that we hope to model.

65
00:05:00,008 --> 00:05:04,071
So in this case,
this is a clustering structure,

66
00:05:04,071 --> 00:05:08,346
the topics and
each document that covers one topic.

67
00:05:08,346 --> 00:05:15,407
And we hope to embed such
preferences in the generative model.

68
00:05:15,407 --> 00:05:18,987
But, if you think about the main
difference between this problem and

69
00:05:18,987 --> 00:05:21,407
the topic model that we
talked about earlier.

70
00:05:21,407 --> 00:05:26,391
And you will see a main requirement
is how can we force every

71
00:05:26,391 --> 00:05:30,867
document to be generated
from precisely one topic,

72
00:05:30,867 --> 00:05:34,650
instead of k topics,
as in the topic model?

73
00:05:35,930 --> 00:05:41,630
So let's revisit the topic
model again in more detail.

74
00:05:41,630 --> 00:05:46,360
So this is a detailed view of
a two component mixture model.

75
00:05:46,360 --> 00:05:49,920
When we have k components,
it looks similar.

76
00:05:49,920 --> 00:05:52,759
So here we see that when
we generate a document,

77
00:05:53,860 --> 00:05:56,210
we generate each word independent.

78
00:05:57,480 --> 00:06:03,969
And when we generate each word, but first
make a choice between these distributions.

79
00:06:03,969 --> 00:06:10,205
We decide to use one of
them with probability.

80
00:06:10,205 --> 00:06:17,743
So p of theta 1 is the probability of
choosing the distribution on the top.

81
00:06:17,743 --> 00:06:22,383
Now we first make this decision regarding
which distribution should be used to

82
00:06:22,383 --> 00:06:23,587
generate the word.

83
00:06:23,587 --> 00:06:27,664
And then we're going to use this
distribution to sample a word.

84
00:06:27,664 --> 00:06:31,042
Now note that in such a generative model,

85
00:06:31,042 --> 00:06:37,550
the decision on which distribution
to use for each word is independent.

86
00:06:37,550 --> 00:06:38,820
So that means, for example,

87
00:06:38,820 --> 00:06:43,173
the here could have generated from
the second distribution, theta 2

88
00:06:43,173 --> 00:06:48,580
whereas text is more likely generated
from the first one on the top.

89
00:06:49,620 --> 00:06:55,060
That means the words in the document that
could have been generated in general

90
00:06:55,060 --> 00:06:56,649
from multiple distributions.

91
00:06:58,390 --> 00:07:02,880
Now this is not what we want,
as we said, for text clustering, for

92
00:07:02,880 --> 00:07:04,090
document clustering,

93
00:07:04,090 --> 00:07:08,060
where we hoped this document will be
generated from precisely one topic.

94
00:07:09,550 --> 00:07:12,880
So now that means we
need to modify the model.

95
00:07:12,880 --> 00:07:13,970
But how?

96
00:07:13,970 --> 00:07:20,170
Well, let's first think about why this
model cannot be used for clustering.

97
00:07:20,170 --> 00:07:23,760
And as I just said, the reason is because

98
00:07:23,760 --> 00:07:27,890
it has allowed multiple topics to
contribute a word to the document.

99
00:07:28,890 --> 00:07:33,000
And that causes confusion because
we're not going to know which cluster

100
00:07:33,000 --> 00:07:34,380
this document is from.

101
00:07:34,380 --> 00:07:37,280
And it's, more importantly
it's violating our assumption

102
00:07:37,280 --> 00:07:41,258
about the partitioning of
documents in the clusters.

103
00:07:41,258 --> 00:07:45,950
If we really have one topic to correspond
it to one cluster of documents,

104
00:07:45,950 --> 00:07:50,670
then we would have a document that we
generate from precisely one topic.

105
00:07:50,670 --> 00:07:54,050
That means all the words in the document

106
00:07:54,050 --> 00:07:57,530
must have been generated from
precisely one distribution.

107
00:07:57,530 --> 00:08:01,950
And this is not true for
such a topic model that we're seeing here.

108
00:08:01,950 --> 00:08:07,640
And that's why this cannot be used for
clustering because it did not ensure

109
00:08:07,640 --> 00:08:12,890
that only one distribution has been used
to generate all the words in one document.

110
00:08:15,110 --> 00:08:17,180
So if you realize this problem,

111
00:08:17,180 --> 00:08:22,110
then we can naturally design alternative
mixture model for doing clustering.

112
00:08:22,110 --> 00:08:24,320
So this is what you're seeing here.

113
00:08:24,320 --> 00:08:29,027
And we again have to make a decision
regarding which distribution to use

114
00:08:29,027 --> 00:08:33,421
to generate this document because
the document could potentially

115
00:08:33,421 --> 00:08:37,592
be generated from any of the k
word distributions that we have.

116
00:08:37,592 --> 00:08:42,581
But this time, once we have made
a decision to choose one of the topics,

117
00:08:42,581 --> 00:08:47,999
we're going to stay with this regime to
generate all the words in the document.

118
00:08:49,768 --> 00:08:54,719
And that means, once we have made
a choice of the distribution in

119
00:08:54,719 --> 00:08:59,397
generating the first word,
we're going to go stay with this

120
00:08:59,397 --> 00:09:04,643
distribution in generating all of
the other words in the document.

121
00:09:04,643 --> 00:09:09,448
So, in other words,
we only make the choice once for,

122
00:09:09,448 --> 00:09:14,671
basically, we make the decision once for
this document and

123
00:09:14,671 --> 00:09:18,754
this state was just to
generate all the words.

124
00:09:18,754 --> 00:09:22,794
Similarly if I had choosing the second
distribution, theta sub 2 here,

125
00:09:22,794 --> 00:09:24,824
you can see which state was this one.

126
00:09:24,824 --> 00:09:27,669
And then generate
the entire document of d.

127
00:09:27,669 --> 00:09:32,540
Now, if you compare this
picture with the previous one,

128
00:09:32,540 --> 00:09:37,717
you will see the decision of
using a particular distribution

129
00:09:37,717 --> 00:09:44,740
is made just once for this document,
in the case of document clustering.

130
00:09:44,740 --> 00:09:46,310
But in the case of topic model,

131
00:09:46,310 --> 00:09:51,080
we have to make as many decisions as
the number of words in the document.

132
00:09:51,080 --> 00:09:54,990
Because for each word, we can make
a potentially different decision.

133
00:09:54,990 --> 00:09:57,140
And that's the key difference
between the two models.

134
00:09:58,240 --> 00:10:01,363
But this is obviously
also a mixed model so

135
00:10:01,363 --> 00:10:05,824
we can just group them together
as one box to show that this is

136
00:10:05,824 --> 00:10:10,214
the model that will give us
a probability of the document.

137
00:10:10,214 --> 00:10:11,766
Now, inside of this model,

138
00:10:11,766 --> 00:10:15,335
there is also this switch of
choosing a different distribution.

139
00:10:15,335 --> 00:10:18,908
And we don't observe that so
that's a mixture model.

140
00:10:18,908 --> 00:10:23,324
And of course a main problem in
document clustering is to infer which

141
00:10:23,324 --> 00:10:26,810
distribution has been used
to generate a document and

142
00:10:26,810 --> 00:10:31,165
that would allow us to recover
the cluster identity of a document.

143
00:10:37,518 --> 00:10:41,911
So it will be useful to think about
the difference from the topic model as

144
00:10:41,911 --> 00:10:44,339
I have also mentioned multiple times.

145
00:10:46,110 --> 00:10:52,370
And there are mainly two differences,

146
00:10:52,370 --> 00:10:55,100
one is the choice of

147
00:10:56,620 --> 00:11:02,315
using that particular distribution is
made just once for document clustering.

148
00:11:02,315 --> 00:11:08,230
Whereas in the topic model, it's made
it multiple times for different words.

149
00:11:08,230 --> 00:11:12,600
The second is that word distribution,
here,

150
00:11:12,600 --> 00:11:17,800
is going to be used to regenerate
all the words for a document.

151
00:11:19,260 --> 00:11:23,612
But, in the case of one
distribution doesn't have to

152
00:11:23,612 --> 00:11:26,467
generate all the words in the document.

153
00:11:26,467 --> 00:11:31,022
Multiple distribution could have been used
to generate the words in the document.

154
00:11:34,322 --> 00:11:37,179
Let's also think about a special case,

155
00:11:37,179 --> 00:11:42,990
when one of the probability of choosing
a particular distribution is equal to 1.

156
00:11:42,990 --> 00:11:46,750
Now that just means we
have no uncertainty now.

157
00:11:46,750 --> 00:11:50,842
We just stick with one
particular distribution.

158
00:11:50,842 --> 00:11:55,189
Now in that case, clearly, we will
see this is no longer mixture model,

159
00:11:55,189 --> 00:11:57,686
because there's no uncertainty here and

160
00:11:57,686 --> 00:12:02,414
we can just use precisely one of the
distributions for generating a document.

161
00:12:02,414 --> 00:12:07,202
And we're going back to
the case of estimating one

162
00:12:07,202 --> 00:12:11,420
order distribution based on one document.

163
00:12:12,880 --> 00:12:15,529
So that's a connection
that we discussed earlier.

164
00:12:15,529 --> 00:12:19,010
Now you can see it more clearly.

165
00:12:19,010 --> 00:12:22,667
So as in all cases of using
a generative model to solve a problem,

166
00:12:22,667 --> 00:12:26,480
we first look at data and
then think about how to design the model.

167
00:12:26,480 --> 00:12:27,720
But once we design the model,

168
00:12:27,720 --> 00:12:31,640
the next step is to write
down the likelihood function.

169
00:12:31,640 --> 00:12:35,070
And after that we're going to look at
the how to estimate the parameters.

170
00:12:36,350 --> 00:12:39,030
So in this case,
what's the likelihood function?

171
00:12:39,030 --> 00:12:43,060
It's going to be very similar to what
you have seen before in topic models but

172
00:12:43,060 --> 00:12:43,960
it will be also different.

173
00:12:45,210 --> 00:12:49,563
Now if you still recall what
the likelihood function looks like in

174
00:12:49,563 --> 00:12:54,515
then you will realize that in general, the
probability of observing a data point from

175
00:12:54,515 --> 00:12:59,010
mixture model is going to be a sum of all
the possibilities of generating the data.

176
00:13:00,520 --> 00:13:03,680
In this case, so it's going to
be a sum over these k topics,

177
00:13:03,680 --> 00:13:06,970
because every one can be
user generated document.

178
00:13:06,970 --> 00:13:12,110
And then inside is the sum you can still
recall what the formula looks like, and

179
00:13:12,110 --> 00:13:18,950
it's going to be the product
of two probabilities.

180
00:13:18,950 --> 00:13:23,450
One is the probability of choosing the
distribution, the other is the probability

181
00:13:23,450 --> 00:13:26,480
of observing a particular
datapoint from that distribution.

182
00:13:27,630 --> 00:13:33,457
So if you map this kind of
formula to our problem here,

183
00:13:33,457 --> 00:13:36,195
you will see the probability
of observing a document d

184
00:13:37,225 --> 00:13:41,997
is basically a sum in this
case of two different

185
00:13:41,997 --> 00:13:47,617
distributions because we have a very
simplified situation of just two clusters.

186
00:13:47,617 --> 00:13:51,657
And so in this case,
you can see it's a sum of two cases.

187
00:13:51,657 --> 00:13:56,461
In each case,
it's indeed the probability of choosing

188
00:13:56,461 --> 00:14:03,600
the distribution either theta 1 or
theta 2.

189
00:14:03,600 --> 00:14:08,810
And then, the probability is
multiplied by the probability of

190
00:14:08,810 --> 00:14:13,790
observing this document from
this particular distribution.

191
00:14:16,430 --> 00:14:21,540
And if you further expanded
this probability of observing

192
00:14:21,540 --> 00:14:28,100
the whole document, we see that it's
a product of observing each word X sub i.

193
00:14:28,100 --> 00:14:33,110
And here we made the assumption that
each word is generated independently, so

194
00:14:33,110 --> 00:14:36,270
the probability of the whole
document is just a product

195
00:14:36,270 --> 00:14:38,690
of the probability of each
word in the document.

196
00:14:40,050 --> 00:14:44,120
So this form should be very
similar to the topic model.

197
00:14:44,120 --> 00:14:48,790
But it's also useful to think about
the difference and for that purpose,

198
00:14:48,790 --> 00:14:56,350
I am also copying the probability of
topic model of these two components here.

199
00:14:56,350 --> 00:15:01,301
So here you can see the formula looks very
similar or in many ways, they are similar.

200
00:15:02,480 --> 00:15:05,060
But there is also some difference.

201
00:15:06,110 --> 00:15:09,740
And in particular,
the difference is on the top.

202
00:15:09,740 --> 00:15:14,850
You see for the mixture model for document
clustering, we first take a product, and

203
00:15:14,850 --> 00:15:15,510
then take a sum.

204
00:15:16,610 --> 00:15:19,770
And that's corresponding
to our assumption of

205
00:15:19,770 --> 00:15:22,680
first make a choice of
choosing one distribution and

206
00:15:22,680 --> 00:15:26,320
then stay with the distribution,
it'll generate all the words.

207
00:15:26,320 --> 00:15:29,170
And that's why we have
the product inside the sum.

208
00:15:30,880 --> 00:15:34,790
The sum corresponds to the choice.

209
00:15:34,790 --> 00:15:39,659
Now, in topic model, we see that
the sum is actually inside the product.

210
00:15:39,659 --> 00:15:42,990
And that's because we generated
each word independently.

211
00:15:42,990 --> 00:15:46,789
And that's why we have
the product outside, but

212
00:15:46,789 --> 00:15:51,602
when we generate each word we
have to make a decision regarding

213
00:15:51,602 --> 00:15:56,437
which distribution we use so
we have a sum there for each word.

214
00:15:56,437 --> 00:16:01,306
But in general,
these are all mixture models and

215
00:16:01,306 --> 00:16:06,887
we can estimate these models
by using the Algorithm,

216
00:16:06,887 --> 00:16:09,990
as we will discuss more later.

217
00:16:09,990 --> 00:16:18,399
[MUSIC]
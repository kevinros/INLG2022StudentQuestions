1
00:00:06,863 --> 00:00:10,600
This lecture is about the methods for
text categorization.

2
00:00:12,650 --> 00:00:16,160
So in this lecture we're going to discuss
how to do text for categorization.

3
00:00:19,643 --> 00:00:24,259
First, there're many methods for
text categorization.

4
00:00:25,690 --> 00:00:30,300
In such a method the idea is
to determine the category

5
00:00:30,300 --> 00:00:33,590
based on some rules that
we design carefully

6
00:00:33,590 --> 00:00:37,560
to reflect the domain knowledge about
the category prediction problem.

7
00:00:37,560 --> 00:00:42,660
So for example, if you want to do topic
categorization for news articles you

8
00:00:42,660 --> 00:00:48,750
can say well, if the news article mentions
word like a game and sports three times.

9
00:00:48,750 --> 00:00:55,650
That we're going to say it's about sports
things like that and this would allow us

10
00:00:55,650 --> 00:00:59,909
to deterministically decide which category
a document that should be put into.

11
00:01:02,090 --> 00:01:09,290
Now such a strategy would work well
if the following conditions hold.

12
00:01:09,290 --> 00:01:14,140
First the categories must be very well
defined and this allows the person to

13
00:01:14,140 --> 00:01:19,309
clearly decide the category
based on some clear rules.

14
00:01:21,700 --> 00:01:24,420
A certainly the categories as

15
00:01:25,600 --> 00:01:31,310
half to be easy to distinguished at
the based on a surface features in text.

16
00:01:31,310 --> 00:01:36,430
So that means some official
features like keywords or

17
00:01:36,430 --> 00:01:40,400
punctuations or whatever,
you can easily identify in text to data.

18
00:01:41,910 --> 00:01:46,680
For example, if there is some
special vocabulary that is known

19
00:01:46,680 --> 00:01:49,440
to only occur in a particular category.

20
00:01:49,440 --> 00:01:53,100
And that would be most effective because
we can easily use such a vocabulary or

21
00:01:53,100 --> 00:01:56,410
padding of such a vocabulary
to recognize this category.

22
00:01:57,690 --> 00:02:02,930
Now we also should have
sufficient knowledge for

23
00:02:02,930 --> 00:02:10,030
designing these words, and so if that's
the case then such a can be effective.

24
00:02:10,030 --> 00:02:17,280
And so it does have a in some domains and
sometimes.

25
00:02:17,280 --> 00:02:22,790
However, in general, there are several
problems with this approach.

26
00:02:22,790 --> 00:02:27,010
First off, because it's label intensive
it requires a lot of manual work.

27
00:02:27,010 --> 00:02:30,730
Obviously, we can't do this for
all kinds of categorization problems.

28
00:02:30,730 --> 00:02:35,190
We have to do it from scratch for
a different problem.

29
00:02:35,190 --> 00:02:38,420
problem because given the rules,
what they need.

30
00:02:38,420 --> 00:02:39,730
So it doesn't scale up well.

31
00:02:41,140 --> 00:02:46,070
Secondly, it cannot handle
uncertainty in rules,

32
00:02:46,070 --> 00:02:51,440
often the rules Aren't 100% reliable.

33
00:02:51,440 --> 00:02:55,340
Take for example looking at
occurrences of words in texts and

34
00:02:55,340 --> 00:02:56,560
trying to decide the topic.

35
00:02:57,570 --> 00:03:02,810
It's actually very hard to
have 100% correct rule.

36
00:03:02,810 --> 00:03:06,460
So for example you can say well,
if it has game, sports,

37
00:03:06,460 --> 00:03:09,710
basketball Then for
sure it's about sports.

38
00:03:09,710 --> 00:03:15,650
But one can also imagine some types of
articles that mention these cures, but

39
00:03:15,650 --> 00:03:21,470
may not be exactly about sports or
only marginally touching sports.

40
00:03:21,470 --> 00:03:26,000
The main topic could be another topic,
a different topic than sports.

41
00:03:27,450 --> 00:03:30,470
So that's one disadvantage
of this approach.

42
00:03:30,470 --> 00:03:34,630
And then finally,
the rules maybe inconsistent and

43
00:03:34,630 --> 00:03:38,020
this would lead to robustness.

44
00:03:38,020 --> 00:03:42,550
More specifically, and sometimes, the
results of categorization may be different

45
00:03:42,550 --> 00:03:45,160
that depending on which
rule to be applied.

46
00:03:45,160 --> 00:03:48,270
So as in that case that you
are facing uncertainty.

47
00:03:48,270 --> 00:03:52,570
And you will also have to decide
an order of applying the rules,

48
00:03:52,570 --> 00:03:57,070
or combination of results
that are contradictory.

49
00:03:57,070 --> 00:04:00,770
So all these are problems
with this approach.

50
00:04:00,770 --> 00:04:04,580
And it turns out that both
problems can be solved or

51
00:04:04,580 --> 00:04:06,179
alleviated by using machine learning.

52
00:04:07,280 --> 00:04:13,190
So these machine learning
methods are more automatic.

53
00:04:13,190 --> 00:04:16,750
But, I still put automatic
in quotation marks because

54
00:04:16,750 --> 00:04:21,940
they are not really completely automatic
cause it still require many work.

55
00:04:21,940 --> 00:04:26,550
More specifically we have to use
a human experts to help in two ways.

56
00:04:26,550 --> 00:04:30,640
First the human experts must annotate
data cells was category labels.

57
00:04:30,640 --> 00:04:36,530
And would tell the computer which
documents should receive which categories.

58
00:04:36,530 --> 00:04:37,780
And this is called training data.

59
00:04:38,810 --> 00:04:43,660
And then secondly, the human experts also
need to provide a set of features to

60
00:04:43,660 --> 00:04:46,180
represent each text object.

61
00:04:46,180 --> 00:04:50,290
That can potentially provide
a clue about the category.

62
00:04:50,290 --> 00:04:54,990
So, we need to provide some basic
features for the computers to look into.

63
00:04:55,990 --> 00:04:59,410
In the case of tax a natural
choice would be the words.

64
00:04:59,410 --> 00:05:05,540
So, using each has a feature is
a very common choice to start with,

65
00:05:05,540 --> 00:05:08,860
but of course there are other
sophisticated features like phrases or

66
00:05:08,860 --> 00:05:13,020
even parts of ancients tags or
even syntax to the structures.

67
00:05:13,020 --> 00:05:18,240
So once human experts can provide this
then we can use machine running to learn

68
00:05:18,240 --> 00:05:22,250
soft rules for
categorization from the training data.

69
00:05:22,250 --> 00:05:24,330
So, soft rules just means,

70
00:05:24,330 --> 00:05:27,900
we're going to get decided which category
we should be assigned for a document,

71
00:05:27,900 --> 00:05:32,990
but it's not going to be use using
a rule that is deterministic.

72
00:05:32,990 --> 00:05:38,690
So we might use something similar
to saying that if it matches games,

73
00:05:38,690 --> 00:05:41,630
sports many times,
it's likely to be sports.

74
00:05:41,630 --> 00:05:44,620
But, we're not going to say exactly for
sure but instead,

75
00:05:44,620 --> 00:05:47,180
we're going to use probabilities or
weights.

76
00:05:47,180 --> 00:05:50,380
So that we can combine
much more evidences.

77
00:05:50,380 --> 00:05:53,450
So, the learning process,
basically is going to figure out which

78
00:05:53,450 --> 00:05:57,140
features are most useful for
separating different categories.

79
00:05:57,140 --> 00:06:01,880
And it's going to also figure out how to
optimally combine features to minimize

80
00:06:01,880 --> 00:06:04,910
errors of the categorization
of the training data.

81
00:06:04,910 --> 00:06:08,930
So the training data,
as you can see here, is very important.

82
00:06:08,930 --> 00:06:10,980
It's the basis for learning.

83
00:06:10,980 --> 00:06:15,050
And then, the trained classifier can be
applied to a new text object to predict

84
00:06:15,050 --> 00:06:16,850
the most likely category.

85
00:06:16,850 --> 00:06:20,820
And that's to simulate
the prediction of what

86
00:06:20,820 --> 00:06:24,675
human Would assign to this text object.

87
00:06:24,675 --> 00:06:27,535
If the human were to make a judgement.

88
00:06:27,535 --> 00:06:33,195
So when we use machine learning for
text categorization we can also

89
00:06:33,195 --> 00:06:39,295
talk about the problem in the general
setting of supervisement.

90
00:06:39,295 --> 00:06:45,800
So the set up is to learn
a classifier to map a value of X.

91
00:06:45,800 --> 00:06:51,380
Into a map of Y so
here X is all the text objects and

92
00:06:51,380 --> 00:06:55,260
Y is all the categories,
a set of categories.

93
00:06:55,260 --> 00:07:00,290
So the class phi will take
any value in x as input and

94
00:07:00,290 --> 00:07:03,280
would generate a value in y as output.

95
00:07:03,280 --> 00:07:08,670
We hope that output y with
this right category for x.

96
00:07:08,670 --> 00:07:12,550
And here correct, of course,
is judged based on the training data.

97
00:07:12,550 --> 00:07:17,740
So that's a general goal in machine
learning problems or supervised learning

98
00:07:17,740 --> 00:07:23,260
problems where you are given some examples
of input and output for a function.

99
00:07:23,260 --> 00:07:27,120
And then the computer's
going to figure out the,

100
00:07:27,120 --> 00:07:30,680
how the function behaves
like based on this examples.

101
00:07:30,680 --> 00:07:34,260
And then try to be able
to compute the values for

102
00:07:34,260 --> 00:07:36,325
future x's that when we have not seen.

103
00:07:38,795 --> 00:07:43,305
So in general all methods
would rely on discriminative

104
00:07:43,305 --> 00:07:46,555
features of text objects to
distinguish different categories.

105
00:07:46,555 --> 00:07:49,295
So that's why these features
are very important and

106
00:07:49,295 --> 00:07:52,775
they have to be provided by humans.

107
00:07:52,775 --> 00:07:57,435
And they will also combine multiple
features in a weight map with weights

108
00:07:57,435 --> 00:08:02,800
to be optimized to minimize
errors on the training data.

109
00:08:02,800 --> 00:08:06,355
So after the learning processes
optimization problem.

110
00:08:06,355 --> 00:08:10,710
An objective function is often tied
into the errors on the training data.

111
00:08:12,610 --> 00:08:16,960
Different methods tend to vary in
their ways of measuring the errors on

112
00:08:16,960 --> 00:08:18,410
the training data.

113
00:08:18,410 --> 00:08:21,900
They might optimize
a different objective function,

114
00:08:21,900 --> 00:08:25,250
which is often also called a loss
function or cost function.

115
00:08:26,520 --> 00:08:31,260
They also tend to vary in their
ways of combining the features.

116
00:08:31,260 --> 00:08:37,220
So a linear combination for
example is simple, is often used.

117
00:08:37,220 --> 00:08:40,360
But they are not as powerful
as nonlinear combinations.

118
00:08:40,360 --> 00:08:43,250
But nonlinear models
might be more complex for

119
00:08:43,250 --> 00:08:45,880
training, so there are tradeoffs as well.

120
00:08:45,880 --> 00:08:48,409
But that would lead to
different variations of

121
00:08:50,250 --> 00:08:53,550
many variations of these learning methods.

122
00:08:53,550 --> 00:08:59,180
So in general we can distinguish two
kinds of classifiers at a high level.

123
00:08:59,180 --> 00:09:01,120
One is called generative classifiers.

124
00:09:01,120 --> 00:09:03,950
The other is called
discriminative classifiers.

125
00:09:03,950 --> 00:09:10,783
The generative classifiers try to learn
what the data looks like in each category.

126
00:09:10,783 --> 00:09:16,715
So it attempts to model the joint
distribution of the data and

127
00:09:16,715 --> 00:09:21,599
the label x and y and
this can then be factored out to

128
00:09:21,599 --> 00:09:26,043
a product of why
the distribution of labels.

129
00:09:26,043 --> 00:09:32,128
And the joint probability
of sorry the conditional

130
00:09:32,128 --> 00:09:36,663
probability of X given Y, so it's Y.

131
00:09:36,663 --> 00:09:40,813
So we first model
the distribution of labels and

132
00:09:40,813 --> 00:09:46,320
then we model how the data is
generate a particular label here.

133
00:09:48,550 --> 00:09:51,740
And once we can estimate these models,

134
00:09:51,740 --> 00:09:56,698
then we can compute this conditional
probability of label given data

135
00:09:56,698 --> 00:10:01,230
based on the probability
of data given label.

136
00:10:02,680 --> 00:10:05,810
And the label distribution
here by using the Bayes Rule.

137
00:10:07,200 --> 00:10:11,840
Now this is the most important thing,
because this conditional probability

138
00:10:11,840 --> 00:10:16,820
of the label can then be used directly
to decide which label is most likely.

139
00:10:18,780 --> 00:10:22,110
So in such approaches objective
function is actually likelihood.

140
00:10:22,110 --> 00:10:26,790
And so,
we model how the data are generated.

141
00:10:26,790 --> 00:10:31,340
So it only indirectly
captures the training errors.

142
00:10:31,340 --> 00:10:34,850
But if we can model the data
in each category accurately,

143
00:10:34,850 --> 00:10:36,930
then we can also classify accurately.

144
00:10:38,080 --> 00:10:43,470
One example is NaÃ¯ve Bayes classifier,
in this case.

145
00:10:43,470 --> 00:10:48,720
The other kind of approaches
are called discriminative classifies,

146
00:10:48,720 --> 00:10:53,600
and these classifies try to learn
what features separate categories.

147
00:10:53,600 --> 00:10:57,885
So they direct or attack the problem of
categorization for separation of classes.

148
00:10:57,885 --> 00:11:03,132
So sorry for the problem.

149
00:11:04,280 --> 00:11:08,860
So, these discriminative
classifiers attempt to model

150
00:11:08,860 --> 00:11:15,670
the conditional probability of the label
given the data point directly.

151
00:11:17,230 --> 00:11:21,290
So, the objective function tends
to directly measure the errors of

152
00:11:21,290 --> 00:11:22,900
categorization on the training data.

153
00:11:24,550 --> 00:11:27,090
Some examples include
a logistical regression,

154
00:11:27,090 --> 00:11:31,710
support vector machines,
and k-nearest neighbors.

155
00:11:31,710 --> 00:11:37,953
We will cover some of these classifiers
in detail in the next few lectures.

156
00:11:37,953 --> 00:11:47,953
[MUSIC]
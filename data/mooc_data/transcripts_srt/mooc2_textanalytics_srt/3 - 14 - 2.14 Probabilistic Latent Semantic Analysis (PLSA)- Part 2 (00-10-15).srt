1
00:00:00,366 --> 00:00:03,025
[SOUND]

2
00:00:08,261 --> 00:00:11,100
We can compute this maximum estimate

3
00:00:11,100 --> 00:00:12,823
by using the EM algorithm.

4
00:00:12,823 --> 00:00:16,544
So in the e step,
we now have to introduce more hidden

5
00:00:16,544 --> 00:00:21,536
variables because we have more topics,
so our hidden variable z now,

6
00:00:21,536 --> 00:00:25,690
which is a topic indicator can
take more than two values.

7
00:00:25,690 --> 00:00:28,888
So specifically will
take a k plus one values,

8
00:00:28,888 --> 00:00:32,200
with b in the noting the background.

9
00:00:32,200 --> 00:00:35,527
And once locate,
to denote other k topics, right.

10
00:00:36,750 --> 00:00:40,740
So, now the e step, as you can
recall is your augmented data, and

11
00:00:40,740 --> 00:00:44,640
by predicting the values
of the hidden variable.

12
00:00:44,640 --> 00:00:47,700
So we're going to predict for

13
00:00:47,700 --> 00:00:52,990
a word, whether the word has come from
one of these k plus one distributions.

14
00:00:52,990 --> 00:00:57,020
This equation allows us to
predict the probability

15
00:00:57,020 --> 00:01:01,540
that the word w in document d is
generated from topic zero sub j.

16
00:01:03,010 --> 00:01:06,050
And the bottom one is
the predicted probability that this

17
00:01:06,050 --> 00:01:08,740
word has been generated
from the background.

18
00:01:08,740 --> 00:01:14,190
Note that we use document
d here to index the word.

19
00:01:14,190 --> 00:01:14,990
Why?

20
00:01:14,990 --> 00:01:18,860
Because whether a word is
from a particular topic

21
00:01:18,860 --> 00:01:20,790
actually depends on the document.

22
00:01:20,790 --> 00:01:22,210
Can you see why?

23
00:01:22,210 --> 00:01:24,360
Well, it's through the pi's.

24
00:01:24,360 --> 00:01:26,870
The pi's are tied to each document.

25
00:01:26,870 --> 00:01:31,020
Each document can have potentially
different pi's, right.

26
00:01:31,020 --> 00:01:33,670
The pi's will then affect our prediction.

27
00:01:33,670 --> 00:01:35,220
So, the pi's are here.

28
00:01:35,220 --> 00:01:36,740
And this depends on the document.

29
00:01:38,510 --> 00:01:41,100
And that might give a different guess for

30
00:01:41,100 --> 00:01:44,880
a word in different documents,
and that's desirable.

31
00:01:46,320 --> 00:01:50,490
In both cases we are using
the Baye's Rule, as I explained, basically

32
00:01:50,490 --> 00:01:55,300
assessing the likelihood of generating
word from each of this division and

33
00:01:55,300 --> 00:01:56,100
there's normalize.

34
00:01:57,800 --> 00:01:59,130
What about the m step?

35
00:01:59,130 --> 00:02:04,420
Well, we may recall the m step is we
take advantage of the inferred z values.

36
00:02:04,420 --> 00:02:05,630
To split the counts.

37
00:02:05,630 --> 00:02:09,920
And then collected the right counts
to re-estimate the parameters.

38
00:02:09,920 --> 00:02:14,490
So in this case, we can re-estimate
our coverage of probability.

39
00:02:14,490 --> 00:02:21,540
And this is re-estimated based on
collecting all the words in the document.

40
00:02:22,650 --> 00:02:26,800
And that's why we have the count
of the word in document.

41
00:02:26,800 --> 00:02:29,130
And sum over all the words.

42
00:02:29,130 --> 00:02:33,330
And then we're going to look at to
what extent this word belongs to

43
00:02:34,340 --> 00:02:36,710
the topic theta sub j.

44
00:02:36,710 --> 00:02:39,210
And this part is our guess from each step.

45
00:02:40,500 --> 00:02:44,930
This tells us how likely this word
is actually from theta sub j.

46
00:02:44,930 --> 00:02:47,372
And when we multiply them together,

47
00:02:47,372 --> 00:02:51,801
we get the discounted count that's
located for topic theta sub j.

48
00:02:51,801 --> 00:02:54,567
And when we normalize
this over all the topics,

49
00:02:54,567 --> 00:02:58,524
we get the distribution of all
the topics to indicate the coverage.

50
00:02:58,524 --> 00:03:04,619
And similarly, the bottom one is the
estimated probability of word for a topic.

51
00:03:04,619 --> 00:03:09,635
And in this case we are using exact
the same count, you can see this is

52
00:03:09,635 --> 00:03:14,651
the same discounted account,
] it tells us to what extend we should

53
00:03:14,651 --> 00:03:19,765
allocate this word [INAUDIBLE] but
then normalization is different.

54
00:03:19,765 --> 00:03:24,325
Because in this case we are interested
in the word distribution, so

55
00:03:24,325 --> 00:03:27,365
we simply normalize this
over all the words.

56
00:03:27,365 --> 00:03:33,202
This is different, in contrast here we
normalize the amount all the topics.

57
00:03:33,202 --> 00:03:35,670
It would be useful to take
a comparison between the two.

58
00:03:37,420 --> 00:03:39,568
This give us different distributions.

59
00:03:39,568 --> 00:03:46,142
And these tells us how to
improve the parameters.

60
00:03:48,279 --> 00:03:55,275
And as I just explained,
in both the formula is we have a maximum

61
00:03:55,275 --> 00:04:00,534
estimate based on allocated
word counts [INAUDIBLE].

62
00:04:00,534 --> 00:04:04,882
Now this phenomena is actually general
phenomena in all the EM algorithms.

63
00:04:04,882 --> 00:04:09,909
In the m-step, you general with
the computer expect an account of

64
00:04:09,909 --> 00:04:15,025
the event based on the e-step result,
and then you just and

65
00:04:15,025 --> 00:04:20,270
then count to four,
particular normalize it, typically.

66
00:04:20,270 --> 00:04:24,965
So, in terms of computation
of this EM algorithm, we can

67
00:04:24,965 --> 00:04:32,290
actually just keep accounting various
events and then normalize them.

68
00:04:32,290 --> 00:04:34,163
And when we thinking this way,

69
00:04:34,163 --> 00:04:37,993
we also have a more concise way
of presenting the EM Algorithm.

70
00:04:37,993 --> 00:04:42,440
It actually helps us better
understand the formulas.

71
00:04:42,440 --> 00:04:44,890
So I'm going to go over
this in some detail.

72
00:04:44,890 --> 00:04:48,692
So as a algorithm we first initialize
all the unknown perimeters randomly,

73
00:04:48,692 --> 00:04:50,200
all right.

74
00:04:50,200 --> 00:04:55,000
So, in our case, we are interested in all
of those coverage perimeters, pi's and

75
00:04:55,000 --> 00:04:59,830
awarded distributions [INAUDIBLE],
and we just randomly normalize them.

76
00:04:59,830 --> 00:05:05,830
This is the initialization step and then
we will repeat until likelihood converges.

77
00:05:05,830 --> 00:05:08,390
Now how do we know whether
likelihood converges?

78
00:05:08,390 --> 00:05:11,740
We can do compute
likelihood at each step and

79
00:05:11,740 --> 00:05:14,960
compare the current likelihood
with the previous likelihood.

80
00:05:14,960 --> 00:05:17,227
If it doesn't change much and
we're going to say it stopped, right.

81
00:05:19,520 --> 00:05:23,392
So, in each step we're
going to do e-step and m-step.

82
00:05:23,392 --> 00:05:27,715
In the e-step we're going to do
augment the data by predicting

83
00:05:27,715 --> 00:05:30,310
the hidden variables.

84
00:05:30,310 --> 00:05:34,400
In this case,
the hidden variable, z sub d, w,

85
00:05:34,400 --> 00:05:41,030
indicates whether the word w in
d is from a topic or background.

86
00:05:41,030 --> 00:05:43,509
And if it's from a topic, which topic.

87
00:05:43,509 --> 00:05:46,767
So if you look at the e-step formulas,

88
00:05:46,767 --> 00:05:52,302
essentially we're actually
normalizing these counts, sorry,

89
00:05:52,302 --> 00:05:58,820
these probabilities of observing
the word from each distribution.

90
00:05:58,820 --> 00:06:03,250
So you can see,
basically the prediction of word

91
00:06:03,250 --> 00:06:07,650
from topic zero sub j is
based on the probability of

92
00:06:07,650 --> 00:06:12,030
selecting that theta sub j as a word
distribution to generate the word.

93
00:06:12,030 --> 00:06:15,990
Multiply by the probability of observing
the word from that distribution.

94
00:06:17,030 --> 00:06:22,030
And I said it's proportional to this
because in the implementation of

95
00:06:22,030 --> 00:06:25,820
EM algorithm you can keep counter for

96
00:06:25,820 --> 00:06:28,830
this quantity, and
in the end it just normalizes it.

97
00:06:28,830 --> 00:06:32,080
So the normalization here
is over all the topics and

98
00:06:32,080 --> 00:06:34,310
then you would get a probability.

99
00:06:36,410 --> 00:06:41,682
Now, in the m-step, we do the same,
and we are going to collect these.

100
00:06:43,980 --> 00:06:46,300
Allocated account for each topic.

101
00:06:47,770 --> 00:06:49,690
And we split words among the topics.

102
00:06:50,970 --> 00:06:53,740
And then we're going to normalize
them in different ways to obtain

103
00:06:53,740 --> 00:06:54,890
the real estimate.

104
00:06:54,890 --> 00:07:00,680
So for example, we can normalize among all
the topics to get the re-estimate of pi,

105
00:07:00,680 --> 00:07:02,040
the coverage.

106
00:07:02,040 --> 00:07:08,230
Or we can re-normalize
based on all the words.

107
00:07:08,230 --> 00:07:09,739
And that would give us
a word distribution.

108
00:07:10,960 --> 00:07:15,860
So it's useful to think algorithm in this
way because when implemented, you can just

109
00:07:15,860 --> 00:07:22,420
use variables, but keep track of
these quantities in each case.

110
00:07:23,800 --> 00:07:31,210
And then you just normalize these
variables to make them distribution.

111
00:07:32,210 --> 00:07:35,340
Now I did not put the constraint for
this one.

112
00:07:35,340 --> 00:07:38,550
And I intentionally leave
this as an exercise for you.

113
00:07:38,550 --> 00:07:42,218
And you can see,
what's the normalizer for this one?

114
00:07:42,218 --> 00:07:47,430
It's of a slightly different form but
it's essentially the same as

115
00:07:47,430 --> 00:07:50,940
the one that you have
seen here in this one.

116
00:07:50,940 --> 00:07:54,710
So in general in the envisioning of EM
algorithms you will see you accumulate

117
00:07:54,710 --> 00:07:59,420
the counts, various counts and
then you normalize them.

118
00:08:01,660 --> 00:08:06,752
So to summarize,
we introduced the PLSA model.

119
00:08:06,752 --> 00:08:10,650
Which is a mixture model with k unigram
language models representing k topics.

120
00:08:11,830 --> 00:08:16,850
And we also added a pre-determined
background language model to

121
00:08:16,850 --> 00:08:19,360
help discover discriminative topics,

122
00:08:19,360 --> 00:08:22,370
because this background language model
can help attract the common terms.

123
00:08:23,800 --> 00:08:28,589
And we select the maximum estimate
that we cant discover topical

124
00:08:28,589 --> 00:08:30,403
knowledge from text data.

125
00:08:30,403 --> 00:08:35,304
In this case PLSA allows us to discover
two things, one is k worded distributions,

126
00:08:35,304 --> 00:08:37,265
each one representing a topic and

127
00:08:37,265 --> 00:08:40,779
the other is the proportion of
each topic in each document.

128
00:08:41,990 --> 00:08:46,510
And such detailed characterization
of coverage of topics in documents

129
00:08:46,510 --> 00:08:48,890
can enable a lot of photo analysis.

130
00:08:48,890 --> 00:08:53,970
For example, we can aggregate
the documents in the particular

131
00:08:53,970 --> 00:08:58,800
pan period to assess the coverage of
a particular topic in a time period.

132
00:08:58,800 --> 00:09:02,540
That would allow us to generate
the temporal chains of topics.

133
00:09:02,540 --> 00:09:08,543
We can also aggregate topics covered in
documents associated with a particular

134
00:09:08,543 --> 00:09:14,198
author and then we can categorize
the topics written by this author, etc.

135
00:09:14,198 --> 00:09:20,190
And in addition to this, we can also
cluster terms and cluster documents.

136
00:09:20,190 --> 00:09:23,230
In fact,
each topic can be regarded as a cluster.

137
00:09:23,230 --> 00:09:25,840
So we already have the term clusters.

138
00:09:25,840 --> 00:09:28,240
In the higher probability,
the words can be regarded as

139
00:09:29,630 --> 00:09:34,560
belonging to one cluster
represented by the topic.

140
00:09:34,560 --> 00:09:37,060
Similarly, documents can be
clustered in the same way.

141
00:09:37,060 --> 00:09:41,948
We can assign a document
to the topic cluster

142
00:09:41,948 --> 00:09:45,944
that's covered most in the document.

143
00:09:45,944 --> 00:09:50,610
So remember, pi's indicate to what extent
each topic is covered in the document,

144
00:09:50,610 --> 00:09:55,510
we can assign the document to the topical
cluster that has the highest pi.

145
00:09:57,340 --> 00:10:00,975
And in general there are many useful
applications of this technique.

146
00:10:03,146 --> 00:10:13,146
[MUSIC]


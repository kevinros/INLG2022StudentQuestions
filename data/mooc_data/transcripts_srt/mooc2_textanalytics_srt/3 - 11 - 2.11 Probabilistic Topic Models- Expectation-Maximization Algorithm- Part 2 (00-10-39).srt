1
00:00:00,012 --> 00:00:08,224
[SOUND]
So

2
00:00:08,224 --> 00:00:12,538
this is indeed a general idea of
the Expectation-Maximization, or EM,

3
00:00:12,538 --> 00:00:13,310
Algorithm.

4
00:00:14,640 --> 00:00:19,210
So in all the EM algorithms we
introduce a hidden variable

5
00:00:19,210 --> 00:00:21,970
to help us solve the problem more easily.

6
00:00:21,970 --> 00:00:25,453
In our case the hidden variable
is a binary variable for

7
00:00:25,453 --> 00:00:27,203
each occurrence of a word.

8
00:00:27,203 --> 00:00:32,020
And this binary variable would
indicate whether the word has

9
00:00:32,020 --> 00:00:35,144
been generated from 0 sub d or 0 sub p.

10
00:00:35,144 --> 00:00:38,420
And here we show some possible
values of these variables.

11
00:00:38,420 --> 00:00:43,470
For example, for the it's from background,
the z value is one.

12
00:00:43,470 --> 00:00:45,105
And text on the other hand.

13
00:00:45,105 --> 00:00:52,040
Is from the topic then it's zero for
z, etc.

14
00:00:53,260 --> 00:00:58,915
Now, of course, we don't observe these z
values, we just imagine they're all such.

15
00:00:58,915 --> 00:01:01,875
Values of z attaching to other words.

16
00:01:02,905 --> 00:01:04,975
And that's why we call
these hidden variables.

17
00:01:06,135 --> 00:01:08,905
Now, the idea that we
talked about before for

18
00:01:08,905 --> 00:01:12,930
predicting the word distribution that
has been used when we generate the word

19
00:01:12,930 --> 00:01:18,840
is it a predictor,
the value of this hidden variable?

20
00:01:18,840 --> 00:01:25,080
And, so, the EM algorithm then,
would work as follows.

21
00:01:25,080 --> 00:01:30,060
First, we'll initialize all
the parameters with random values.

22
00:01:30,060 --> 00:01:34,960
In our case,
the parameters are mainly the probability.

23
00:01:34,960 --> 00:01:37,840
of a word, given by theta sub d.

24
00:01:37,840 --> 00:01:39,680
So this is an initial addition stage.

25
00:01:39,680 --> 00:01:44,150
These initialized values would allow
us to use base roll to take a guess

26
00:01:44,150 --> 00:01:48,510
of these z values, so
we'd guess these values.

27
00:01:48,510 --> 00:01:53,580
We can't say for sure whether
textt is from background or not.

28
00:01:53,580 --> 00:01:55,090
But we can have our guess.

29
00:01:55,090 --> 00:01:57,620
This is given by this formula.

30
00:01:57,620 --> 00:01:59,710
It's called an E-step.

31
00:01:59,710 --> 00:02:06,520
And so the algorithm would then try to
use the E-step to guess these z values.

32
00:02:06,520 --> 00:02:12,190
After that, it would then invoke
another that's called M-step.

33
00:02:12,190 --> 00:02:17,490
In this step we simply take advantage
of the inferred z values and

34
00:02:17,490 --> 00:02:22,825
then just group words that are in
the same distribution like these

35
00:02:22,825 --> 00:02:26,315
from that ground including this as well.

36
00:02:27,585 --> 00:02:32,865
We can then normalize the count
to estimate the probabilities or

37
00:02:32,865 --> 00:02:35,479
to revise our estimate of the parameters.

38
00:02:36,590 --> 00:02:42,310
So let me also illustrate
that we can group the words

39
00:02:42,310 --> 00:02:46,760
that are believed to have
come from zero sub d, and

40
00:02:46,760 --> 00:02:50,010
that's text, mining algorithm,
for example, and clustering.

41
00:02:51,760 --> 00:02:55,718
And we group them together to help us

42
00:02:55,718 --> 00:03:01,170
re-estimate the parameters
that we're interested in.

43
00:03:01,170 --> 00:03:05,120
So these will help us
estimate these parameters.

44
00:03:06,170 --> 00:03:09,970
Note that before we just set
these parameter values randomly.

45
00:03:09,970 --> 00:03:15,670
But with this guess, we will have
somewhat improved estimate of this.

46
00:03:15,670 --> 00:03:18,740
Of course, we don't know exactly
whether it's zero or one.

47
00:03:18,740 --> 00:03:24,850
So we're not going to really
do the split in a hard way.

48
00:03:24,850 --> 00:03:26,800
But rather we're going to
do a softer split.

49
00:03:26,800 --> 00:03:27,980
And this is what happened here.

50
00:03:29,150 --> 00:03:34,420
So we're going to adjust the count by
the probability that would believe

51
00:03:34,420 --> 00:03:38,410
this word has been generated
by using the theta sub d.

52
00:03:39,840 --> 00:03:42,580
And you can see this,
where does this come from?

53
00:03:42,580 --> 00:03:46,630
Well, this has come from here, right?

54
00:03:46,630 --> 00:03:48,120
From the E-step.

55
00:03:48,120 --> 00:03:52,472
So the EM Algorithm would
iteratively improve uur initial

56
00:03:52,472 --> 00:03:57,375
estimate of parameters by using
E-step first and then M-step.

57
00:03:57,375 --> 00:04:02,458
The E-step is to augment the data
with additional information, like z.

58
00:04:02,458 --> 00:04:05,910
And the M-step is to take advantage

59
00:04:05,910 --> 00:04:08,660
of the additional information
to separate the data.

60
00:04:08,660 --> 00:04:13,467
To split the data accounts and
then collect the right data accounts to

61
00:04:13,467 --> 00:04:17,870
re-estimate our parameter.

62
00:04:17,870 --> 00:04:22,400
And then once we have a new generation of
parameter, we're going to repeat this.

63
00:04:22,400 --> 00:04:25,150
We are going the E-step again.

64
00:04:25,150 --> 00:04:28,520
To improve our estimate
of the hidden variables.

65
00:04:28,520 --> 00:04:33,630
And then that would lead to another
generation of re-estimated parameters.

66
00:04:34,770 --> 00:04:37,910
For the word distribution
that we are interested in.

67
00:04:39,610 --> 00:04:44,670
Okay, so, as I said,
the bridge between the two

68
00:04:44,670 --> 00:04:50,380
is really the variable z, hidden variable,
which indicates how likely

69
00:04:50,380 --> 00:04:55,200
this water is from the top water
distribution, theta sub p.

70
00:04:56,810 --> 00:05:00,780
So, this slide has a lot of content and
you may need to.

71
00:05:00,780 --> 00:05:03,850
Pause the reader to digest it.

72
00:05:03,850 --> 00:05:07,300
But this basically captures
the essence of EM Algorithm.

73
00:05:07,300 --> 00:05:12,500
Start with initial values that
are often random themself.

74
00:05:12,500 --> 00:05:18,150
And then we invoke E-step followed
by M-step to get an improved

75
00:05:18,150 --> 00:05:19,690
setting of parameters.

76
00:05:19,690 --> 00:05:23,340
And then we repeated this, so
this a Hill-Climbing algorithm

77
00:05:23,340 --> 00:05:27,060
that would gradually improve
the estimate of parameters.

78
00:05:27,060 --> 00:05:30,050
As I will explain later
there is some guarantee for

79
00:05:30,050 --> 00:05:35,340
reaching a local maximum of
the log-likelihood function.

80
00:05:35,340 --> 00:05:40,180
So lets take a look at the computation for
a specific case, so

81
00:05:40,180 --> 00:05:41,840
these formulas are the EM.

82
00:05:41,840 --> 00:05:48,220
Formulas that you see before, and
you can also see there are superscripts,

83
00:05:48,220 --> 00:05:53,720
here, like here, n,
to indicate the generation of parameters.

84
00:05:53,720 --> 00:05:56,040
Like here for example we have n plus one.

85
00:05:56,040 --> 00:05:59,728
That means we have improved.

86
00:05:59,728 --> 00:06:04,047
From here to here we have an improvement.

87
00:06:04,047 --> 00:06:08,106
So in this setting we have assumed the two
numerals have equal probabilities and

88
00:06:08,106 --> 00:06:09,689
the background model is null.

89
00:06:09,689 --> 00:06:11,872
So what are the relevance
of the statistics?

90
00:06:11,872 --> 00:06:13,892
Well these are the word counts.

91
00:06:13,892 --> 00:06:18,290
So assume we have just four words,
and their counts are like this.

92
00:06:18,290 --> 00:06:22,680
And this is our background model that
assigns high probabilities to common

93
00:06:22,680 --> 00:06:23,380
words like the.

94
00:06:25,910 --> 00:06:29,860
And in the first iteration,
you can picture what will happen.

95
00:06:29,860 --> 00:06:32,280
Well first we initialize all the values.

96
00:06:32,280 --> 00:06:37,360
So here, this probability that we're
interested in is normalized into a uniform

97
00:06:37,360 --> 00:06:38,890
distribution of all the words.

98
00:06:40,330 --> 00:06:45,940
And then the E-step would give us a guess
of the distribution that has been used.

99
00:06:45,940 --> 00:06:48,470
That will generate each word.

100
00:06:48,470 --> 00:06:51,450
We can see we have different
probabilities for different words.

101
00:06:51,450 --> 00:06:52,430
Why?

102
00:06:52,430 --> 00:06:56,840
Well, that's because these words have
different probabilities in the background.

103
00:06:56,840 --> 00:07:00,020
So even though the two
distributions are equally likely.

104
00:07:00,020 --> 00:07:05,320
And then our initial audition say uniform
distribution because of the difference

105
00:07:05,320 --> 00:07:09,270
in the background of the distribution,
we have different guess the probability.

106
00:07:09,270 --> 00:07:14,280
So these words are believed to
be more likely from the topic.

107
00:07:15,820 --> 00:07:17,930
These on the other hand are less likely.

108
00:07:17,930 --> 00:07:19,030
Probably from background.

109
00:07:20,620 --> 00:07:23,040
So once we have these z values,

110
00:07:23,040 --> 00:07:28,810
we know in the M-step these probabilities
will be used to adjust the counts.

111
00:07:28,810 --> 00:07:33,670
So four must be multiplied by this 0.33

112
00:07:33,670 --> 00:07:38,190
in order to get the allocated
accounts toward the topic.

113
00:07:39,550 --> 00:07:43,770
And this is done by this multiplication.

114
00:07:43,770 --> 00:07:49,700
Note that if our guess says this
is 100% If this is one point zero,

115
00:07:52,380 --> 00:07:58,010
then we just get the full count
of this word for this topic.

116
00:07:58,010 --> 00:08:01,200
In general it's not going
to be one point zero.

117
00:08:01,200 --> 00:08:06,760
So we're just going to get some percentage
of this counts toward this topic.

118
00:08:06,760 --> 00:08:09,550
Then we simply normalize these counts

119
00:08:09,550 --> 00:08:13,170
to have a new generation
of parameters estimate.

120
00:08:13,170 --> 00:08:16,600
So you can see, compare this with
the older one, which is here.

121
00:08:18,330 --> 00:08:23,060
So compare this with this one and
we'll see the probability is different.

122
00:08:23,060 --> 00:08:25,930
Not only that, we also see some

123
00:08:25,930 --> 00:08:30,110
words that are believed to have come from
the topic will have a higher probability.

124
00:08:30,110 --> 00:08:31,400
Like this one, text.

125
00:08:32,530 --> 00:08:35,930
And of course, this new generation of
parameters would allow us to further

126
00:08:35,930 --> 00:08:42,680
adjust the inferred latent variable or
hidden variable values.

127
00:08:42,680 --> 00:08:45,742
So we have a new generation of values,

128
00:08:45,742 --> 00:08:51,115
because of the E-step based on
the new generation of parameters.

129
00:08:51,115 --> 00:08:56,343
And these new inferred values
of Zs will give us then

130
00:08:56,343 --> 00:09:03,166
another generation of the estimate
of probabilities of the word.

131
00:09:03,166 --> 00:09:07,990
And so on and so forth so this is what
would actually happen when we compute

132
00:09:07,990 --> 00:09:11,750
these probabilities
using the EM Algorithm.

133
00:09:11,750 --> 00:09:16,745
As you can see in the last row
where we show the log-likelihood,

134
00:09:16,745 --> 00:09:20,985
and the likelihood is increasing
as we do the iteration.

135
00:09:20,985 --> 00:09:25,875
And note that these log-likelihood is
negative because the probability is

136
00:09:25,875 --> 00:09:30,070
between 0 and 1 when you take a logarithm,
it becomes a negative value.

137
00:09:30,070 --> 00:09:33,180
Now what's also interesting is,
you'll note the last column.

138
00:09:33,180 --> 00:09:36,600
And these are the inverted word split.

139
00:09:36,600 --> 00:09:42,150
And these are the probabilities
that a word is believed to

140
00:09:42,150 --> 00:09:47,980
have come from one distribution, in this
case the topical distribution, all right.

141
00:09:47,980 --> 00:09:50,580
And you might wonder whether
this would be also useful.

142
00:09:50,580 --> 00:09:55,540
Because our main goal is to
estimate these word distributions.

143
00:09:55,540 --> 00:09:57,400
So this is our primary goal.

144
00:09:57,400 --> 00:10:00,900
We hope to have a more discriminative
order of distribution.

145
00:10:00,900 --> 00:10:04,400
But the last column is also bi-product.

146
00:10:04,400 --> 00:10:07,170
This also can actually be very useful.

147
00:10:07,170 --> 00:10:08,380
You can think about that.

148
00:10:08,380 --> 00:10:10,220
We want to use, is to for

149
00:10:10,220 --> 00:10:16,080
example is to estimate to what extent this
document has covered background words.

150
00:10:16,080 --> 00:10:18,165
And this, when we add this up or

151
00:10:18,165 --> 00:10:23,304
take the average we will kind of know to
what extent it has covered background

152
00:10:23,304 --> 00:10:27,823
versus content was that are not
explained well by the background.

153
00:10:27,823 --> 00:10:37,823
[MUSIC]


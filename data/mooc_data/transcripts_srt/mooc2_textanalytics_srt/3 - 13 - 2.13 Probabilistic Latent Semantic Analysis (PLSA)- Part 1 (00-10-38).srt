1
00:00:00,012 --> 00:00:07,295
[SOUND]
This

2
00:00:07,295 --> 00:00:11,390
lecture is about probabilistic and
latent Semantic Analysis or PLSA.

3
00:00:12,710 --> 00:00:18,000
In this lecture we're going to introduce
probabilistic latent semantic analysis,

4
00:00:18,000 --> 00:00:18,770
often called PLSA.

5
00:00:18,770 --> 00:00:26,060
This is the most basic topic model,
also one of the most useful topic models.

6
00:00:26,060 --> 00:00:30,890
Now this kind of models
can in general be used to

7
00:00:30,890 --> 00:00:34,560
mine multiple topics from text documents.

8
00:00:34,560 --> 00:00:39,410
And PRSA is one of the most basic
topic models for doing this.

9
00:00:39,410 --> 00:00:43,800
So let's first examine this power
in the e-mail for more detail.

10
00:00:43,800 --> 00:00:47,710
Here I show a sample article which is
a blog article about Hurricane Katrina.

11
00:00:48,830 --> 00:00:51,100
And I show some simple topics.

12
00:00:51,100 --> 00:00:55,870
For example government response,
flood of the city of New Orleans.

13
00:00:55,870 --> 00:00:57,420
Donation and the background.

14
00:00:59,260 --> 00:01:04,070
You can see in the article we use
words from all these distributions.

15
00:01:05,150 --> 00:01:09,540
So we first for example see there's
a criticism of government response and

16
00:01:09,540 --> 00:01:14,740
this is followed by discussion of flooding
of the city and donation et cetera.

17
00:01:14,740 --> 00:01:17,440
We also see background
words mixed with them.

18
00:01:18,840 --> 00:01:23,740
So the overall of topic analysis here
is to try to decode these topics behind

19
00:01:23,740 --> 00:01:28,250
the text, to segment the topics,
to figure out which words are from which

20
00:01:28,250 --> 00:01:33,820
distribution and to figure out first,
what are these topics?

21
00:01:33,820 --> 00:01:36,420
How do we know there's a topic
about government response.

22
00:01:36,420 --> 00:01:39,020
There's a topic about a flood in the city.

23
00:01:39,020 --> 00:01:41,850
So these are the tasks
at the top of the model.

24
00:01:42,870 --> 00:01:46,110
If we had discovered these
topics can color these words,

25
00:01:46,110 --> 00:01:50,030
as you see here,
to separate the different topics.

26
00:01:50,030 --> 00:01:54,390
Then you can do a lot of things,
such as summarization, or segmentation,

27
00:01:54,390 --> 00:01:59,800
of the topics,
clustering of the sentences etc.

28
00:01:59,800 --> 00:02:04,220
So the formal definition of problem of
mining multiple topics from text is

29
00:02:04,220 --> 00:02:04,870
shown here.

30
00:02:04,870 --> 00:02:09,270
And this is after a slide that you
have seen in an earlier lecture.

31
00:02:09,270 --> 00:02:14,100
So the input is a collection, the number
of topics, and a vocabulary set, and

32
00:02:14,100 --> 00:02:15,060
of course the text data.

33
00:02:16,300 --> 00:02:18,760
And then the output is of two kinds.

34
00:02:18,760 --> 00:02:21,720
One is the topic category,
characterization.

35
00:02:21,720 --> 00:02:22,520
Theta i's.

36
00:02:22,520 --> 00:02:24,790
Each theta i is a word distribution.

37
00:02:24,790 --> 00:02:28,160
And second, it's the topic coverage for
each document.

38
00:02:28,160 --> 00:02:30,130
These are pi sub i j's.

39
00:02:30,130 --> 00:02:33,490
And they tell us which document it covers.

40
00:02:33,490 --> 00:02:35,440
Which topic to what extent.

41
00:02:35,440 --> 00:02:37,960
So we hope to generate these as output.

42
00:02:37,960 --> 00:02:41,350
Because there are many useful
applications if we can do that.

43
00:02:42,880 --> 00:02:47,100
So the idea of PLSA is
actually very similar to

44
00:02:47,100 --> 00:02:50,660
the two component mixture model
that we have already introduced.

45
00:02:50,660 --> 00:02:54,760
The only difference is that we
are going to have more than two topics.

46
00:02:54,760 --> 00:02:57,960
Otherwise, it is essentially the same.

47
00:02:57,960 --> 00:03:03,730
So here I illustrate how we can generate
the text that has multiple topics and

48
00:03:03,730 --> 00:03:06,490
naturally in all cases

49
00:03:06,490 --> 00:03:11,310
of Probabilistic modelling would want
to figure out the likelihood function.

50
00:03:11,310 --> 00:03:13,400
So we would also ask the question,

51
00:03:13,400 --> 00:03:18,200
what's the probability of observing
a word from such a mixture model?

52
00:03:18,200 --> 00:03:19,470
Now if you look at this picture and

53
00:03:19,470 --> 00:03:21,840
compare this with the picture
that we have seen earlier,

54
00:03:21,840 --> 00:03:25,580
you will see the only difference is
that we have added more topics here.

55
00:03:26,940 --> 00:03:32,900
So, before we have just one topic,
besides the background topic.

56
00:03:32,900 --> 00:03:35,990
But now we have more topics.

57
00:03:35,990 --> 00:03:38,260
Specifically, we have k topics now.

58
00:03:38,260 --> 00:03:43,930
All these are topics that we assume
that exist in the text data.

59
00:03:43,930 --> 00:03:49,450
So the consequence is that our switch for
choosing a topic is now a multiway switch.

60
00:03:49,450 --> 00:03:51,210
Before it's just a two way switch.

61
00:03:51,210 --> 00:03:53,420
We can think of it as flipping a coin.

62
00:03:53,420 --> 00:03:55,110
But now we have multiple ways.

63
00:03:55,110 --> 00:03:59,660
First we can flip a coin to decide
whether we're talk about the background.

64
00:03:59,660 --> 00:04:06,913
So it's the background lambda
sub B versus non-background.

65
00:04:06,913 --> 00:04:11,490
1 minus lambda sub B gives
us the probability of

66
00:04:11,490 --> 00:04:16,300
actually choosing a non-background topic.

67
00:04:16,300 --> 00:04:17,860
After we have made this decision,

68
00:04:17,860 --> 00:04:24,750
we have to make another decision to
choose one of these K distributions.

69
00:04:24,750 --> 00:04:26,480
So there are K way switch here.

70
00:04:26,480 --> 00:04:30,120
And this is characterized by pi,
and this sum to one.

71
00:04:31,450 --> 00:04:33,775
This is just the difference of designs.

72
00:04:33,775 --> 00:04:36,745
Which is a little bit more complicated.

73
00:04:36,745 --> 00:04:40,655
But once we decide which distribution to
use the rest is the same we are going to

74
00:04:40,655 --> 00:04:45,145
just generate a word by using one of
these distributions as shown here.

75
00:04:46,885 --> 00:04:50,920
So now lets look at the question
about the likelihood.

76
00:04:50,920 --> 00:04:55,780
So what's the probability of observing
a word from such a distribution?

77
00:04:55,780 --> 00:04:57,250
What do you think?

78
00:04:57,250 --> 00:05:01,150
Now we've seen this
problem many times now and

79
00:05:01,150 --> 00:05:05,210
if you can recall, it's generally a sum.

80
00:05:05,210 --> 00:05:08,540
Of all the different possibilities
of generating a word.

81
00:05:08,540 --> 00:05:14,260
So let's first look at how the word can
be generated from the background mode.

82
00:05:14,260 --> 00:05:18,340
Well, the probability that the word is
generated from the background model

83
00:05:18,340 --> 00:05:22,700
is lambda multiplied by the probability
of the word from the background mode.

84
00:05:22,700 --> 00:05:24,200
Model, right.

85
00:05:24,200 --> 00:05:25,150
Two things must happen.

86
00:05:25,150 --> 00:05:28,270
First, we have to have
chosen the background model,

87
00:05:28,270 --> 00:05:31,730
and that's the probability of lambda,
of sub b.

88
00:05:31,730 --> 00:05:36,330
Then second, we must have actually
obtained the word w from the background,

89
00:05:36,330 --> 00:05:39,161
and that's probability
of w given theta sub b.

90
00:05:40,220 --> 00:05:41,790
Okay, so similarly,

91
00:05:41,790 --> 00:05:46,020
we can figure out the probability of
observing the word from another topic.

92
00:05:46,020 --> 00:05:48,530
Like the topic theta sub k.

93
00:05:48,530 --> 00:05:51,890
Now notice that here's
the product of three terms.

94
00:05:51,890 --> 00:05:57,023
And that's because of the choice
of topic theta sub k,

95
00:05:57,023 --> 00:06:00,630
only happens if two things happen.

96
00:06:00,630 --> 00:06:04,020
One is we decide not to
talk about background.

97
00:06:04,020 --> 00:06:07,630
So, that's a probability
of 1 minus lambda sub B.

98
00:06:07,630 --> 00:06:13,290
Second, we also have to actually choose
theta sub K among these K topics.

99
00:06:13,290 --> 00:06:16,000
So that's probability of theta sub K,
or pi.

100
00:06:17,900 --> 00:06:21,460
And similarly, the probability of
generating a word from the second.

101
00:06:21,460 --> 00:06:26,480
The topic and the first topic
are like what you are seeing here.

102
00:06:26,480 --> 00:06:27,250
And so

103
00:06:27,250 --> 00:06:32,480
in the end the probability of observing
the word is just a sum of all these cases.

104
00:06:32,480 --> 00:06:38,080
And I have to stress again this is a very
important formula to know because this is

105
00:06:38,080 --> 00:06:44,150
really key to understanding all the topic
models and indeed a lot of mixture models.

106
00:06:44,150 --> 00:06:47,410
So make sure that you really
understand the probability

107
00:06:49,410 --> 00:06:53,390
of w is indeed the sum of these terms.

108
00:06:56,540 --> 00:07:00,620
So, next,
once we have the likelihood function,

109
00:07:00,620 --> 00:07:05,250
we would be interested in
knowing the parameters.

110
00:07:05,250 --> 00:07:07,250
All right, so to estimate the parameters.

111
00:07:07,250 --> 00:07:07,760
But firstly,

112
00:07:07,760 --> 00:07:13,510
let's put all these together to have the
complete likelihood of function for PLSA.

113
00:07:13,510 --> 00:07:19,010
The first line shows the probability of a
word as illustrated on the previous slide.

114
00:07:19,010 --> 00:07:20,980
And this is an important
formula as I said.

115
00:07:22,560 --> 00:07:24,250
So let's take a closer look at this.

116
00:07:24,250 --> 00:07:27,430
This actually commands all
the important parameters.

117
00:07:27,430 --> 00:07:29,280
So first of all we see lambda sub b here.

118
00:07:29,280 --> 00:07:31,539
This represents a percentage
of background words

119
00:07:32,610 --> 00:07:35,560
that we believe exist in the text data.

120
00:07:35,560 --> 00:07:39,220
And this can be a known value
that we set empirically.

121
00:07:41,180 --> 00:07:43,380
Second, we see the background
language model, and

122
00:07:43,380 --> 00:07:45,210
typically we also assume this is known.

123
00:07:45,210 --> 00:07:48,000
We can use a large collection of text, or

124
00:07:48,000 --> 00:07:51,780
use all the text that we have available
to estimate the world of distribution.

125
00:07:52,890 --> 00:07:55,008
Now next in the next stop this formula.

126
00:07:55,008 --> 00:07:57,960
[COUGH] Excuse me.

127
00:07:57,960 --> 00:08:00,160
You see two interesting
kind of parameters,

128
00:08:00,160 --> 00:08:01,886
those are the most important parameters.

129
00:08:01,886 --> 00:08:04,690
That we are.

130
00:08:04,690 --> 00:08:06,190
So one is pi's.

131
00:08:06,190 --> 00:08:10,060
And these are the coverage
of a topic in the document.

132
00:08:11,280 --> 00:08:15,310
And the other is word distributions
that characterize all the topics.

133
00:08:18,530 --> 00:08:23,780
So the next line,
then is simply to plug this

134
00:08:23,780 --> 00:08:26,280
in to calculate
the probability of document.

135
00:08:26,280 --> 00:08:29,720
This is, again, of the familiar
form where you have a sum and

136
00:08:29,720 --> 00:08:32,050
you have a count of
a word in the document.

137
00:08:32,050 --> 00:08:35,100
And then log of a probability.

138
00:08:35,100 --> 00:08:39,040
Now it's a little bit more
complicated than the two component.

139
00:08:39,040 --> 00:08:43,890
Because now we have more components,
so the sum involves more terms.

140
00:08:43,890 --> 00:08:47,750
And then this line is just
the likelihood for the whole collection.

141
00:08:47,750 --> 00:08:51,130
And it's very similar, just accounting for
more documents in the collection.

142
00:08:52,470 --> 00:08:54,060
So what are the unknown parameters?

143
00:08:54,060 --> 00:08:55,960
I already said that there are two kinds.

144
00:08:55,960 --> 00:08:59,150
One is coverage,
one is word distributions.

145
00:08:59,150 --> 00:09:02,350
Again, it's a useful exercise for
you to think about.

146
00:09:02,350 --> 00:09:04,730
Exactly how many
parameters there are here.

147
00:09:05,750 --> 00:09:07,940
How many unknown parameters are there?

148
00:09:07,940 --> 00:09:08,680
Now, try and

149
00:09:08,680 --> 00:09:13,090
think out that question will help you
understand the model in more detail.

150
00:09:13,090 --> 00:09:17,760
And will also allow you to understand
what would be the output that we generate

151
00:09:17,760 --> 00:09:20,430
when use PLSA to analyze text data?

152
00:09:20,430 --> 00:09:22,480
And these are precisely
the unknown parameters.

153
00:09:24,480 --> 00:09:28,200
So after we have obtained
the likelihood function shown here,

154
00:09:28,200 --> 00:09:30,820
the next is to worry about
the parameter estimation.

155
00:09:32,050 --> 00:09:34,770
And we can do the usual think,
maximum likelihood estimator.

156
00:09:34,770 --> 00:09:40,190
So again, it's a constrained optimization
problem, like what we have seen before.

157
00:09:40,190 --> 00:09:44,350
Only that we have a collection of text and
we have more parameters to estimate.

158
00:09:44,350 --> 00:09:48,655
And we still have two constraints,
two kinds of constraints.

159
00:09:48,655 --> 00:09:50,145
One is the word distributions.

160
00:09:51,245 --> 00:09:56,525
All the words must have probabilities
that's sum to one for one distribution.

161
00:09:56,525 --> 00:09:59,975
The other is the topic
coverage distribution and

162
00:09:59,975 --> 00:10:05,200
a document will have to cover
precisely these k topics so

163
00:10:05,200 --> 00:10:08,820
the probability of covering each
topic that would have to sum to 1.

164
00:10:08,820 --> 00:10:13,190
So at this point though it's basically
a well defined applied math problem,

165
00:10:13,190 --> 00:10:16,370
you just need to figure out
the solutions to optimization problem.

166
00:10:16,370 --> 00:10:18,670
There's a function with many variables.

167
00:10:18,670 --> 00:10:22,481
and we need to just figure
out the patterns of these

168
00:10:22,481 --> 00:10:26,397
variables to make the function
reach its maximum.

169
00:10:26,397 --> 00:10:36,397
>> [MUSIC]


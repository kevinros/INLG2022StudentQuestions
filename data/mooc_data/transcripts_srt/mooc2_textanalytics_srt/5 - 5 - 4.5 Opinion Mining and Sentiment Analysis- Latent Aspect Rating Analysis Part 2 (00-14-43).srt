1
00:00:00,025 --> 00:00:05,363
[SOUND] This lecture is a continued

2
00:00:05,363 --> 00:00:12,997
discussion of
Latent Aspect Rating Analysis.

3
00:00:12,997 --> 00:00:18,440
Earlier, we talked about how to solve
the problem of LARA in two stages.

4
00:00:18,440 --> 00:00:22,440
But we first do segmentation
of different aspects.

5
00:00:22,440 --> 00:00:26,856
And then we use a latent regression
model to learn the aspect ratings and

6
00:00:26,856 --> 00:00:28,334
then later the weight.

7
00:00:28,334 --> 00:00:33,372
Now it's also possible to develop
a unified generative model for

8
00:00:33,372 --> 00:00:35,478
solving this problem, and

9
00:00:35,478 --> 00:00:41,360
that is we not only model the generational
over-rating based on text.

10
00:00:41,360 --> 00:00:45,030
We also model the generation of text,
and so

11
00:00:45,030 --> 00:00:47,410
a natural solution would
be to use topic model.

12
00:00:47,410 --> 00:00:49,350
So given the entity,

13
00:00:49,350 --> 00:00:54,700
we can assume there are aspects that
are described by word distributions.

14
00:00:54,700 --> 00:00:55,830
Topics.

15
00:00:55,830 --> 00:01:00,550
And then we an use a topic model to model
the generation of the reviewed text.

16
00:01:01,592 --> 00:01:07,045
I will assume words in the review text
are drawn from these distributions.

17
00:01:08,475 --> 00:01:12,265
In the same way as we assumed for
generating model like PRSA.

18
00:01:13,605 --> 00:01:18,470
And then we can then plug in
the latent regression model to

19
00:01:18,470 --> 00:01:23,710
use the text to further
predict the overrating.

20
00:01:23,710 --> 00:01:26,220
And that means when we first
predict the aspect rating and

21
00:01:26,220 --> 00:01:30,520
then combine them with aspect weights
to predict the overall rating.

22
00:01:30,520 --> 00:01:34,280
So this would give us
a unified generated model,

23
00:01:34,280 --> 00:01:39,870
where we model both the generation of text
and the overall ready condition on text.

24
00:01:40,910 --> 00:01:46,150
So we don't have time to discuss
this model in detail as in

25
00:01:46,150 --> 00:01:51,990
many other cases in this part of the cause
where we discuss the cutting edge topics,

26
00:01:51,990 --> 00:01:55,940
but there's a reference site here
where you can find more details.

27
00:01:57,130 --> 00:02:00,070
So now I'm going to show you some
simple results that you can get

28
00:02:00,070 --> 00:02:02,760
by using these kind of generated models.

29
00:02:02,760 --> 00:02:05,450
First, it's about rating decomposition.

30
00:02:05,450 --> 00:02:09,070
So here, what you see
are the decomposed ratings for

31
00:02:09,070 --> 00:02:13,100
three hotels that have
the same overall rating.

32
00:02:13,100 --> 00:02:15,570
So if you just look at the overall rating,

33
00:02:15,570 --> 00:02:18,980
you can't really tell much
difference between these hotels.

34
00:02:18,980 --> 00:02:24,270
But by decomposing these
ratings into aspect ratings

35
00:02:24,270 --> 00:02:28,559
we can see some hotels have higher
ratings for some dimensions,

36
00:02:28,559 --> 00:02:33,580
like value, but others might score better
in other dimensions, like location.

37
00:02:33,580 --> 00:02:37,680
And so this can give you detailed
opinions at the aspect level.

38
00:02:38,750 --> 00:02:42,940
Now here, the ground-truth is
shown in the parenthesis, so

39
00:02:42,940 --> 00:02:46,530
it also allows you to see whether
the prediction is accurate.

40
00:02:46,530 --> 00:02:52,490
It's not always accurate but It's mostly
still reflecting some of the trends.

41
00:02:53,490 --> 00:02:58,880
The second result you compare
different reviewers on the same hotel.

42
00:02:58,880 --> 00:03:05,490
So the table shows the decomposed ratings
for two reviewers about same hotel.

43
00:03:05,490 --> 00:03:08,470
Again their high level
overall ratings are the same.

44
00:03:08,470 --> 00:03:13,440
So if you just look at the overall
ratings, you don't really get that much

45
00:03:13,440 --> 00:03:15,730
information about the difference
between the two reviewers.

46
00:03:15,730 --> 00:03:17,360
But after you decompose the ratings,

47
00:03:17,360 --> 00:03:21,870
you can see clearly that they have
high scores on different dimensions.

48
00:03:21,870 --> 00:03:26,030
So this shows that model can review
differences in opinions of different

49
00:03:26,030 --> 00:03:30,260
reviewers and such a detailed
understanding can help us understand

50
00:03:30,260 --> 00:03:35,960
better about reviewers and also better
about their feedback on the hotel.

51
00:03:35,960 --> 00:03:38,420
This is something very interesting,

52
00:03:38,420 --> 00:03:40,800
because this is in some
sense some byproduct.

53
00:03:40,800 --> 00:03:43,900
In our problem formulation,
we did not really have to do this.

54
00:03:43,900 --> 00:03:47,770
But the design of the generating
model has this component.

55
00:03:47,770 --> 00:03:52,930
And these are sentimental weights for
words in different aspects.

56
00:03:52,930 --> 00:03:58,750
And you can see the highly weighted words
versus the negatively loaded weighted

57
00:03:58,750 --> 00:04:01,190
words here for
each of the four dimensions.

58
00:04:01,190 --> 00:04:05,310
Value, rooms, location, and cleanliness.

59
00:04:05,310 --> 00:04:08,990
The top words clearly make sense, and
the bottom words also make sense.

60
00:04:10,230 --> 00:04:12,540
So this shows that with this approach,

61
00:04:12,540 --> 00:04:16,300
we can also learn sentiment
information directly from the data.

62
00:04:16,300 --> 00:04:21,410
Now, this kind of lexicon is very useful
because in general, a word like long,

63
00:04:21,410 --> 00:04:26,240
let's say, may have different sentiment
polarities for different context.

64
00:04:26,240 --> 00:04:31,270
So if I say the battery life of this
laptop is long, then that's positive.

65
00:04:31,270 --> 00:04:36,440
But if I say the rebooting time for
the laptop is long, that's bad, right?

66
00:04:36,440 --> 00:04:40,210
So even for
reviews about the same product, laptop,

67
00:04:40,210 --> 00:04:46,010
the word long is ambiguous, it could
mean positive or it could mean negative.

68
00:04:46,010 --> 00:04:50,500
But this kind of lexicon, that we can
learn by using this kind of generated

69
00:04:50,500 --> 00:04:55,810
models, can show whether a word is
positive for a particular aspect.

70
00:04:55,810 --> 00:05:01,590
So this is clearly very useful, and in
fact such a lexicon can be directly used

71
00:05:01,590 --> 00:05:04,810
to tag other reviews about hotels or

72
00:05:04,810 --> 00:05:07,890
tag comments about hotels in
social media like Tweets.

73
00:05:08,910 --> 00:05:15,030
And what's also interesting is that since
this is almost completely unsupervised,

74
00:05:15,030 --> 00:05:20,450
well assuming the reviews whose
overall rating are available And

75
00:05:20,450 --> 00:05:24,400
then this can allow us to learn form
potentially larger amount of data on

76
00:05:24,400 --> 00:05:27,070
the internet to reach sentiment lexicon.

77
00:05:28,190 --> 00:05:31,530
And here are some results to
validate the preference words.

78
00:05:31,530 --> 00:05:36,165
Remember the model can infer wether
a reviewer cares more about service or

79
00:05:36,165 --> 00:05:37,550
the price.

80
00:05:37,550 --> 00:05:41,380
Now how do we know whether
the inferred weights are correct?

81
00:05:41,380 --> 00:05:45,500
And this poses a very difficult
challenge for evaluation.

82
00:05:45,500 --> 00:05:49,480
Now here we show some
interesting way of evaluating.

83
00:05:50,840 --> 00:05:55,030
What you see here are the prices
of hotels in different cities, and

84
00:05:55,030 --> 00:06:01,010
these are the prices of hotels that are
favored by different groups of reviewers.

85
00:06:01,010 --> 00:06:04,400
The top ten are the reviewers
was the highest

86
00:06:04,400 --> 00:06:08,460
inferred value to other aspect ratio.

87
00:06:09,600 --> 00:06:13,840
So for example value versus location,
value versus room, etcetera.

88
00:06:13,840 --> 00:06:20,110
Now the top ten of the reviewers that
have the highest ratios by this measure.

89
00:06:20,110 --> 00:06:23,210
And that means these reviewers
tend to put a lot of

90
00:06:23,210 --> 00:06:26,120
weight on value as compared
with other dimensions.

91
00:06:26,120 --> 00:06:28,760
So that means they really
emphasize on value.

92
00:06:30,430 --> 00:06:32,950
The bottom ten on the other
hand of the reviewers.

93
00:06:32,950 --> 00:06:34,610
The lowest ratio, what does that mean?

94
00:06:34,610 --> 00:06:39,420
Well it means these reviewers have
put higher weights on other aspects

95
00:06:39,420 --> 00:06:41,110
than value.

96
00:06:41,110 --> 00:06:46,740
So those are people that cared about
another dimension and they didn't care so

97
00:06:46,740 --> 00:06:51,450
much the value in some sense, at least
as compared with the top ten group.

98
00:06:52,470 --> 00:06:56,610
Now these ratios are computer based on
the inferred weights from the model.

99
00:06:57,820 --> 00:07:02,020
So now you can see the average prices
of hotels favored by top ten reviewers

100
00:07:02,020 --> 00:07:07,360
are indeed much cheaper than those
that are favored by the bottom ten.

101
00:07:07,360 --> 00:07:14,720
And this provides some indirect way
of validating the inferred weights.

102
00:07:14,720 --> 00:07:16,950
It just means the weights are not random.

103
00:07:16,950 --> 00:07:19,430
They are actually meaningful here.

104
00:07:19,430 --> 00:07:22,570
In comparison,
the average price in these three cities,

105
00:07:22,570 --> 00:07:26,940
you can actually see the top ten
tend to have below average in price,

106
00:07:26,940 --> 00:07:30,780
whereas the bottom half, where they care
a lot about other things like a service or

107
00:07:30,780 --> 00:07:36,870
room condition tend to have hotels
that have higher prices than average.

108
00:07:36,870 --> 00:07:40,590
So with these results we can build
a lot of interesting applications.

109
00:07:40,590 --> 00:07:45,098
For example, a direct application would be
to generate the rated aspect, the summary,

110
00:07:45,098 --> 00:07:48,920
and because of the decomposition we
have now generated the summaries for

111
00:07:48,920 --> 00:07:49,970
each aspect.

112
00:07:49,970 --> 00:07:54,050
The positive sentences the negative
sentences about each aspect.

113
00:07:54,050 --> 00:07:57,750
It's more informative than original review
that just has an overall rating and

114
00:07:57,750 --> 00:07:58,280
review text.

115
00:07:58,280 --> 00:08:01,940
Here are some other results

116
00:08:01,940 --> 00:08:06,990
about the aspects that's covered
from reviews with no ratings.

117
00:08:06,990 --> 00:08:08,810
These are mp3 reviews,

118
00:08:08,810 --> 00:08:13,910
and these results show that the model
can discover some interesting aspects.

119
00:08:13,910 --> 00:08:18,320
Commented on low overall ratings versus
those higher overall per ratings.

120
00:08:18,320 --> 00:08:21,550
And they care more about
the different aspects.

121
00:08:22,590 --> 00:08:25,790
Or they comment more on
the different aspects.

122
00:08:25,790 --> 00:08:29,796
So that can help us discover for
example, consumers'

123
00:08:29,796 --> 00:08:34,460
trend in appreciating different
features of products.

124
00:08:34,460 --> 00:08:39,980
For example, one might have discovered
the trend that people tend to

125
00:08:39,980 --> 00:08:45,550
like larger screens of cell phones or
light weight of laptop, etcetera.

126
00:08:45,550 --> 00:08:49,960
Such knowledge can be useful for

127
00:08:49,960 --> 00:08:56,150
manufacturers to design their
next generation of products.

128
00:08:56,150 --> 00:09:01,020
Here are some interesting results
on analyzing users rating behavior.

129
00:09:01,020 --> 00:09:04,650
So what you see is average weights

130
00:09:04,650 --> 00:09:09,470
along different dimensions by
different groups of reviewers.

131
00:09:09,470 --> 00:09:16,950
And on the left side you see the weights
of viewers that like the expensive hotels.

132
00:09:16,950 --> 00:09:21,000
They gave the expensive hotels 5 Stars,
and

133
00:09:21,000 --> 00:09:24,810
you can see their average rates
tend to be more for some service.

134
00:09:24,810 --> 00:09:29,520
And that suggests that people like
expensive hotels because of good service,

135
00:09:29,520 --> 00:09:30,990
and that's not surprising.

136
00:09:30,990 --> 00:09:33,620
That's also another way to
validate it by inferred weights.

137
00:09:34,800 --> 00:09:40,330
If you look at the right side where,
look at the column of 5 Stars.

138
00:09:40,330 --> 00:09:43,460
These are the reviewers that
like the cheaper hotels, and

139
00:09:43,460 --> 00:09:45,770
they gave cheaper hotels five stars.

140
00:09:45,770 --> 00:09:48,600
As we expected and
they put more weight on value,

141
00:09:48,600 --> 00:09:51,110
and that's why they like
the cheaper hotels.

142
00:09:52,570 --> 00:09:56,770
But if you look at the, when they didn't
like expensive hotels, or cheaper hotels,

143
00:09:56,770 --> 00:10:00,600
then you'll see that they tended to
have more weights on the condition of

144
00:10:00,600 --> 00:10:03,070
the room cleanness.

145
00:10:04,210 --> 00:10:08,840
So this shows that by using this model,
we can infer some

146
00:10:08,840 --> 00:10:13,900
information that's very hard to obtain
even if you read all the reviews.

147
00:10:13,900 --> 00:10:18,740
Even if you read all the reviews it's
very hard to infer such preferences or

148
00:10:18,740 --> 00:10:20,890
such emphasis.

149
00:10:20,890 --> 00:10:24,450
So this is a case where text mining
algorithms can go beyond what

150
00:10:24,450 --> 00:10:27,440
humans can do, to review
interesting patterns in the data.

151
00:10:27,440 --> 00:10:29,900
And this of course can be very useful.

152
00:10:29,900 --> 00:10:32,340
You can compare different hotels,

153
00:10:32,340 --> 00:10:37,550
compare the opinions from different
consumer groups, in different locations.

154
00:10:37,550 --> 00:10:39,430
And of course, the model is general.

155
00:10:39,430 --> 00:10:43,270
It can be applied to any
reviews with overall ratings.

156
00:10:43,270 --> 00:10:45,870
So this is a very useful
technique that can

157
00:10:45,870 --> 00:10:47,970
support a lot of text mining applications.

158
00:10:50,250 --> 00:10:54,830
Finally the results of applying this
model for personalized ranking or

159
00:10:54,830 --> 00:10:56,090
recommendation of entities.

160
00:10:57,790 --> 00:11:02,270
So because we can infer the reviewers
weights on different dimensions,

161
00:11:02,270 --> 00:11:06,240
we can allow a user to actually
say what do you care about.

162
00:11:06,240 --> 00:11:09,550
So for example, I have a query
here that shows 90% of the weight

163
00:11:09,550 --> 00:11:12,930
should be on value and 10% on others.

164
00:11:12,930 --> 00:11:15,180
So that just means I don't
care about other aspect.

165
00:11:15,180 --> 00:11:17,620
I just care about getting a cheaper hotel.

166
00:11:17,620 --> 00:11:21,450
My emphasis is on the value dimension.

167
00:11:21,450 --> 00:11:26,310
Now what we can do with such query
is we can use reviewers that we

168
00:11:26,310 --> 00:11:31,000
believe have a similar preference
to recommend a hotels for you.

169
00:11:31,000 --> 00:11:31,860
How can we know that?

170
00:11:31,860 --> 00:11:36,525
Well, we can infer the weights of
those reviewers on different aspects.

171
00:11:36,525 --> 00:11:40,325
We can find the reviewers whose
weights are more precise,

172
00:11:40,325 --> 00:11:43,795
of course inferred rates
are similar to yours.

173
00:11:43,795 --> 00:11:46,885
And then use those reviewers to
recommend hotels for you and

174
00:11:46,885 --> 00:11:51,212
this is what we call personalized or
rather query specific recommendations.

175
00:11:51,212 --> 00:11:56,030
Now the non-personalized
recommendations now shown on the top,

176
00:11:56,030 --> 00:12:01,870
and you can see the top results generally
have much higher price, than the lower

177
00:12:01,870 --> 00:12:06,250
group and that's because when the
reviewer's cared more about the value as

178
00:12:06,250 --> 00:12:12,760
dictated by this query they tended
to really favor low price hotels.

179
00:12:12,760 --> 00:12:16,860
So this is yet
another application of this technique.

180
00:12:18,280 --> 00:12:22,220
It shows that by doing text mining
we can understand the users better.

181
00:12:22,220 --> 00:12:25,570
And once we can handle users better
we can solve these users better.

182
00:12:25,570 --> 00:12:28,790
So to summarize our discussion
of opinion mining in general,

183
00:12:28,790 --> 00:12:31,650
this is a very important topic and
with a lot of applications.

184
00:12:33,220 --> 00:12:37,780
And as a text sentiment
analysis can be readily done by

185
00:12:37,780 --> 00:12:39,280
using just text categorization.

186
00:12:39,280 --> 00:12:41,430
But standard technique
tends to not be enough.

187
00:12:41,430 --> 00:12:44,020
And so we need to have enriched
feature implementation.

188
00:12:45,020 --> 00:12:48,410
And we also need to consider
the order of those categories.

189
00:12:48,410 --> 00:12:52,630
And we'll talk about ordinal
regression for some of these problem.

190
00:12:52,630 --> 00:12:55,580
We have also assume that
the generating models are powerful for

191
00:12:55,580 --> 00:12:57,110
mining latent user preferences.

192
00:12:57,110 --> 00:13:02,120
This in particular in the generative
model for mining latent regression.

193
00:13:02,120 --> 00:13:05,560
And we embed some interesting
preference information and

194
00:13:05,560 --> 00:13:09,660
send the weights of words in the model
as a result we can learn most

195
00:13:09,660 --> 00:13:13,820
useful information when
fitting the model to the data.

196
00:13:13,820 --> 00:13:16,960
Now most approaches have been proposed and
evaluated.

197
00:13:16,960 --> 00:13:21,910
For product reviews, and that was because
in such a context, the opinion holder and

198
00:13:21,910 --> 00:13:23,790
the opinion target are clear.

199
00:13:23,790 --> 00:13:26,220
And they are easy to analyze.

200
00:13:26,220 --> 00:13:29,710
And there, of course,
also have a lot of practical applications.

201
00:13:29,710 --> 00:13:35,800
But opinion mining from news and
social media is also important, but that's

202
00:13:35,800 --> 00:13:40,980
more difficult than analyzing review data,
mainly because the opinion holders and

203
00:13:40,980 --> 00:13:45,220
opinion targets are all interested.

204
00:13:45,220 --> 00:13:46,610
So that calls for

205
00:13:46,610 --> 00:13:49,790
natural management processing
techniques to uncover them accurately.

206
00:13:50,990 --> 00:13:53,390
Here are some suggested readings.

207
00:13:53,390 --> 00:13:59,790
The first two are small books that
are of some use of this topic,

208
00:13:59,790 --> 00:14:04,370
where you can find a lot of discussion
about other variations of the problem and

209
00:14:04,370 --> 00:14:07,050
techniques proposed for
solving the problem.

210
00:14:08,280 --> 00:14:12,430
The next two papers about
generating models for

211
00:14:12,430 --> 00:14:14,330
rating the aspect rating analysis.

212
00:14:14,330 --> 00:14:18,456
The first one is about solving
the problem using two stages, and

213
00:14:18,456 --> 00:14:23,194
the second one is about a unified model
where the topic model is integrated

214
00:14:23,194 --> 00:14:27,726
with the regression model to solve
the problem using a unified model.

215
00:14:30,977 --> 00:14:40,977
[MUSIC]


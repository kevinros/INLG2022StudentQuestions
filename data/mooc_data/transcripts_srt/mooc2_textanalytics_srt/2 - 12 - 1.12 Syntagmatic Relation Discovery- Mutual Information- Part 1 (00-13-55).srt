1
00:00:00,025 --> 00:00:07,457
[SOUND].

2
00:00:07,457 --> 00:00:11,800
This lecture is about the syntagmatic
relation discovery and mutual information.

3
00:00:13,400 --> 00:00:18,196
In this lecture we are going to continue
discussing syntagmatic relation discovery.

4
00:00:18,196 --> 00:00:20,850
In particular,
we are going to talk about another

5
00:00:20,850 --> 00:00:24,880
the concept in the information series,
we called it mutual information and

6
00:00:24,880 --> 00:00:28,760
how it can be used to discover
syntagmatic relations.

7
00:00:28,760 --> 00:00:32,880
Before we talked about the problem
of conditional entropy and

8
00:00:32,880 --> 00:00:38,014
that is the conditional entropy
computed different pairs of words.

9
00:00:38,014 --> 00:00:42,600
It is not really comparable, so
that makes it harder with this cover,

10
00:00:42,600 --> 00:00:48,360
strong synagmatic relations
globally from corpus.

11
00:00:48,360 --> 00:00:53,050
So now we are going to introduce mutual
information, which is another concept

12
00:00:53,050 --> 00:00:57,370
in the information series
that allows us to, sometimes,

13
00:00:57,370 --> 00:01:03,460
normalize the conditional entropy to make
it more comparable across different pairs.

14
00:01:04,930 --> 00:01:10,090
In particular, mutual information
in order to find I(X:Y),

15
00:01:10,090 --> 00:01:17,380
matches the entropy reduction
of X obtained from knowing Y.

16
00:01:17,380 --> 00:01:22,270
More specifically the question we
are interested in here is how much

17
00:01:22,270 --> 00:01:25,463
of an entropy of X can
we obtain by knowing Y.

18
00:01:27,220 --> 00:01:31,940
So mathematically it can be
defined as the difference between

19
00:01:31,940 --> 00:01:36,670
the original entropy of X, and
the condition of Y of X given Y.

20
00:01:37,970 --> 00:01:42,730
And you might see,
as you can see here it can also be defined

21
00:01:42,730 --> 00:01:47,790
as reduction of entropy of
Y because of knowing X.

22
00:01:48,930 --> 00:01:54,070
Now normally the two conditional
interface H of X given Y and

23
00:01:54,070 --> 00:01:58,240
the entropy of Y given X are not equal,
but interestingly,

24
00:01:58,240 --> 00:02:05,476
the reduction of entropy by knowing
one of them, is actually equal.

25
00:02:05,476 --> 00:02:12,805
So, this quantity is called a Mutual
Information in order to buy I here.

26
00:02:12,805 --> 00:02:17,085
And this function has some interesting
properties, first it is also non-negative.

27
00:02:17,085 --> 00:02:21,415
This is easy to understand because
the original entropy is always

28
00:02:22,782 --> 00:02:29,132
not going to be lower than the possibility
reduced conditional entropy.

29
00:02:29,132 --> 00:02:33,512
In other words, the conditional entropy
will never exceed the original entropy.

30
00:02:33,512 --> 00:02:37,784
Knowing some information can
always help us potentially, but

31
00:02:37,784 --> 00:02:40,282
will not hurt us in predicting x.

32
00:02:41,510 --> 00:02:46,375
The signal property is that it
is symmetric like additional

33
00:02:46,375 --> 00:02:51,142
entropy is not symmetrical,
mutual information is, and

34
00:02:51,142 --> 00:02:56,394
the third property is that It
reaches its minimum, zero, if and

35
00:02:56,394 --> 00:03:01,580
only if the two random variables
are completely independent.

36
00:03:01,580 --> 00:03:07,949
That means knowing one of them does not
tell us anything about the other and

37
00:03:07,949 --> 00:03:14,626
this last property can be verified by
simply looking at the equation above and

38
00:03:14,626 --> 00:03:19,144
it reaches 0 if and
only the conditional entropy of X

39
00:03:19,144 --> 00:03:24,102
[INAUDIBLE] Y is exactly the same
as original entropy of X.

40
00:03:24,102 --> 00:03:28,344
So that means knowing why it did not
help at all and that is when X and

41
00:03:28,344 --> 00:03:30,520
a Y are completely independent.

42
00:03:32,120 --> 00:03:37,880
Now when we fix X to rank different
Ys using conditional entropy

43
00:03:37,880 --> 00:03:44,180
would give the same order as
ranking based on mutual information

44
00:03:44,180 --> 00:03:49,940
because in the function here,
H(X) is fixed because X is fixed.

45
00:03:49,940 --> 00:03:53,820
So ranking based on mutual entropy is
exactly the same as ranking based on

46
00:03:53,820 --> 00:03:57,600
the conditional entropy of X given Y, but

47
00:03:57,600 --> 00:04:03,058
the mutual information allows us to
compare different pairs of x and y.

48
00:04:03,058 --> 00:04:07,990
So, that is why mutual information is
more general and in general, more useful.

49
00:04:10,688 --> 00:04:14,420
So, let us examine the intuition
of using mutual information for

50
00:04:14,420 --> 00:04:15,880
Syntagmatical Relation Mining.

51
00:04:17,150 --> 00:04:20,430
Now, the question we ask forcing
that relation mining is,

52
00:04:20,430 --> 00:04:24,300
whenever "eats" occurs,
what other words also tend to occur?

53
00:04:25,610 --> 00:04:30,710
So this question can be framed as
a mutual information question, that is,

54
00:04:30,710 --> 00:04:33,055
which words have high mutual
information was eats,

55
00:04:33,055 --> 00:04:37,700
so computer the missing information
between eats and other words.

56
00:04:39,050 --> 00:04:44,520
And if we do that, and it is basically
a base on the same as conditional

57
00:04:44,520 --> 00:04:48,990
we will see that words that
are strongly associated with eats,

58
00:04:48,990 --> 00:04:50,960
will have a high point.

59
00:04:50,960 --> 00:04:55,200
Whereas words that are not related
will have lower mutual information.

60
00:04:55,200 --> 00:04:58,530
For this, I will give some example here.

61
00:04:58,530 --> 00:05:01,220
The mutual information between "eats" and
"meats",

62
00:05:01,220 --> 00:05:05,650
which is the same as between "meats" and
"eats," because the information is

63
00:05:05,650 --> 00:05:10,960
symmetrical is expected to be higher than
the mutual information between eats and

64
00:05:10,960 --> 00:05:14,638
the, because knowing the does not
really help us as a predictor.

65
00:05:14,638 --> 00:05:17,998
It is similar, and
knowing eats does not help us predicting,

66
00:05:17,998 --> 00:05:22,280
the as well.

67
00:05:22,280 --> 00:05:26,970
And you also can easily
see that the mutual

68
00:05:26,970 --> 00:05:32,030
information between a word and
itself is the largest,

69
00:05:32,030 --> 00:05:37,890
which is equal to
the entropy of this word and

70
00:05:37,890 --> 00:05:42,740
so, because in this case the reduction is

71
00:05:42,740 --> 00:05:48,530
maximum because knowing one allows
us to predict the other completely.

72
00:05:48,530 --> 00:05:50,570
So the conditional entropy is zero,

73
00:05:50,570 --> 00:05:54,472
therefore the mutual information
reaches its maximum.

74
00:05:54,472 --> 00:06:02,520
It is going to be larger, then are equal
to the machine volume eats in other words.

75
00:06:02,520 --> 00:06:05,420
In other words picking any other word and

76
00:06:05,420 --> 00:06:08,588
the computer picking between eats and
that word.

77
00:06:08,588 --> 00:06:13,511
You will not get any information larger
the computation from eats and itself.

78
00:06:16,386 --> 00:06:21,390
So now let us look at how to
compute the mute information.

79
00:06:21,390 --> 00:06:23,490
Now in order to do that, we often

80
00:06:25,110 --> 00:06:29,100
use a different form of mutual
information, and we can mathematically

81
00:06:29,100 --> 00:06:34,190
rewrite the mutual information
into the form shown on this slide.

82
00:06:34,190 --> 00:06:38,655
Where we essentially see
a formula that computes what is

83
00:06:38,655 --> 00:06:43,075
called a KL-divergence or divergence.

84
00:06:43,075 --> 00:06:45,615
This is another term
in information theory.

85
00:06:45,615 --> 00:06:48,865
It measures the divergence
between two distributions.

86
00:06:50,615 --> 00:06:54,645
Now, if you look at the formula,
it is also sum over many combinations of

87
00:06:54,645 --> 00:06:58,190
different values of the two random
variables but inside the sum,

88
00:06:58,190 --> 00:07:04,110
mainly we are doing a comparison
between two joint distributions.

89
00:07:04,110 --> 00:07:06,690
The numerator has the joint,

90
00:07:06,690 --> 00:07:11,110
actual observed the joint distribution
of the two random variables.

91
00:07:12,690 --> 00:07:15,720
The bottom part or the denominator can be

92
00:07:15,720 --> 00:07:20,695
interpreted as the expected joint
distribution of the two random variables,

93
00:07:20,695 --> 00:07:26,782
if they were independent because when
two random variables are independent,

94
00:07:26,782 --> 00:07:32,810
they are joined distribution is equal to
the product of the two probabilities.

95
00:07:35,300 --> 00:07:39,800
So this comparison will tell us whether
the two variables are indeed independent.

96
00:07:39,800 --> 00:07:43,170
If they are indeed independent then we
would expect that the two are the same,

97
00:07:44,390 --> 00:07:49,470
but if the numerator is different
from the denominator, that would mean

98
00:07:49,470 --> 00:07:54,530
the two variables are not independent and
that helps measure the association.

99
00:07:56,120 --> 00:08:00,110
The sum is simply to take into
consideration of all of the combinations

100
00:08:00,110 --> 00:08:04,180
of the values of these
two random variables.

101
00:08:04,180 --> 00:08:08,750
In our case, each random variable
can choose one of the two values,

102
00:08:08,750 --> 00:08:13,950
zero or one, so
we have four combinations here.

103
00:08:13,950 --> 00:08:17,330
If we look at this form of mutual
information, it shows that the mutual

104
00:08:17,330 --> 00:08:21,230
information matches the divergence
of the actual joint distribution

105
00:08:21,230 --> 00:08:25,800
from the expected distribution
under the independence assumption.

106
00:08:25,800 --> 00:08:30,144
The larger this divergence is, the higher
the mutual information would be.

107
00:08:33,507 --> 00:08:37,091
So now let us further look at what
are exactly the probabilities,

108
00:08:37,091 --> 00:08:39,840
involved in this formula
of mutual information.

109
00:08:41,300 --> 00:08:45,080
And here, this is all the probabilities
involve, and it is easy for

110
00:08:45,080 --> 00:08:46,500
you to verify that.

111
00:08:46,500 --> 00:08:51,610
Basically, we have first to
[INAUDIBLE] probabilities

112
00:08:51,610 --> 00:08:56,380
corresponding to the presence or
absence of each word.

113
00:08:56,380 --> 00:08:59,610
So, for w1,
we have two probabilities shown here.

114
00:09:02,600 --> 00:09:07,995
They should sum to one, because a word
can either be present or absent.

115
00:09:07,995 --> 00:09:13,260
In the segment, and similarly for

116
00:09:13,260 --> 00:09:18,230
the second word, we also have two
probabilities representing presence or

117
00:09:18,230 --> 00:09:20,920
absences of this word, and
there is some to y as well.

118
00:09:21,920 --> 00:09:26,162
And finally, we have a lot of
joined probabilities that represent

119
00:09:26,162 --> 00:09:31,100
the scenarios of co-occurrences of
the two words, and they are shown here.

120
00:09:34,513 --> 00:09:39,107
And they sum to one because the two
words can only have these four

121
00:09:39,107 --> 00:09:41,420
possible scenarios.

122
00:09:41,420 --> 00:09:43,730
Either they both occur, so

123
00:09:43,730 --> 00:09:49,500
in that case both variables will have
a value of one, or one of them occurs.

124
00:09:49,500 --> 00:09:50,579
There are two scenarios.

125
00:09:51,660 --> 00:09:55,910
In these two cases one of the random
variables will be equal to one and

126
00:09:55,910 --> 00:10:03,560
the other will be zero and finally we have
the scenario when none of them occurs.

127
00:10:03,560 --> 00:10:06,420
This is when the two variables
taking a value of zero.

128
00:10:07,620 --> 00:10:12,855
So these are the probabilities involved
in the calculation of mutual information,

129
00:10:12,855 --> 00:10:13,600
over here.

130
00:10:16,007 --> 00:10:18,416
Once we know how to calculate
these probabilities,

131
00:10:18,416 --> 00:10:20,670
we can easily calculate
the new gene formation.

132
00:10:24,063 --> 00:10:28,231
It is also interesting to know that
there are actually some relations or

133
00:10:28,231 --> 00:10:32,960
constraint among these probabilities,
and we already saw two of them, right?

134
00:10:32,960 --> 00:10:36,400
So in the previous slide,

135
00:10:36,400 --> 00:10:41,830
that you have seen that
the marginal probabilities of these

136
00:10:41,830 --> 00:10:46,114
words sum to one and
we also have seen this constraint,

137
00:10:46,114 --> 00:10:53,190
that says the two words have these
four scenarios of co-occurrency,

138
00:10:53,190 --> 00:10:57,370
but we also have some additional
constraints listed in the bottom.

139
00:10:58,600 --> 00:11:03,670
For example, this one means if we add up

140
00:11:03,670 --> 00:11:07,890
the probabilities that we observe
the two words occur together and

141
00:11:07,890 --> 00:11:12,500
the probabilities when the first word
occurs and the second word does not occur.

142
00:11:12,500 --> 00:11:16,860
We get exactly the probability
that the first word is observed.

143
00:11:16,860 --> 00:11:20,040
In other words, when the word is observed.

144
00:11:20,040 --> 00:11:22,210
When the first word is observed, and

145
00:11:22,210 --> 00:11:27,640
there are only two scenarios, depending on
whether the second word is also observed.

146
00:11:27,640 --> 00:11:31,750
So, this probability captures the first
scenario when the second word

147
00:11:31,750 --> 00:11:33,860
actually is also observed, and

148
00:11:33,860 --> 00:11:38,130
this captures the second scenario
when the second word is not observed.

149
00:11:38,130 --> 00:11:40,145
So, we only see the first word, and

150
00:11:40,145 --> 00:11:45,410
it is easy to see the other equations
also follow the same reasoning.

151
00:11:46,980 --> 00:11:50,980
Now these equations allow us to
compute some probabilities based on

152
00:11:50,980 --> 00:11:54,610
other probabilities, and
this can simplify the computation.

153
00:11:55,750 --> 00:12:01,010
So more specifically,
if we know the probability that

154
00:12:01,010 --> 00:12:06,490
a word is present, like in this case,
so if we know this, and

155
00:12:06,490 --> 00:12:12,630
if we know the probability of
the presence of the second word,

156
00:12:12,630 --> 00:12:17,002
then we can easily compute
the absence probability, right?

157
00:12:17,002 --> 00:12:22,770
It is very easy to use this
equation to do that, and so

158
00:12:22,770 --> 00:12:27,820
we take care of the computation of
these probabilities of presence and

159
00:12:27,820 --> 00:12:29,950
absence of each word.

160
00:12:29,950 --> 00:12:33,146
Now let's look at
the [INAUDIBLE] distribution.

161
00:12:33,146 --> 00:12:36,460
Let us assume that we also have available

162
00:12:36,460 --> 00:12:39,548
the probability that
they occurred together.

163
00:12:39,548 --> 00:12:44,220
Now it is easy to see that we can
actually compute all the rest of these

164
00:12:44,220 --> 00:12:45,829
probabilities based on these.

165
00:12:46,870 --> 00:12:51,170
Specifically for
example using this equation we can compute

166
00:12:51,170 --> 00:12:56,260
the probability that the first word
occurred and the second word did not,

167
00:12:56,260 --> 00:13:02,020
because we know these probabilities in
the boxes, and similarly using this

168
00:13:02,020 --> 00:13:05,364
equation we can compute the probability
that we observe only the second word.

169
00:13:05,364 --> 00:13:06,000
Word.

170
00:13:06,000 --> 00:13:10,421
And then finally,
this probability can be calculated

171
00:13:10,421 --> 00:13:14,745
by using this equation because
now this is known, and

172
00:13:14,745 --> 00:13:19,282
this is also known, and
this is already known, right.

173
00:13:19,282 --> 00:13:23,120
So this can be easier to calculate.

174
00:13:23,120 --> 00:13:24,430
So now this can be calculated.

175
00:13:26,080 --> 00:13:30,989
So this slide shows that we only
need to know how to compute

176
00:13:30,989 --> 00:13:35,800
these three probabilities
that are shown in the boxes,

177
00:13:35,800 --> 00:13:43,092
naming the presence of each word and the
co-occurence of both words, in a segment.

178
00:13:43,092 --> 00:13:53,092
[MUSIC]


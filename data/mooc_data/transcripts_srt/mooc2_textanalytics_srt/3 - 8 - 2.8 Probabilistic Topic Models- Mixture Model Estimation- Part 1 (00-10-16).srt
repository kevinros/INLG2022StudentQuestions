1
00:00:00,012 --> 00:00:08,031
[SOUND]
This

2
00:00:08,031 --> 00:00:10,570
lecture is about mixture model estimation.

3
00:00:12,240 --> 00:00:16,260
In this lecture we're going to continue
discussing probabilistic topic models.

4
00:00:16,260 --> 00:00:16,830
In particular,

5
00:00:16,830 --> 00:00:20,380
we're going to talk about how to estimate
the parameters of a mixture model.

6
00:00:23,010 --> 00:00:26,870
So let's first look at our motivation for
using a mixture model.

7
00:00:26,870 --> 00:00:30,026
And we hope to factor out
the background words.

8
00:00:30,026 --> 00:00:33,480
From the top-words equation.

9
00:00:33,480 --> 00:00:39,990
The idea is to assume that the text data
actually contained two kinds of words.

10
00:00:39,990 --> 00:00:44,817
One kind is from the background here.

11
00:00:44,817 --> 00:00:48,800
So, the is, we, etc.

12
00:00:48,800 --> 00:00:53,820
And the other kind is from our pop board
distribution that we are interested in.

13
00:00:56,310 --> 00:01:01,420
So in order to solve this problem
of factoring out background words,

14
00:01:01,420 --> 00:01:05,780
we can set up our mixture model as false.

15
00:01:05,780 --> 00:01:09,250
We're going to assume that we
already know the parameters of

16
00:01:09,250 --> 00:01:14,010
all the values for
all the parameters in the mixture model,

17
00:01:14,010 --> 00:01:19,110
except for the water distribution
of which is our target.

18
00:01:20,130 --> 00:01:25,130
So this is a case of customizing
a probabilist model so

19
00:01:25,130 --> 00:01:29,500
that we embedded a known variable
that we are interested in.

20
00:01:29,500 --> 00:01:31,265
But we're going to simplify other things.

21
00:01:31,265 --> 00:01:34,180
We're going to assume we
have knowledge above others.

22
00:01:34,180 --> 00:01:37,760
And this is a powerful way
of customizing a model.

23
00:01:37,760 --> 00:01:39,500
For a particular need.

24
00:01:39,500 --> 00:01:40,320
Now you can imagine,

25
00:01:40,320 --> 00:01:45,000
we could have assumed that we also
don't know the background words.

26
00:01:45,000 --> 00:01:46,230
But in this case,

27
00:01:46,230 --> 00:01:51,810
our goal is to factor out precisely
those high probability background words.

28
00:01:51,810 --> 00:01:55,530
So we assume the background
model is already fixed.

29
00:01:56,680 --> 00:02:01,670
And one problem here is how
can we adjust theta sub d

30
00:02:01,670 --> 00:02:06,270
in order to maximize the probability
of the observed document here and

31
00:02:06,270 --> 00:02:07,920
we assume all the other
perimeters are now.

32
00:02:09,470 --> 00:02:12,530
Now although we designed
the model holistically.

33
00:02:12,530 --> 00:02:16,230
To try to factor out
these background words.

34
00:02:16,230 --> 00:02:21,056
It's unclear whether,
if we use maximum write or estimator.

35
00:02:21,056 --> 00:02:25,850
We will actually end up having
a whole distribution where the Common

36
00:02:25,850 --> 00:02:29,940
words like the would indeed have
smaller probabilities than before.

37
00:02:31,220 --> 00:02:36,880
Now in this case it turns
out the answer is yes.

38
00:02:36,880 --> 00:02:41,070
And when we set up
the probability in this way,

39
00:02:41,070 --> 00:02:45,720
when we use maximum likelihood or
we will end up having a word distribution

40
00:02:46,890 --> 00:02:49,990
where the use common words
would be factored out.

41
00:02:49,990 --> 00:02:52,570
By the use of the background
rule of distribution.

42
00:02:53,580 --> 00:02:56,710
So to understand why this is so,

43
00:02:56,710 --> 00:03:00,600
it's useful to examine
the behavior of a mixture model.

44
00:03:00,600 --> 00:03:03,910
So we're going to look at
a very very simple case.

45
00:03:03,910 --> 00:03:08,860
In order to understand some interesting
behaviors of a mixture model.

46
00:03:08,860 --> 00:03:15,130
The observed pattern here actually are
generalizable to mixture model in general.

47
00:03:15,130 --> 00:03:17,920
But it's much easier to
understand this behavior

48
00:03:17,920 --> 00:03:21,750
when we use A very simple case
like what we are seeing here.

49
00:03:21,750 --> 00:03:25,290
So specifically in this case,
let's assume that

50
00:03:25,290 --> 00:03:29,670
the probability choosing each of
the two models is exactly the same.

51
00:03:29,670 --> 00:03:33,400
So we're going to flip a fair coin
to decide which model to use.

52
00:03:34,420 --> 00:03:36,610
Furthermore, we're going
to assume there are.

53
00:03:36,610 --> 00:03:39,510
Precisely two words, the and text.

54
00:03:39,510 --> 00:03:46,120
Obviously this is a very naive
oversimplification of the actual text,

55
00:03:46,120 --> 00:03:52,110
but again, it's useful to examine
the behavior in such a special case.

56
00:03:53,690 --> 00:03:58,180
So we further assume that the background
model gives probability of

57
00:03:58,180 --> 00:04:03,059
0.9 towards the end text 0.1.

58
00:04:03,059 --> 00:04:08,340
Now, lets also assume that our data is
extremely simple the document has just

59
00:04:08,340 --> 00:04:13,820
two words text and the so now lets right
down the likeable function in such a case.

60
00:04:13,820 --> 00:04:18,350
First, what's the probability of text,
and what's the probably of the.

61
00:04:19,550 --> 00:04:22,340
I hope by this point you'll
be able to write it down.

62
00:04:23,760 --> 00:04:28,644
So the probability of text is
basically the sum over two cases,

63
00:04:28,644 --> 00:04:33,460
where each case corresponds with
to each of the order distribution

64
00:04:34,480 --> 00:04:38,060
and it accounts for
the two ways of generating text.

65
00:04:39,490 --> 00:04:43,580
And inside each case, we have
the probability of choosing the model,

66
00:04:43,580 --> 00:04:50,360
which is 0.5 multiplied by the probability
of observing text from that model.

67
00:04:50,360 --> 00:04:54,980
Similarly, the,
would have a probability of the same form,

68
00:04:54,980 --> 00:04:57,480
just what is different is
the exact probabilities.

69
00:04:58,900 --> 00:05:03,490
So naturally our lateral function
is just a product of the two.

70
00:05:03,490 --> 00:05:07,110
So It's very easy to see that,

71
00:05:08,140 --> 00:05:11,000
once you understand what's
the probability of each word.

72
00:05:11,000 --> 00:05:15,450
Which is also why it's so
important to understand what's exactly

73
00:05:15,450 --> 00:05:19,870
the probability of observing each
word from such a mixture model.

74
00:05:19,870 --> 00:05:25,690
Now, the interesting question now is,
how can we then optimize this likelihood?

75
00:05:25,690 --> 00:05:29,420
Well, you will note that
there are only two variables.

76
00:05:29,420 --> 00:05:32,270
They are precisely the two
probabilities of the two words.

77
00:05:32,270 --> 00:05:35,950
Text [INAUDIBLE] given by theta sub d.

78
00:05:35,950 --> 00:05:39,660
And this is because we have assumed
that all the other parameters are known.

79
00:05:41,240 --> 00:05:45,460
So, now the question is a very
simple algebra question.

80
00:05:45,460 --> 00:05:48,450
So, we have a simple expression
with two variables and

81
00:05:48,450 --> 00:05:53,140
we hope to choose the values of these
two variables to maximize this function.

82
00:05:54,270 --> 00:05:58,910
And the exercises that we have
seen some simple algebra problems.

83
00:06:00,150 --> 00:06:04,650
Note that the two probabilities must
sum to one, so there's some constraint.

84
00:06:06,340 --> 00:06:08,080
If there were no constraint of course,

85
00:06:08,080 --> 00:06:12,020
we would set both probabilities to
their maximum value which would be one,

86
00:06:12,020 --> 00:06:18,000
to maximize, But we can't do that
because text then the must sum to one.

87
00:06:18,000 --> 00:06:20,240
We can't give both a probability of one.

88
00:06:21,840 --> 00:06:25,150
So, now the question is how should
we allocate the probability and

89
00:06:25,150 --> 00:06:27,090
the math between the two words.

90
00:06:27,090 --> 00:06:28,400
What do you think?

91
00:06:28,400 --> 00:06:32,320
Now, it would be useful to look
at this formula For a moment, and

92
00:06:32,320 --> 00:06:36,540
to see what, intuitively,
what we do in order to

93
00:06:36,540 --> 00:06:39,940
do set these probabilities to
maximize the value of this function.

94
00:06:42,420 --> 00:06:44,310
Okay, if we look into this further,

95
00:06:44,310 --> 00:06:50,070
then we see some interesting behavior
of The two component models in that

96
00:06:50,070 --> 00:06:54,730
they will be collaborating to maximize
the probability of the observed data.

97
00:06:54,730 --> 00:06:57,790
Which is dictated by the maximum
likelihood estimator.

98
00:06:57,790 --> 00:07:02,020
But they are also competing in some way,
and in particular,

99
00:07:02,020 --> 00:07:05,350
they would be competing on the words.

100
00:07:05,350 --> 00:07:09,140
And they would tend to back high
probabilities on different words

101
00:07:09,140 --> 00:07:14,680
to avoid this competition in some sense or
to gain advantages in this competition.

102
00:07:14,680 --> 00:07:16,970
So again,
looking at this objective function and

103
00:07:16,970 --> 00:07:20,220
we have a constraint on
the two probabilities.

104
00:07:21,360 --> 00:07:25,460
Now, if you look at
the formula intuitively,

105
00:07:25,460 --> 00:07:30,509
you might feel that you want to set the
probability of text to be somewhat larger.

106
00:07:32,130 --> 00:07:38,160
And this inducing can be work supported
by mathematical fact, which is when

107
00:07:38,160 --> 00:07:43,280
the sum of two variables is
a constant then the product of them

108
00:07:43,280 --> 00:07:49,150
which is maximum when they are equal,
and this is a fact we know from algebra.

109
00:07:49,150 --> 00:07:52,910
Now if we plug that [INAUDIBLE] It
would mean that we have to make the two

110
00:07:52,910 --> 00:07:55,100
probabilities equal.

111
00:07:56,170 --> 00:07:57,830
And when we make them equal and

112
00:07:57,830 --> 00:08:02,180
then if we consider the constraint it
will be easy to solve this problem, and

113
00:08:02,180 --> 00:08:09,310
the solution is the probability of tax
will be .09 and probability is .01.

114
00:08:09,310 --> 00:08:14,150
The probability of text is now much
larger than probability of the, and

115
00:08:14,150 --> 00:08:17,200
this is not the case when
have just one distribution.

116
00:08:17,200 --> 00:08:21,040
And this is clearly because of
the use of the background model,

117
00:08:21,040 --> 00:08:26,480
which assigned the very high probability
to the and low probability to text.

118
00:08:26,480 --> 00:08:30,270
And if you look at the equation
you will see obviously

119
00:08:30,270 --> 00:08:33,300
some interaction of the two
distributions here.

120
00:08:35,070 --> 00:08:39,090
In particular,
you will see in order to make them equal.

121
00:08:39,090 --> 00:08:46,350
And then the probability assigned
by theta sub d must be higher for

122
00:08:46,350 --> 00:08:50,849
a word that has a smaller
probability given by the background.

123
00:08:53,380 --> 00:08:56,690
This is obvious from
examining this equation.

124
00:08:56,690 --> 00:08:59,850
Because the background part is weak for
text.

125
00:08:59,850 --> 00:09:00,710
It's small.

126
00:09:00,710 --> 00:09:04,900
So in order to compensate for that,
we must make the probability for

127
00:09:04,900 --> 00:09:11,270
text given by theta sub D somewhat larger,
so that the two sides can be balanced.

128
00:09:11,270 --> 00:09:17,280
So this is in fact a very
general behavior of this model.

129
00:09:17,280 --> 00:09:21,780
And that is, if one distribution assigns a
high probability to one word than another,

130
00:09:21,780 --> 00:09:25,540
then the other distribution
would tend to do the opposite.

131
00:09:25,540 --> 00:09:28,960
Basically it would discourage other
distributions to do the same And

132
00:09:28,960 --> 00:09:34,650
this is to balance them out so
we can account for all kinds of words.

133
00:09:34,650 --> 00:09:38,811
And this also means that by using
a background model that is fixed into

134
00:09:38,811 --> 00:09:42,341
assigned high probabilities
through background words.

135
00:09:42,341 --> 00:09:47,194
We can indeed encourages the unknown
topical one of this to assign smaller

136
00:09:47,194 --> 00:09:50,035
probabilities for such common words.

137
00:09:50,035 --> 00:09:54,302
Instead put more probability
than this on the content words,

138
00:09:54,302 --> 00:09:58,170
that cannot be explained well
by the background model.

139
00:09:58,170 --> 00:10:02,754
Meaning that they have a very small
probability from the background motor like

140
00:10:02,754 --> 00:10:03,452
text here.

141
00:10:03,452 --> 00:10:13,452
[MUSIC]


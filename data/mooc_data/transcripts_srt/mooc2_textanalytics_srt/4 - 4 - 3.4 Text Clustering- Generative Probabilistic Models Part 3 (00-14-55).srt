1
00:00:00,012 --> 00:00:08,289
[SOUND]
This lecture

2
00:00:08,289 --> 00:00:12,379
is a continuing discussion of generative
probabilistic models for tax classroom.

3
00:00:14,210 --> 00:00:17,210
In this lecture, we're going to
do a finishing discussion of

4
00:00:17,210 --> 00:00:19,630
generative probabilistic models for
text crossing.

5
00:00:21,590 --> 00:00:26,635
So this is a slide that you have seen
before and here, we show how we define

6
00:00:26,635 --> 00:00:32,371
the mixture model for text crossing and
what the likelihood function looks like.

7
00:00:32,371 --> 00:00:36,879
And we can also compute
the maximum likelihood estimate,

8
00:00:36,879 --> 00:00:39,186
to estimate the parameters.

9
00:00:39,186 --> 00:00:43,804
In this lecture, we're going to do talk
more about how exactly we're going to

10
00:00:43,804 --> 00:00:46,569
compute the maximum likelihood estimate.

11
00:00:46,569 --> 00:00:55,185
As in most cases the Algorithm can be used
to solve this problem for mixture models.

12
00:00:55,185 --> 00:01:00,860
So here's the detail of this Algorithm for
document clustering.

13
00:01:00,860 --> 00:01:04,450
Now, if you have understood
how Algorithm works for

14
00:01:04,450 --> 00:01:09,490
topic models like TRSA, and
I think here it would be very similar.

15
00:01:09,490 --> 00:01:15,790
And we just need to adapt a little
bit to this new mixture model.

16
00:01:15,790 --> 00:01:22,784
So as you may recall Algorithm starts with
initialization of all the parameters.

17
00:01:22,784 --> 00:01:27,140
So this is the same as what
happened before for topic models.

18
00:01:28,490 --> 00:01:33,340
And then we're going to repeat
until the likelihood converges and

19
00:01:33,340 --> 00:01:37,310
in each step we'll do E step and M step.

20
00:01:37,310 --> 00:01:38,060
In M step,

21
00:01:38,060 --> 00:01:44,040
we're going to infer which distribution
has been used to generate each document.

22
00:01:44,040 --> 00:01:48,540
So I have to introduce
a hidden variable Zd for

23
00:01:48,540 --> 00:01:54,610
each document and this variable could take
a value from the range of 1 through k,

24
00:01:54,610 --> 00:01:56,910
representing k different distributions.

25
00:01:59,570 --> 00:02:04,140
More specifically basically, we're going
to apply base rules to infer which

26
00:02:04,140 --> 00:02:09,240
distribution is more likely to
have generated this document, or

27
00:02:09,240 --> 00:02:16,290
computing the posterior probability of
the distribution given the document.

28
00:02:17,390 --> 00:02:21,490
And we know it's proportional
to the probability

29
00:02:21,490 --> 00:02:26,880
of selecting this
distribution p of Z the i.

30
00:02:26,880 --> 00:02:32,080
And the probability of generating this
whole document from the distribution which

31
00:02:32,080 --> 00:02:40,000
is the product of the probabilities of
world for this document as you see here.

32
00:02:40,000 --> 00:02:43,980
Now, as you all clear this use for
kind of remember,

33
00:02:45,050 --> 00:02:50,300
the normalizer or
the constraint on this probability.

34
00:02:50,300 --> 00:02:56,059
So in this case, we know
the constraint on this probability in

35
00:02:56,059 --> 00:03:02,173
E-Step is that all the probabilities
of Z equals i must sum to 1.

36
00:03:02,173 --> 00:03:06,658
Because the documented must have been
generated from precisely one of these k

37
00:03:06,658 --> 00:03:07,870
topics.

38
00:03:07,870 --> 00:03:11,860
So the probability of being generated
from each of them should sum to 1.

39
00:03:11,860 --> 00:03:18,220
And if you know this constraint, then
you can easily compute this distribution

40
00:03:18,220 --> 00:03:24,350
as long as you know what
it is proportional to.

41
00:03:24,350 --> 00:03:29,379
So once you compute this product that
you see here, then you simply normalize

42
00:03:31,220 --> 00:03:35,070
these probabilities,
to make them sum to 1 over all the topics.

43
00:03:35,070 --> 00:03:40,070
So that's E-Step, after E-Step we
want to know which distribution is

44
00:03:40,070 --> 00:03:44,030
more likely to have generated this
document d, and which is unlikely.

45
00:03:45,460 --> 00:03:49,988
And then in M-Step we're going to
re-estimate all the parameters based on

46
00:03:49,988 --> 00:03:54,443
the in further z values or in further
knowledge about which distribution

47
00:03:54,443 --> 00:03:57,089
has been used to generate which document.

48
00:03:57,089 --> 00:04:02,522
So the re-estimation involves two kinds
of parameters 1 is p of theta and

49
00:04:02,522 --> 00:04:08,400
this is the probability of selecting
a particular distribution.

50
00:04:08,400 --> 00:04:09,980
Before we observe anything,

51
00:04:09,980 --> 00:04:13,230
we don't have any knowledge about
which cluster is more likely.

52
00:04:13,230 --> 00:04:17,558
But after we have observed
that these documents,

53
00:04:17,558 --> 00:04:23,544
then we can crack the evidence to
infer which cluster is more likely.

54
00:04:23,544 --> 00:04:28,166
And so this is proportional to the sum of

55
00:04:28,166 --> 00:04:32,940
the probability of Z
sub d j is equal to i.

56
00:04:34,680 --> 00:04:40,640
And so this gives us all
the evidence about using topic i,

57
00:04:40,640 --> 00:04:43,380
theta i to generate a document.

58
00:04:43,380 --> 00:04:48,590
Pull them together and again,
we normalize them into probabilities.

59
00:04:50,420 --> 00:04:53,300
So this is for key of theta sub i.

60
00:04:54,560 --> 00:04:58,969
Now the other kind of parameters
are the probabilities of words in each

61
00:04:58,969 --> 00:05:01,144
distribution, in each cluster.

62
00:05:01,144 --> 00:05:05,384
And this is very similar
to the case piz and

63
00:05:05,384 --> 00:05:10,230
here we just report the kinds
of words that are in

64
00:05:10,230 --> 00:05:15,442
documents that are inferred
to have been generated

65
00:05:15,442 --> 00:05:20,380
from a particular topic of theta i here.

66
00:05:20,380 --> 00:05:25,240
This would allows to then
estimate how many words have

67
00:05:25,240 --> 00:05:28,807
actually been generated from theta i.

68
00:05:28,807 --> 00:05:32,694
And then we'll normalize again these
accounts in the probabilities so

69
00:05:32,694 --> 00:05:36,550
that the probabilities on all
the words would sum to up.

70
00:05:36,550 --> 00:05:40,560
Note that it's very important to
understand these constraints as

71
00:05:40,560 --> 00:05:45,890
they are precisely the normalizing
in all these formulas.

72
00:05:45,890 --> 00:05:53,380
And it's also important to know
that the distribution is over what?

73
00:05:54,490 --> 00:05:59,730
For example, the probability of
theta is over all the k topics,

74
00:05:59,730 --> 00:06:02,730
that's why these k
probabilities will sum to 1.

75
00:06:02,730 --> 00:06:07,304
Whereas the probability of a word given
theta is a probability distribution

76
00:06:07,304 --> 00:06:08,527
over all the words.

77
00:06:08,527 --> 00:06:13,100
So there are many probabilities and
they have to sum to 1.

78
00:06:13,100 --> 00:06:17,279
So now, let's take a look at
a simple example of two clusters.

79
00:06:17,279 --> 00:06:21,340
I've two clusters,
I've assumed some initialized values for

80
00:06:21,340 --> 00:06:23,440
the two distributions.

81
00:06:23,440 --> 00:06:28,270
And let's assume we randomly
initialize two probability of

82
00:06:28,270 --> 00:06:33,140
selecting each cluster as 0.5,
so equally likely.

83
00:06:33,140 --> 00:06:36,400
And then let's consider one
document that you have seen here.

84
00:06:36,400 --> 00:06:41,350
There are two occurrences of text and
two occurrences of mining.

85
00:06:41,350 --> 00:06:44,910
So there are four words together and
medical and

86
00:06:44,910 --> 00:06:47,100
health did not occur in this document.

87
00:06:47,100 --> 00:06:49,209
So let's think about the hidden variable.

88
00:06:50,360 --> 00:06:54,970
Now for each document then we
much use a hidden variable.

89
00:06:54,970 --> 00:06:58,907
And before in piz,
we used one hidden variable for

90
00:06:58,907 --> 00:07:03,804
each work because that's
the output from one mixture model.

91
00:07:03,804 --> 00:07:06,562
So in our case the output
from the mixture model or

92
00:07:06,562 --> 00:07:10,810
the observation from mixture
model is a document, not a word.

93
00:07:10,810 --> 00:07:14,920
So now we have one hidden variable
attached to the document.

94
00:07:14,920 --> 00:07:18,338
Now that hidden variable must tell us
which distribution has been used to

95
00:07:18,338 --> 00:07:19,525
generate the document.

96
00:07:19,525 --> 00:07:24,010
So it's going to take two values,
one and two to indicate the two topics.

97
00:07:25,350 --> 00:07:29,940
So now how do we infer which
distribution has been used generally d?

98
00:07:29,940 --> 00:07:33,380
Well it's been used base rule,
so it looks like this.

99
00:07:33,380 --> 00:07:39,071
In order for the first topic
theta 1 to generate a document,

100
00:07:39,071 --> 00:07:41,530
two things must happen.

101
00:07:41,530 --> 00:07:45,210
First, theta sub 1 must
have been selected.

102
00:07:45,210 --> 00:07:48,050
So it's given by p of theta 1.

103
00:07:48,050 --> 00:07:54,144
Second, it must have also be generating
the four words in the document.

104
00:07:54,144 --> 00:07:59,004
Namely, two occurrences of text and
two occurrences of sub mining.

105
00:07:59,004 --> 00:08:04,283
And that's why you see the numerator
has the product of the probability of

106
00:08:04,283 --> 00:08:10,182
selecting theta 1 and the probability of
generating the document from theta 1.

107
00:08:10,182 --> 00:08:15,011
So the denominator is just the sum of
two possibilities of generality in

108
00:08:15,011 --> 00:08:16,146
this document.

109
00:08:16,146 --> 00:08:21,138
And you can plug in the numerical
values to verify indeed in this case,

110
00:08:21,138 --> 00:08:25,283
the document is more likely
to be generated from theta 1,

111
00:08:25,283 --> 00:08:27,915
much more likely than from theta 2.

112
00:08:27,915 --> 00:08:30,236
So once we have this probability,

113
00:08:30,236 --> 00:08:35,950
we can easily compute the probability
of Z equals 2, given this document.

114
00:08:35,950 --> 00:08:36,690
How?

115
00:08:36,690 --> 00:08:38,720
Well, we can use the constraint.

116
00:08:38,720 --> 00:08:43,339
That's going to be 1 minus 100 over 101.

117
00:08:43,339 --> 00:08:47,500
So now it's important that you note
that in such a computation there

118
00:08:47,500 --> 00:08:50,520
is a potential problem of underflow.

119
00:08:50,520 --> 00:08:55,700
And that is because if you look at the
original numerator and the denominator,

120
00:08:55,700 --> 00:09:00,530
it involves the competition of
a product of many small probabilities.

121
00:09:00,530 --> 00:09:03,050
Imagine if a document has many words and

122
00:09:03,050 --> 00:09:09,360
it's going to be a very small value here
that can cause the problem of underflow.

123
00:09:09,360 --> 00:09:14,294
So to solve the problem,
we can use a normalize.

124
00:09:14,294 --> 00:09:18,537
So here you see that we take
a average of all these two math

125
00:09:18,537 --> 00:09:23,340
solutions to compute average at
the screen called a theta bar.

126
00:09:24,530 --> 00:09:28,590
And this average distribution
would be comparable to

127
00:09:28,590 --> 00:09:33,650
each of these distributions in terms
of the quantities or the magnitude.

128
00:09:33,650 --> 00:09:38,440
So we can then divide the numerator and

129
00:09:38,440 --> 00:09:42,070
the denominator both by this normalizer.

130
00:09:42,070 --> 00:09:47,688
So basically this normalizes
the probability of generating

131
00:09:47,688 --> 00:09:52,990
this document by using this
average word distribution.

132
00:09:52,990 --> 00:09:56,310
So you can see the normalizer is here.

133
00:09:56,310 --> 00:10:00,690
And since we have used exactly the same
normalizer for the numerator and

134
00:10:00,690 --> 00:10:02,240
the denominator.

135
00:10:02,240 --> 00:10:07,940
The whole value of this expression is not
changed but by doing this normalization

136
00:10:07,940 --> 00:10:14,480
you can see we can make the numerators and
the denominators more manageable

137
00:10:14,480 --> 00:10:19,890
in that the overall value is not
going to be very small for each.

138
00:10:19,890 --> 00:10:22,660
And thus we can avoid
the underflow problem.

139
00:10:24,580 --> 00:10:29,530
In some other times we sometimes
also use logarithm of the product

140
00:10:29,530 --> 00:10:33,570
to convert this into a sum
of log of probabilities.

141
00:10:33,570 --> 00:10:36,080
This can help preserve precision as well,
but

142
00:10:36,080 --> 00:10:40,720
in this case we cannot use
algorithm to solve the problem.

143
00:10:40,720 --> 00:10:44,030
Because there is a sum in the denominator,
but

144
00:10:44,030 --> 00:10:49,230
this kind of normalizes can be
effective for solving this problem.

145
00:10:49,230 --> 00:10:53,754
So it's a technique that's sometimes
useful in other situations in other

146
00:10:53,754 --> 00:10:55,057
situations as well.

147
00:10:55,057 --> 00:10:56,630
Now let's look at the M-Step.

148
00:10:56,630 --> 00:11:01,557
So from the E-Step we can see our estimate
of which distribution is more likely to

149
00:11:01,557 --> 00:11:03,521
have generated a document at d.

150
00:11:03,521 --> 00:11:08,157
And you can see d1's more like
got it from the first topic,

151
00:11:08,157 --> 00:11:12,090
where is d2 is more like
from second topic, etc.

152
00:11:12,090 --> 00:11:15,800
Now, let's think about what we
need to compute in M-step well

153
00:11:15,800 --> 00:11:18,460
basically we need to
re-estimate all the parameters.

154
00:11:18,460 --> 00:11:22,750
First, look at p of theta 1 and
p of theta 2.

155
00:11:22,750 --> 00:11:24,010
How do we estimate that?

156
00:11:24,010 --> 00:11:31,625
Intuitively you can just pool together
these z, the probabilities from E-step.

157
00:11:31,625 --> 00:11:36,335
So if all of these documents say,
well they're more likely from theta 1,

158
00:11:36,335 --> 00:11:42,240
then we intuitively would give
a higher probability to theta 1.

159
00:11:42,240 --> 00:11:46,240
In this case,
we can just take an average of these

160
00:11:46,240 --> 00:11:50,920
probabilities that you see here and
we've obtain a 0.6 for theta 1.

161
00:11:50,920 --> 00:11:53,860
So 01 is more likely and then theta 2.

162
00:11:53,860 --> 00:11:59,542
So you can see probability of
02 would be natural in 0.4.

163
00:11:59,542 --> 00:12:01,680
What about these word of probabilities?

164
00:12:01,680 --> 00:12:04,490
Well we do the same, and
intuition is the same.

165
00:12:04,490 --> 00:12:05,802
So we're going to see,

166
00:12:05,802 --> 00:12:09,148
in order to estimate the probabilities
of words in theta 1,

167
00:12:09,148 --> 00:12:13,295
we're going to look at which documents
have been generated from theta 1.

168
00:12:13,295 --> 00:12:17,510
And we're going to pull together the words
in those documents and normalize them.

169
00:12:17,510 --> 00:12:19,400
So this is basically what I just said.

170
00:12:20,510 --> 00:12:25,970
More specifically, we're going to do for
example, use all the kinds of text in

171
00:12:25,970 --> 00:12:31,103
these documents for estimating
the probability of text given theta 1.

172
00:12:31,103 --> 00:12:34,480
But we're not going to use their
raw count or total accounts.

173
00:12:34,480 --> 00:12:40,110
Instead, we can do that discount them
by the probabilities that each document

174
00:12:40,110 --> 00:12:43,567
is likely been generated from theta 1.

175
00:12:43,567 --> 00:12:47,490
So these gives us some
fractional accounts.

176
00:12:47,490 --> 00:12:50,480
And then these accounts
would be then normalized

177
00:12:50,480 --> 00:12:52,530
in order to get the probability.

178
00:12:52,530 --> 00:12:53,720
Now, how do we normalize them?

179
00:12:53,720 --> 00:12:57,810
Well these probability of
these words must assign to 1.

180
00:12:57,810 --> 00:13:02,790
So to summarize our discussion of
generative models for clustering.

181
00:13:02,790 --> 00:13:07,500
Well we show that a slight variation
of topic model can be used for

182
00:13:07,500 --> 00:13:08,760
clustering documents.

183
00:13:08,760 --> 00:13:12,730
And this also shows the power
of generating models in general.

184
00:13:12,730 --> 00:13:18,180
By changing the generation assumption and
changing the model slightly we can achieve

185
00:13:18,180 --> 00:13:23,010
different goals, and we can capture
different patterns and types of data.

186
00:13:23,010 --> 00:13:27,430
So in this case, each cluster is
represented by unigram language model

187
00:13:27,430 --> 00:13:31,940
word distribution and
that is similar to topic model.

188
00:13:31,940 --> 00:13:35,680
So here you can see the word distribution
actually generates a term cluster

189
00:13:35,680 --> 00:13:37,900
as a by-product.

190
00:13:37,900 --> 00:13:41,090
A document that is generated by first
choosing a unigram language model.

191
00:13:41,090 --> 00:13:44,190
And then generating all the words
in the document are using

192
00:13:44,190 --> 00:13:45,960
just a single language model.

193
00:13:45,960 --> 00:13:49,830
And this is very different from again
copy model where we can generate

194
00:13:49,830 --> 00:13:54,910
the words in the document by using
multiple unigram language models.

195
00:13:56,740 --> 00:14:01,530
And then the estimated model parameters
are given both topic characterization

196
00:14:01,530 --> 00:14:02,950
of each cluster and

197
00:14:02,950 --> 00:14:06,190
the probabilistic assignment of
each document into a cluster.

198
00:14:07,290 --> 00:14:12,140
And this probabilistic assignment
sometimes is useful for some applications.

199
00:14:12,140 --> 00:14:15,710
But if we want to achieve
harder clusters mainly to

200
00:14:16,800 --> 00:14:20,160
partition documents into
disjointed clusters.

201
00:14:20,160 --> 00:14:25,073
Then we can just force a document into
the cluster corresponding to the words

202
00:14:25,073 --> 00:14:29,467
distribution that's most likely
to have generated the document.

203
00:14:29,467 --> 00:14:33,669
We've also shown that the Algorithm can
be used to compute the maximum likelihood

204
00:14:33,669 --> 00:14:34,990
estimate.

205
00:14:34,990 --> 00:14:38,798
And in this case, we need to use a special

206
00:14:38,798 --> 00:14:43,393
number addition technique
to avoid underflow.

207
00:14:43,393 --> 00:14:53,393
[MUSIC]
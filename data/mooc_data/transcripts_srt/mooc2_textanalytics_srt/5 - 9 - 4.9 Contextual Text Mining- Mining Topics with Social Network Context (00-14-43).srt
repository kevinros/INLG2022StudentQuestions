1
00:00:00,025 --> 00:00:05,098
[SOUND] This lecture is about

2
00:00:05,098 --> 00:00:09,728
how to mine text data with

3
00:00:09,728 --> 00:00:15,028
social network as context.

4
00:00:15,028 --> 00:00:18,400
In this lecture we're going to continue
discussing contextual text mining.

5
00:00:18,400 --> 00:00:25,000
In particular, we're going to look at
the social network of others as context.

6
00:00:26,160 --> 00:00:31,670
So first, what's our motivation for using
network context for analysis of text?

7
00:00:32,970 --> 00:00:36,440
The context of a text
article can form a network.

8
00:00:37,460 --> 00:00:40,470
For example the authors
of research articles

9
00:00:40,470 --> 00:00:42,000
might form collaboration networks.

10
00:00:44,140 --> 00:00:48,300
But authors of social media content
might form social networks.

11
00:00:48,300 --> 00:00:51,910
For example,
in Twitter people might follow each other.

12
00:00:51,910 --> 00:01:00,570
Or in Facebook as people might
claim friends of others, etc.

13
00:01:00,570 --> 00:01:07,940
So such context connects
the content of the others.

14
00:01:07,940 --> 00:01:12,422
Similarly, locations associated with
text can also be connected to form

15
00:01:12,422 --> 00:01:13,952
geographical network.

16
00:01:13,952 --> 00:01:18,507
But in general you can can imagine
the metadata of the text data

17
00:01:18,507 --> 00:01:22,809
can form some kind of network
if they have some relations.

18
00:01:24,550 --> 00:01:29,650
Now there is some benefit in
jointly analyzing text and

19
00:01:29,650 --> 00:01:34,450
its social network context or
network context in general.

20
00:01:34,450 --> 00:01:40,590
And that's because we can use network to
impose some constraints on topics of text.

21
00:01:41,860 --> 00:01:45,716
So for example it's reasonable
to assume that authors

22
00:01:45,716 --> 00:01:50,560
connected in collaboration networks
tend to write about the similar topics.

23
00:01:53,760 --> 00:01:57,850
So such heuristics can be used
to guide us in analyzing topics.

24
00:01:57,850 --> 00:02:06,540
Text also can help characterize the
content associated with each subnetwork.

25
00:02:06,540 --> 00:02:10,520
And this is to say that both

26
00:02:11,980 --> 00:02:15,370
kinds of data, the network and
text, can help each other.

27
00:02:16,850 --> 00:02:21,991
So for example the difference in
opinions expressed that are in

28
00:02:21,991 --> 00:02:27,720
two subnetworks can be reviewed by
doing this type of joint analysis.

29
00:02:30,560 --> 00:02:38,980
So here briefly you could use a model
called a network supervised topic model.

30
00:02:40,380 --> 00:02:43,970
In this slide we're going to
give some general ideas.

31
00:02:43,970 --> 00:02:46,880
And then in the next slide we're
going to give some more details.

32
00:02:48,520 --> 00:02:53,930
But in general in this part of the course
we don't have enough time to cover

33
00:02:53,930 --> 00:02:56,940
these frontier topics in detail.

34
00:02:56,940 --> 00:03:01,870
But we provide references
that would allow you to

35
00:03:01,870 --> 00:03:04,300
read more about the topic
to know the details.

36
00:03:05,560 --> 00:03:09,400
But it should still be useful
to know the general ideas.

37
00:03:09,400 --> 00:03:16,190
And to know what they can do to know
when you might be able to use them.

38
00:03:16,190 --> 00:03:22,140
So the general idea of network
supervised topic model is the following.

39
00:03:22,140 --> 00:03:28,320
Let's start with viewing
the regular topic models.

40
00:03:28,320 --> 00:03:33,750
Like if you had an LDA as
sorting optimization problem.

41
00:03:33,750 --> 00:03:34,810
Of course, in this case,

42
00:03:34,810 --> 00:03:38,170
the optimization objective
function is a likelihood function.

43
00:03:38,170 --> 00:03:42,910
So we often use maximum likelihood
estimator to obtain the parameters.

44
00:03:42,910 --> 00:03:47,456
And these parameters will give us
useful information that we want to

45
00:03:47,456 --> 00:03:50,000
obtain from text data.

46
00:03:50,000 --> 00:03:51,760
For example, topics.

47
00:03:51,760 --> 00:03:56,590
So we want to maximize the probability
of tests that are given the parameters

48
00:03:56,590 --> 00:04:01,490
generally denoted by number.

49
00:04:01,490 --> 00:04:06,490
The main idea of incorporating network is

50
00:04:06,490 --> 00:04:12,520
to think about the constraints that
can be imposed based on the network.

51
00:04:12,520 --> 00:04:16,110
In general,
the idea is to use the network to

52
00:04:16,110 --> 00:04:20,350
impose some constraints on
the model parameters, lambda here.

53
00:04:20,350 --> 00:04:21,010
For example,

54
00:04:21,010 --> 00:04:27,340
the text at adjacent nodes of the network
can be similar to cover similar topics.

55
00:04:27,340 --> 00:04:31,360
Indeed, in many cases,
they tend to cover similar topics.

56
00:04:34,010 --> 00:04:38,039
So we may be able to smooth
the topic distributions

57
00:04:39,170 --> 00:04:43,510
on the graph on the network so
that adjacent nodes will have

58
00:04:43,510 --> 00:04:48,253
very similar topic distributions.

59
00:04:48,253 --> 00:04:53,780
So they will share a common
distribution on the topics.

60
00:04:53,780 --> 00:05:00,460
Or have just a slight variations of the
topic of distributions, of the coverage.

61
00:05:02,210 --> 00:05:07,200
So, technically, what we can do
is simply to add a network and

62
00:05:07,200 --> 00:05:11,530
use the regularizers to the likelihood
of objective function as shown here.

63
00:05:11,530 --> 00:05:14,480
So instead of just optimize
the probability of test

64
00:05:14,480 --> 00:05:18,590
data given parameters lambda, we're
going to optimize another function F.

65
00:05:19,770 --> 00:05:26,400
This function combines the likelihood with
a regularizer function called R here.

66
00:05:26,400 --> 00:05:31,820
And the regularizer defines
the the parameters lambda and the Network.

67
00:05:31,820 --> 00:05:34,330
It tells us basically

68
00:05:34,330 --> 00:05:38,540
what kind of parameters are preferred
from a network constraint perspective.

69
00:05:38,540 --> 00:05:41,470
So you can easily see this is in effect

70
00:05:41,470 --> 00:05:45,730
implementing the idea of imposing
some prior on the model parameters.

71
00:05:45,730 --> 00:05:50,060
Only that we're not necessary
having a probabilistic model, but

72
00:05:50,060 --> 00:05:51,140
the idea is the same.

73
00:05:51,140 --> 00:05:55,450
We're going to combine the two in
one single objective function.

74
00:05:57,770 --> 00:06:02,130
So, the advantage of this idea
is that it's quite general.

75
00:06:02,130 --> 00:06:06,270
Here the top model can be any
generative model for text.

76
00:06:07,350 --> 00:06:11,370
It doesn't have to be PLSA or
LEA, or the current topic models.

77
00:06:12,830 --> 00:06:17,080
And similarly,
the network can be also in a network.

78
00:06:17,080 --> 00:06:19,280
Any graph that connects
these text objects.

79
00:06:22,531 --> 00:06:26,470
This regularizer can
also be any regularizer.

80
00:06:26,470 --> 00:06:31,440
We can be flexible in capturing different
heuristics that we want to capture.

81
00:06:32,590 --> 00:06:36,210
And finally,
the function F can also vary, so

82
00:06:36,210 --> 00:06:38,840
there can be many different
ways to combine them.

83
00:06:38,840 --> 00:06:42,490
So, this general idea is actually quite,
quite powerful.

84
00:06:42,490 --> 00:06:47,530
It offers a general approach
to combining these different

85
00:06:47,530 --> 00:06:52,620
types of data in single
optimization framework.

86
00:06:52,620 --> 00:06:54,809
And this general idea can really
be applied for any problem.

87
00:06:56,900 --> 00:06:59,280
But here in this paper reference here,

88
00:06:59,280 --> 00:07:05,540
a particular instantiation
called a NetPLSA was started.

89
00:07:05,540 --> 00:07:06,962
In this case, it's just for

90
00:07:06,962 --> 00:07:11,990
instantiating of PLSA to incorporate this
simple constraint imposed by network.

91
00:07:11,990 --> 00:07:15,730
And the prior here is the neighbors on

92
00:07:15,730 --> 00:07:18,070
the network must have
similar topic distribution.

93
00:07:18,070 --> 00:07:20,500
They must cover similar
topics in similar ways.

94
00:07:20,500 --> 00:07:22,720
And that's basically
what it says in English.

95
00:07:24,080 --> 00:07:27,470
So technically we just have
a modified objective function here.

96
00:07:27,470 --> 00:07:32,780
Let's define both the texts you can
actually see in the network graph G here.

97
00:07:34,070 --> 00:07:36,050
And if you look at this formula,

98
00:07:36,050 --> 00:07:38,489
you can actually recognize
some part fairly familiarly.

99
00:07:40,100 --> 00:07:45,046
Because they are, they should be
fairly familiar to you by now.

100
00:07:45,046 --> 00:07:49,182
So can you recognize which
part is the likelihood for

101
00:07:49,182 --> 00:07:51,720
the test given the topic model?

102
00:07:52,720 --> 00:07:58,962
Well if you look at it, you will see this
part is precisely the PLSA log-likelihood

103
00:07:58,962 --> 00:08:04,160
that we want to maximize when we
estimate parameters for PLSA alone.

104
00:08:04,160 --> 00:08:10,305
But the second equation shows some
additional constraints on the parameters.

105
00:08:10,305 --> 00:08:15,610
And in particular,
we'll see here it's to measure

106
00:08:15,610 --> 00:08:21,300
the difference between the topic
coverage at node u and node v.

107
00:08:21,300 --> 00:08:25,570
The two adjacent nodes on the network.

108
00:08:25,570 --> 00:08:27,680
We want their distributions to be similar.

109
00:08:27,680 --> 00:08:31,880
So here we are computing the square
of their differences and

110
00:08:31,880 --> 00:08:34,400
we want to minimize this difference.

111
00:08:34,400 --> 00:08:40,825
And note that there's a negative sign in
front of this sum, this whole sum here.

112
00:08:40,825 --> 00:08:46,204
So this makes it possible to find

113
00:08:46,204 --> 00:08:51,385
the parameters that are both to

114
00:08:51,385 --> 00:08:57,780
maximize the PLSA log-likelihood.

115
00:08:57,780 --> 00:09:01,560
That means the parameters
will fit the data well and,

116
00:09:01,560 --> 00:09:05,600
also to respect that this
constraint from the network.

117
00:09:06,700 --> 00:09:09,640
And this is the negative
sign that I just mentioned.

118
00:09:09,640 --> 00:09:12,742
Because this is an negative sign,
when we maximize this

119
00:09:12,742 --> 00:09:16,780
object in function we'll actually
minimize this statement term here.

120
00:09:19,589 --> 00:09:23,906
So if we look further in
this picture we'll see

121
00:09:23,906 --> 00:09:29,560
the results will weight of
edge between u and v here.

122
00:09:29,560 --> 00:09:32,120
And that space from out network.

123
00:09:32,120 --> 00:09:34,495
If you have a weight that says well,

124
00:09:34,495 --> 00:09:38,470
these two nodes are strong
collaborators of researchers.

125
00:09:38,470 --> 00:09:45,510
These two are strong connections
between two people in a social network.

126
00:09:45,510 --> 00:09:46,740
And they would have weight.

127
00:09:46,740 --> 00:09:52,590
Then that means it would be more important
that they're topic coverages are similar.

128
00:09:52,590 --> 00:09:54,070
And that's basically what it says here.

129
00:09:55,460 --> 00:09:57,660
And finally you see
a parameter lambda here.

130
00:09:57,660 --> 00:10:02,260
This is a new parameter to control
the influence of network constraint.

131
00:10:02,260 --> 00:10:07,460
We can see easily, if lambda is set to 0,
we just go back to the standard PLSA.

132
00:10:07,460 --> 00:10:09,470
But when lambda is set to a larger value,

133
00:10:09,470 --> 00:10:14,920
then we will let the network
influence the estimated models more.

134
00:10:14,920 --> 00:10:19,650
So as you can see, the effect here is
that we're going to do basically PLSA.

135
00:10:19,650 --> 00:10:24,020
But we're going to also try
to make the topic coverages

136
00:10:24,020 --> 00:10:28,860
on the two nodes that are strongly
connected to be similar.

137
00:10:28,860 --> 00:10:30,930
And we ensure their coverages are similar.

138
00:10:33,800 --> 00:10:37,860
So here are some of the several results,
from that paper.

139
00:10:37,860 --> 00:10:41,440
This is slide shows the record
results of using PLSA.

140
00:10:41,440 --> 00:10:45,917
And the data here is DBLP data,
bibliographic data,

141
00:10:45,917 --> 00:10:48,970
about research articles.

142
00:10:48,970 --> 00:10:55,140
And the experiments have to do with
using four communities of applications.

143
00:10:55,140 --> 00:10:56,800
IR information retrieval.

144
00:10:56,800 --> 00:10:59,400
DM stands for data mining.

145
00:10:59,400 --> 00:11:00,860
ML for machinery and web.

146
00:11:00,860 --> 00:11:05,240
There are four communities of articles,
and we were hoping

147
00:11:06,780 --> 00:11:14,590
to see that the topic mining can help
us uncover these four communities.

148
00:11:14,590 --> 00:11:19,860
But from these assembled topics that you
have seen here that are generated by PLSA.

149
00:11:19,860 --> 00:11:24,400
And PLSA is unable to generate
the four communities that

150
00:11:24,400 --> 00:11:26,750
correspond to our intuition.

151
00:11:26,750 --> 00:11:30,310
The reason was because they
are all mixed together and

152
00:11:30,310 --> 00:11:33,620
there are many words that
are shared by these communities.

153
00:11:33,620 --> 00:11:37,900
So it's not that easy to use
four topics to separate them.

154
00:11:37,900 --> 00:11:41,750
If we use more topics,
perhaps we will have more coherent topics.

155
00:11:42,960 --> 00:11:48,420
But what's interesting is that if we
use the NetPLSA where the network,

156
00:11:48,420 --> 00:11:54,180
the collaboration network in this case of
authors is used to impose constraints.

157
00:11:54,180 --> 00:11:57,210
And in this case we also use four topics.

158
00:11:57,210 --> 00:12:01,780
But Ned Pierre said we gave
much more meaningful topics.

159
00:12:01,780 --> 00:12:07,690
So here we'll see that these topics
correspond well to the four communities.

160
00:12:07,690 --> 00:12:09,340
The first is information retrieval.

161
00:12:09,340 --> 00:12:11,260
The second is data mining.

162
00:12:11,260 --> 00:12:12,410
Third is machine learning.

163
00:12:12,410 --> 00:12:13,970
And the fourth is web.

164
00:12:13,970 --> 00:12:18,771
So that separation was mostly
because of the influence of network

165
00:12:18,771 --> 00:12:24,000
where with leverage is
a collaboration network information.

166
00:12:24,000 --> 00:12:28,300
Essentially the people that
form a collaborating network

167
00:12:28,300 --> 00:12:32,280
would then be kind of assumed
to write about similar topics.

168
00:12:32,280 --> 00:12:35,210
And that's why we're going to
have more coherent topics.

169
00:12:35,210 --> 00:12:39,500
And if you just listen to text data
alone based on the occurrences,

170
00:12:39,500 --> 00:12:42,700
you won't get such coherent topics.

171
00:12:42,700 --> 00:12:45,720
Even though a topic model, like PLSA or

172
00:12:45,720 --> 00:12:50,790
LDA also should be able to
pick up co-occurring words.

173
00:12:50,790 --> 00:12:55,581
So in general the topics
that they generate represent

174
00:12:55,581 --> 00:12:58,680
words that co-occur each other.

175
00:12:58,680 --> 00:13:03,980
But still they cannot generate such
a coherent results as NetPLSA,

176
00:13:03,980 --> 00:13:07,700
showing that the network
contest is very useful here.

177
00:13:08,740 --> 00:13:13,143
Now a similar model could have been also
useful to to characterize the content

178
00:13:13,143 --> 00:13:16,270
associated with each
subnetwork of collaborations.

179
00:13:19,497 --> 00:13:24,585
So a more general view of text
mining in context of network is you

180
00:13:24,585 --> 00:13:29,870
treat text as living in a rich
information network environment.

181
00:13:29,870 --> 00:13:35,700
And that means we can connect all the
related data together as a big network.

182
00:13:35,700 --> 00:13:41,750
And text data can be associated with
a lot of structures in the network.

183
00:13:41,750 --> 00:13:46,420
For example, text data can be associated
with the nodes of the network, and

184
00:13:46,420 --> 00:13:51,100
that's basically what we just
discussed in the NetPLSA.

185
00:13:51,100 --> 00:13:56,614
But text data can be associated with age
as well, or paths or even subnetworks.

186
00:13:56,614 --> 00:14:01,010
And such a way to represent texts
that are in the big environment of

187
00:14:01,010 --> 00:14:04,170
all the context information
is very powerful.

188
00:14:04,170 --> 00:14:09,860
Because it allows to analyze all the data,
all the information together.

189
00:14:09,860 --> 00:14:16,130
And so in general, analysis of text
should be using the entire network

190
00:14:16,130 --> 00:14:21,270
information that's
related to the text data.

191
00:14:21,270 --> 00:14:23,350
So here's one suggested reading.

192
00:14:23,350 --> 00:14:27,742
And this is the paper about NetPLSA where
you can find more details about the model

193
00:14:27,742 --> 00:14:29,300
and how to make such a model.

194
00:14:29,300 --> 00:14:39,300
[MUSIC]


1
00:00:07,553 --> 00:00:12,636
So, I just showed you that empirically
the likelihood will converge,

2
00:00:12,636 --> 00:00:17,041
but theoretically it can also
be proved that EM algorithm will

3
00:00:17,041 --> 00:00:19,295
converge to a local maximum.

4
00:00:19,295 --> 00:00:24,925
So here's just an illustration of what
happened and a detailed explanation.

5
00:00:24,925 --> 00:00:29,613
This required more knowledge about that,

6
00:00:29,613 --> 00:00:36,910
some of that inequalities,
that we haven't really covered yet.

7
00:00:39,380 --> 00:00:45,040
So here what you see is on the X
dimension, we have a c0 value.

8
00:00:45,040 --> 00:00:46,799
This is a parameter that we have.

9
00:00:46,799 --> 00:00:49,714
On the y axis we see
the likelihood function.

10
00:00:49,714 --> 00:00:57,171
So this curve is the original
likelihood function,

11
00:00:57,171 --> 00:01:04,110
and this is the one that
we hope to maximize.

12
00:01:04,110 --> 00:01:06,630
And we hope to find a c0 value
at this point to maximize this.

13
00:01:06,630 --> 00:01:11,480
But in the case of Mitsumoto we can
not easily find an analytic solution

14
00:01:11,480 --> 00:01:12,470
to the problem.

15
00:01:12,470 --> 00:01:14,698
So, we have to resolve
the numerical errors, and

16
00:01:14,698 --> 00:01:16,457
the EM algorithm is such an algorithm.

17
00:01:16,457 --> 00:01:17,850
It's a Hill-Climb algorithm.

18
00:01:17,850 --> 00:01:22,490
That would mean you start
with some random guess.

19
00:01:22,490 --> 00:01:26,260
Let's say you start from here,
that's your starting point.

20
00:01:26,260 --> 00:01:32,090
And then you try to improve
this by moving this to

21
00:01:32,090 --> 00:01:35,420
another point where you can
have a higher likelihood.

22
00:01:35,420 --> 00:01:37,630
So that's the ideal hill climbing.

23
00:01:37,630 --> 00:01:43,030
And in the EM algorithm, the way we
achieve this is to do two things.

24
00:01:43,030 --> 00:01:46,940
First, we'll fix a lower
bound of likelihood function.

25
00:01:46,940 --> 00:01:48,628
So this is the lower bound.

26
00:01:48,628 --> 00:01:49,128
See here.

27
00:01:51,010 --> 00:01:57,560
And once we fit the lower bound,
we can then maximize the lower bound.

28
00:01:57,560 --> 00:01:59,420
And of course, the reason why this works,

29
00:01:59,420 --> 00:02:02,850
is because the lower bound
is much easier to optimize.

30
00:02:02,850 --> 00:02:05,780
So we know our current guess is here.

31
00:02:05,780 --> 00:02:11,530
And by maximizing the lower bound,
we'll move this point to the top.

32
00:02:11,530 --> 00:02:12,030
To here.

33
00:02:13,300 --> 00:02:14,650
Right?

34
00:02:14,650 --> 00:02:20,150
And we can then map to the original
likelihood function, we find this point.

35
00:02:20,150 --> 00:02:25,600
Because it's a lower bound, we are
guaranteed to improve this guess, right?

36
00:02:25,600 --> 00:02:30,570
Because we improve our lower bound and
then the original likelihood

37
00:02:30,570 --> 00:02:35,040
curve which is above this lower bound
will definitely be improved as well.

38
00:02:36,310 --> 00:02:39,090
So we already know it's
improving the lower bound.

39
00:02:39,090 --> 00:02:42,440
So we definitely improve this
original likelihood function,

40
00:02:42,440 --> 00:02:47,253
which is above this lower bound.

41
00:02:47,253 --> 00:02:49,770
So, in our example,

42
00:02:49,770 --> 00:02:53,520
the current guess is parameter value
given by the current generation.

43
00:02:53,520 --> 00:02:57,660
And then the next guess is
the re-estimated parameter values.

44
00:02:57,660 --> 00:03:01,110
From this illustration you
can see the next guess

45
00:03:01,110 --> 00:03:03,620
is always better than the current guess.

46
00:03:03,620 --> 00:03:06,930
Unless it has reached the maximum,
where it will be stuck there.

47
00:03:06,930 --> 00:03:08,008
So the two would be equal.

48
00:03:08,008 --> 00:03:12,821
So, the E-step is basically

49
00:03:12,821 --> 00:03:17,650
to compute this lower bound.

50
00:03:17,650 --> 00:03:22,061
We don't directly just compute
this likelihood function but

51
00:03:22,061 --> 00:03:25,452
we compute the length of
the variable values and

52
00:03:25,452 --> 00:03:28,990
these are basically a part
of this lower bound.

53
00:03:28,990 --> 00:03:31,150
This helps determine the lower bound.

54
00:03:31,150 --> 00:03:34,460
The M-step on the other hand is
to maximize the lower bound.

55
00:03:34,460 --> 00:03:37,480
It allows us to move
parameters to a new point.

56
00:03:37,480 --> 00:03:41,460
And that's why EM algorithm is guaranteed
to converge to a local maximum.

57
00:03:42,490 --> 00:03:46,720
Now, as you can imagine,
when we have many local maxima,

58
00:03:46,720 --> 00:03:50,100
we also have to repeat the EM
algorithm multiple times.

59
00:03:50,100 --> 00:03:54,340
In order to figure out which one
is the actual global maximum.

60
00:03:54,340 --> 00:03:59,070
And this actually in general is a
difficult problem in numeral optimization.

61
00:03:59,070 --> 00:04:02,689
So here for
example had we started from here,

62
00:04:02,689 --> 00:04:06,223
then we gradually just
climb up to this top.

63
00:04:06,223 --> 00:04:11,227
So, that's not optimal, and
we'd like to climb up all the way to here,

64
00:04:11,227 --> 00:04:16,575
so the only way to climb up to this gear
is to start from somewhere here or here.

65
00:04:16,575 --> 00:04:22,767
So, in the EM algorithm, we generally
would have to start from different points

66
00:04:22,767 --> 00:04:27,880
or have some other way to determine
a good initial starting point.

67
00:04:29,840 --> 00:04:34,320
To summarize in this lecture we
introduced the EM algorithm.

68
00:04:34,320 --> 00:04:38,683
This is a general algorithm for computing
maximum maximum likelihood estimate of all

69
00:04:38,683 --> 00:04:42,153
kinds of models, so
not just for our simple model.

70
00:04:42,153 --> 00:04:46,468
And it's a hill-climbing algorithm, so it
can only converge to a local maximum and

71
00:04:46,468 --> 00:04:48,250
it will depend on initial points.

72
00:04:49,770 --> 00:04:55,414
The general idea is that we will have
two steps to improve the estimate of.

73
00:04:55,414 --> 00:05:00,270
In the E-step we roughly [INAUDIBLE]
how many there are by predicting values

74
00:05:00,270 --> 00:05:05,560
of useful hidden variables that we
would use to simplify the estimation.

75
00:05:05,560 --> 00:05:10,056
In our case, this is the distribution
that has been used to generate the word.

76
00:05:10,056 --> 00:05:15,750
In the M-step then we would exploit
such augmented data which would make

77
00:05:15,750 --> 00:05:20,790
it easier to estimate the distribution,
to improve the estimate of parameters.

78
00:05:20,790 --> 00:05:24,860
Here improve is guaranteed in
terms of the likelihood function.

79
00:05:24,860 --> 00:05:30,240
Note that it's not necessary that we
will have a stable convergence of

80
00:05:30,240 --> 00:05:35,260
parameter value even though the likelihood
function is ensured to increase.

81
00:05:35,260 --> 00:05:40,370
There are some properties that have to
be satisfied in order for the parameters

82
00:05:40,370 --> 00:05:44,640
also to convert into some stable value.

83
00:05:47,500 --> 00:05:50,790
Now here data augmentation
is done probabilistically.

84
00:05:50,790 --> 00:05:51,360
That means,

85
00:05:51,360 --> 00:05:54,830
we're not going to just say exactly
what's the value of a hidden variable.

86
00:05:54,830 --> 00:05:59,390
But we're going to have a probability
distribution over the possible values of

87
00:05:59,390 --> 00:06:01,140
these hidden variables.

88
00:06:01,140 --> 00:06:05,990
So this causes a split of counts
of events probabilistically.

89
00:06:07,430 --> 00:06:12,783
And in our case we'll split the word
counts between the two distributions.

90
00:06:12,783 --> 00:06:22,783
[MUSIC]


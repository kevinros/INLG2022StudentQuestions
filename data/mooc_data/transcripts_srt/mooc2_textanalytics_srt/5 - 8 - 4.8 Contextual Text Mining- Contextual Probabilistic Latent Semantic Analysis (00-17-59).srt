1
00:00:00,401 --> 00:00:07,552
[MUSIC]

2
00:00:07,552 --> 00:00:10,524
This lecture is about
a specific technique for

3
00:00:10,524 --> 00:00:16,097
Contextual Text Mining called Contextual
Probabilistic Latent Semantic Analysis.

4
00:00:19,162 --> 00:00:23,930
In this lecture, we're going to continue
discussing Contextual Text Mining.

5
00:00:23,930 --> 00:00:28,990
And we're going to introduce Contextual
Probablitistic Latent Semantic Analysis

6
00:00:28,990 --> 00:00:32,630
as exchanging of POS for
doing contextual text mining.

7
00:00:34,390 --> 00:00:40,310
Recall that in contextual text mining
we hope to analyze topics in text,

8
00:00:40,310 --> 00:00:42,285
in consideration of the context so

9
00:00:42,285 --> 00:00:46,950
that we can associate the topics with a
property of the context were interesting.

10
00:00:48,240 --> 00:00:54,033
So in this approach, contextual
probabilistic latent semantic analysis,

11
00:00:54,033 --> 00:00:58,487
or CPLSA, the main idea is to
express to the add interesting

12
00:00:58,487 --> 00:01:01,890
context variables into a generating model.

13
00:01:03,150 --> 00:01:06,860
Recall that before when we generate
the text we generally assume we'll start

14
00:01:06,860 --> 00:01:10,730
wIth some topics, and
then assemble words from some topics.

15
00:01:10,730 --> 00:01:18,130
But here, we're going to add context
variables, so that the coverage of topics,

16
00:01:18,130 --> 00:01:23,500
and also the content of topics
would be tied in context.

17
00:01:23,500 --> 00:01:27,607
Or in other words, we're going to let
the context Influence both coverage and

18
00:01:27,607 --> 00:01:28,900
the content of a topic.

19
00:01:31,172 --> 00:01:37,370
The consequences that this will enable
us to discover contextualized topics.

20
00:01:37,370 --> 00:01:41,320
Make the topics more interesting,
more meaningful.

21
00:01:41,320 --> 00:01:46,120
Because we can then have topics
that can be interpreted as

22
00:01:46,120 --> 00:01:49,070
specifically to a particular
context that we are interested in.

23
00:01:49,070 --> 00:01:50,590
For example, a particular time period.

24
00:01:52,020 --> 00:01:55,639
As an extension of PLSA model,

25
00:01:55,639 --> 00:02:01,330
CPLSA does the following changes.

26
00:02:01,330 --> 00:02:05,770
Firstly it would model the conditional
likelihood of text given context.

27
00:02:07,110 --> 00:02:12,990
That clearly suggests that the generation
of text would then depend on context,

28
00:02:12,990 --> 00:02:16,520
and that allows us to bring
context into the generative model.

29
00:02:18,230 --> 00:02:22,300
Secondly, it makes two specific
assumptions about the dependency

30
00:02:22,300 --> 00:02:24,650
of topics on context.

31
00:02:24,650 --> 00:02:28,420
One is to assume that depending on
the context, depending on different time

32
00:02:28,420 --> 00:02:33,630
periods or different locations, we assume
that there are different views of a topic

33
00:02:33,630 --> 00:02:37,370
or different versions of word
descriptions that characterize a topic.

34
00:02:38,540 --> 00:02:42,260
And this assumption allows
us to discover different

35
00:02:42,260 --> 00:02:45,430
variations of the same topic
in different contexts.

36
00:02:46,500 --> 00:02:53,059
The other is that we assume the topic
coverage also depends on the context.

37
00:02:55,150 --> 00:02:56,810
That means depending on the time or

38
00:02:56,810 --> 00:02:59,630
location, we might cover
topics differently.

39
00:03:00,670 --> 00:03:03,890
Again, this dependency
would then allow us to

40
00:03:03,890 --> 00:03:08,680
capture the association of
topics with specific contexts.

41
00:03:08,680 --> 00:03:14,540
We can still use the EM algorithm to solve
the problem of parameter estimation.

42
00:03:16,280 --> 00:03:22,520
And in this case, the estimated parameters
would naturally contain context variables.

43
00:03:22,520 --> 00:03:23,590
And in particular,

44
00:03:23,590 --> 00:03:29,940
a lot of conditional probabilities
of topics given certain context.

45
00:03:29,940 --> 00:03:33,090
And this is what allows you
to do contextual text mining.

46
00:03:33,090 --> 00:03:34,610
So this is the basic idea.

47
00:03:35,750 --> 00:03:41,470
Now, we don't have time to
introduce this model in detail,

48
00:03:41,470 --> 00:03:45,700
but there are references here that you
can look into to know more detail.

49
00:03:45,700 --> 00:03:52,120
Here I just want to explain the high
level ideas in more detail.

50
00:03:52,120 --> 00:03:55,610
Particularly I want to explain
the generation process.

51
00:03:55,610 --> 00:04:00,330
Of text data that has context
associated in such a model.

52
00:04:01,550 --> 00:04:05,660
So as you see here, we can assume
there are still multiple topics.

53
00:04:05,660 --> 00:04:11,410
For example, some topics might represent
a themes like a government response,

54
00:04:11,410 --> 00:04:14,270
donation Or the city of New Orleans.

55
00:04:14,270 --> 00:04:18,803
Now this example is in the context
of Hurricane Katrina and

56
00:04:18,803 --> 00:04:20,570
that hit New Orleans.

57
00:04:22,915 --> 00:04:27,400
Now as you can see we
assume there are different

58
00:04:27,400 --> 00:04:31,548
views associated with each of the topics.

59
00:04:31,548 --> 00:04:36,530
And these are shown as View 1,
View 2, View 3.

60
00:04:36,530 --> 00:04:41,475
Each view is a different
version of word distributions.

61
00:04:41,475 --> 00:04:44,715
And these views are tied
to some context variables.

62
00:04:44,715 --> 00:04:50,125
For example, tied to the location Texas,
or the time July 2005,

63
00:04:50,125 --> 00:04:54,475
or the occupation of the author
being a sociologist.

64
00:04:56,205 --> 00:05:01,560
Now, on the right side, now we assume
the document has context information.

65
00:05:01,560 --> 00:05:04,370
So the time is known to be July 2005.

66
00:05:04,370 --> 00:05:06,710
The location is Texas, etc.

67
00:05:06,710 --> 00:05:11,410
And such context information is
what we hope to model as well.

68
00:05:11,410 --> 00:05:13,300
So we're not going to just model the text.

69
00:05:15,100 --> 00:05:20,980
And so one idea here is to model
the variations of top content and

70
00:05:20,980 --> 00:05:21,920
various content.

71
00:05:21,920 --> 00:05:25,970
And this gives us different views
of the water distributions.

72
00:05:27,720 --> 00:05:32,360
Now on the bottom you will see the theme
coverage of top Coverage might also vary

73
00:05:32,360 --> 00:05:39,310
according to these context
because in the case

74
00:05:39,310 --> 00:05:44,320
of a location like Texas, people might
want to cover the red topics more.

75
00:05:44,320 --> 00:05:46,130
That's New Orleans.

76
00:05:46,130 --> 00:05:47,690
That's visualized here.

77
00:05:47,690 --> 00:05:50,930
But in a certain time period,

78
00:05:50,930 --> 00:05:56,280
maybe Particular topic and
will be covered more.

79
00:05:56,280 --> 00:06:00,980
So this variation is
also considered in CPLSA.

80
00:06:00,980 --> 00:06:07,685
So to generate the searcher document With
context, with first also choose a view.

81
00:06:08,695 --> 00:06:14,055
And this view of course now could
be from any of these contexts.

82
00:06:14,055 --> 00:06:17,080
Let's say, we have taken this
view that depends on the time.

83
00:06:17,080 --> 00:06:18,310
In the middle.

84
00:06:18,310 --> 00:06:21,850
So now, we will have a specific
version of word distributions.

85
00:06:21,850 --> 00:06:25,030
Now, you can see some probabilities
of words for each topic.

86
00:06:26,710 --> 00:06:28,830
Now, once we have chosen a view,

87
00:06:28,830 --> 00:06:34,400
now the situation will be very similar
to what happened in standard ((PRSA))

88
00:06:34,400 --> 00:06:38,860
We assume we have got word distribution
associated with each topic, right?

89
00:06:39,870 --> 00:06:43,070
And then next, we will also choose
a coverage from the bottom, so

90
00:06:43,070 --> 00:06:47,988
we're going to choose a particular
coverage, and that coverage,

91
00:06:47,988 --> 00:06:55,305
before is fixed in PLSA, and
assigned to a particular document.

92
00:06:55,305 --> 00:06:57,825
Each document has just one
coverage distribution.

93
00:06:58,885 --> 00:07:03,925
Now here, because we consider context, so
the distribution of topics or the coverage

94
00:07:03,925 --> 00:07:08,770
of Topics can vary depending on the
context that has influenced the coverage.

95
00:07:10,020 --> 00:07:13,470
So, for example,
we might pick a particular coverage.

96
00:07:13,470 --> 00:07:19,090
Let's say in this case we picked
a document specific coverage.

97
00:07:20,590 --> 00:07:23,440
Now with the coverage and
these word distributions

98
00:07:23,440 --> 00:07:26,590
we can generate a document in
exactly the same way as in PLSA.

99
00:07:26,590 --> 00:07:32,450
So what it means, we're going to
use the coverage to choose a topic,

100
00:07:32,450 --> 00:07:34,880
to choose one of these three topics.

101
00:07:34,880 --> 00:07:38,230
Let's say we have picked the yellow topic.

102
00:07:38,230 --> 00:07:43,450
Then we'll draw a word from this
particular topic on the top.

103
00:07:44,760 --> 00:07:46,880
Okay, so
we might get a word like government.

104
00:07:46,880 --> 00:07:50,840
And then next time we might
choose a different topic, and

105
00:07:50,840 --> 00:07:53,640
we'll get donate, etc.

106
00:07:53,640 --> 00:07:55,550
Until we generate all the words.

107
00:07:55,550 --> 00:07:58,550
And this is basically
the same process as in PLSA.

108
00:08:00,200 --> 00:08:05,220
So the main difference is
when we obtain the coverage.

109
00:08:05,220 --> 00:08:11,250
And the word distribution,
we let the context influence our choice So

110
00:08:11,250 --> 00:08:16,050
in other words we have extra switches
that are tied to these contacts that will

111
00:08:16,050 --> 00:08:20,950
control the choices of different views
of topics and the choices of coverage.

112
00:08:22,010 --> 00:08:25,430
And naturally the model we have
more parameters to estimate.

113
00:08:25,430 --> 00:08:29,010
But once we can estimate those
parameters that involve the context,

114
00:08:29,010 --> 00:08:33,080
then we will be able to understand
the context specific views of topics,

115
00:08:33,080 --> 00:08:36,020
or context specific coverages of topics.

116
00:08:36,020 --> 00:08:38,850
And this is precisely what we
want in contextual text mining.

117
00:08:40,450 --> 00:08:42,950
So here are some simple results.

118
00:08:42,950 --> 00:08:44,340
From using such a model.

119
00:08:44,340 --> 00:08:48,240
Not necessary exactly the same model,
but similar models.

120
00:08:48,240 --> 00:08:50,860
So on this slide you see
some sample results of

121
00:08:50,860 --> 00:08:54,950
comparing news articles about Iraq War and
Afghanistan War.

122
00:08:56,315 --> 00:09:02,855
Now we have about 30 articles on Iraq
wa,r and 26 articles on Afghanistan war.

123
00:09:02,855 --> 00:09:08,852
And in this case,
the goal is to review the common topic.

124
00:09:08,852 --> 00:09:11,332
It's covered in both sets of articles and

125
00:09:11,332 --> 00:09:17,352
the differences of variations of
the topic in each of the two collections.

126
00:09:18,622 --> 00:09:23,400
So in this case the context is explicitly
specified by the topic or collection.

127
00:09:25,040 --> 00:09:30,420
And we see the results here
show that there is a common

128
00:09:30,420 --> 00:09:36,040
theme that's corresponding to
Cluster 1 here in this column.

129
00:09:36,040 --> 00:09:42,260
And there is a common theme indicting that
United Nations is involved in both Wars.

130
00:09:42,260 --> 00:09:45,630
It's a common topic covered
in both sets of articles.

131
00:09:45,630 --> 00:09:48,970
And that's indicated by the high
probability words shown here, united and

132
00:09:48,970 --> 00:09:49,860
nations.

133
00:09:51,160 --> 00:09:54,680
Now if you know the background,
of course this is not surprising and

134
00:09:54,680 --> 00:10:00,340
this topic is indeed very
relevant to both wars.

135
00:10:00,340 --> 00:10:04,900
If you look at the column further and
then what's interesting's that the next

136
00:10:04,900 --> 00:10:09,336
two cells of word
distributions actually tell us

137
00:10:09,336 --> 00:10:14,790
collection specific variations
of the topic of United Nations.

138
00:10:14,790 --> 00:10:16,660
So it indicates that the Iraq War,

139
00:10:16,660 --> 00:10:21,060
United Nations was more involved
in weapons factions, whereas in

140
00:10:21,060 --> 00:10:25,710
the Afghanistan War it was more involved
in maybe aid to Northern Alliance.

141
00:10:25,710 --> 00:10:29,060
It's a different variation of
the topic of United Nations.

142
00:10:30,100 --> 00:10:33,140
So this shows that by
bringing the context.

143
00:10:33,140 --> 00:10:36,215
In this case different the walls or
different the collection of texts.

144
00:10:36,215 --> 00:10:40,034
We can have topical variations
tied to these contexts,

145
00:10:40,034 --> 00:10:45,250
to review the differences of coverage
of the United Nations in the two wars.

146
00:10:46,290 --> 00:10:50,200
Now similarly if you look at
the second cluster Class two,

147
00:10:50,200 --> 00:10:52,710
it has to do with the killing of people,
and, again,

148
00:10:52,710 --> 00:10:56,320
it's not surprising if you know
the background about wars.

149
00:10:56,320 --> 00:10:59,660
All the wars involve killing of people,
but

150
00:10:59,660 --> 00:11:03,640
imagine if you are not familiar
with the text collections.

151
00:11:03,640 --> 00:11:05,120
We have a lot of text articles, and

152
00:11:05,120 --> 00:11:10,230
such a technique can reveal the common
topics covered in both sets of articles.

153
00:11:10,230 --> 00:11:14,715
It can be used to review common topics
in multiple sets of articles as well.

154
00:11:14,715 --> 00:11:19,581
If you look at of course in
that column of cluster two,

155
00:11:19,581 --> 00:11:26,143
you see variations of killing of people
and that corresponds to different contexts

156
00:11:28,279 --> 00:11:31,582
And here is another example of results

157
00:11:31,582 --> 00:11:36,440
obtained from blog articles
about Hurricane Katrina.

158
00:11:37,470 --> 00:11:42,320
In this case,
what you see here is visualization of

159
00:11:42,320 --> 00:11:46,090
the trends of topics over time.

160
00:11:47,240 --> 00:11:52,980
And the top one shows just
the temporal trends of two topics.

161
00:11:52,980 --> 00:11:58,980
One is oil price, and one is about
the flooding of the city of New Orleans.

162
00:12:00,060 --> 00:12:06,280
Now these topics are obtained from
blog articles about Hurricane Katrina.

163
00:12:07,300 --> 00:12:09,395
And people talk about these topics.

164
00:12:09,395 --> 00:12:12,370
And end up teaching to some other topics.

165
00:12:12,370 --> 00:12:15,000
But the visualisation shows
that with this technique,

166
00:12:15,000 --> 00:12:18,020
we can have conditional
distribution of time.

167
00:12:18,020 --> 00:12:19,660
Given a topic.

168
00:12:19,660 --> 00:12:23,420
So this allows us to plot
this conditional probability

169
00:12:23,420 --> 00:12:26,000
the curve is like what you're seeing here.

170
00:12:26,000 --> 00:12:31,560
We see that, initially, the two
curves tracked each other very well.

171
00:12:31,560 --> 00:12:40,010
But later we see the topic of New Orleans
was mentioned again but oil price was not.

172
00:12:40,010 --> 00:12:44,060
And this turns out to be

173
00:12:44,060 --> 00:12:49,010
the time period when another hurricane,
hurricane Rita hit the region.

174
00:12:49,010 --> 00:12:52,470
And that apparently triggered more
discussion about the flooding of the city.

175
00:12:54,900 --> 00:13:00,010
The bottom curve shows
the coverage of this topic

176
00:13:00,010 --> 00:13:05,320
about flooding of the city by block
articles in different locations.

177
00:13:05,320 --> 00:13:11,620
And it also shows some shift of
coverage that might be related to

178
00:13:11,620 --> 00:13:19,150
people's migrating from the state
of Louisiana to Texas for example.

179
00:13:20,570 --> 00:13:25,650
So in this case we can see the time can
be used as context to review trends of

180
00:13:25,650 --> 00:13:26,150
topics.

181
00:13:27,780 --> 00:13:33,070
These are some additional
results on spacial patterns.

182
00:13:33,070 --> 00:13:37,850
In this case it was about
the topic of government response.

183
00:13:37,850 --> 00:13:41,690
And there was some criticism about
the slow response of government

184
00:13:41,690 --> 00:13:42,649
in the case of Hurricane Katrina.

185
00:13:44,020 --> 00:13:48,280
And the discussion now is
covered in different locations.

186
00:13:48,280 --> 00:13:54,260
And these visualizations show the coverage
in different weeks of the event.

187
00:13:54,260 --> 00:13:59,610
And initially it's covered
mostly in the victim states,

188
00:13:59,610 --> 00:14:05,530
in the South, but then gradually
spread into other locations.

189
00:14:05,530 --> 00:14:09,760
But in week four,
which is shown on the bottom left,

190
00:14:09,760 --> 00:14:14,370
we see a pattern that's very similar
to the first week on the top left.

191
00:14:14,370 --> 00:14:18,700
And that's when again
Hurricane Rita hit in the region.

192
00:14:18,700 --> 00:14:22,540
So such a technique would allow
us to use location as context

193
00:14:22,540 --> 00:14:24,960
to examine their issues of topics.

194
00:14:24,960 --> 00:14:27,280
And of course the moral
is completely general so

195
00:14:27,280 --> 00:14:30,980
you can apply this to any
other connections of text.

196
00:14:30,980 --> 00:14:32,850
To review spatial temporal patterns.

197
00:14:34,460 --> 00:14:37,390
His view found another application
of this kind of model,

198
00:14:37,390 --> 00:14:41,960
where we look at the use of the model for
event impact analysis.

199
00:14:43,290 --> 00:14:46,370
So here we're looking at the research
articles information retrieval.

200
00:14:46,370 --> 00:14:49,480
IR, particularly SIGIR papers.

201
00:14:49,480 --> 00:14:53,180
And the topic we are focusing on
is about the retrieval models.

202
00:14:53,180 --> 00:14:58,440
And you can see the top words with high
probability about this model on the left.

203
00:14:59,580 --> 00:15:04,290
And then we hope to examine
the impact of two events.

204
00:15:04,290 --> 00:15:08,290
One is a start of TREC, for
Text and Retrieval Conference.

205
00:15:08,290 --> 00:15:11,459
This is a major evaluation
sponsored by U.S.

206
00:15:11,459 --> 00:15:16,722
government, and was launched in 1992 or
around that time.

207
00:15:16,722 --> 00:15:20,690
And that is known to have made a impact on

208
00:15:20,690 --> 00:15:22,790
the topics of research
information retrieval.

209
00:15:23,870 --> 00:15:28,680
The other is the publication of
a seminal paper, by Croft and Porte.

210
00:15:28,680 --> 00:15:31,850
This is about a language model
approach to information retrieval.

211
00:15:31,850 --> 00:15:36,440
It's also known to have made a high
impact on information retrieval research.

212
00:15:36,440 --> 00:15:39,780
So we hope to use this kind of
model to understand impact.

213
00:15:39,780 --> 00:15:44,090
The idea here is simply to
use the time as context.

214
00:15:44,090 --> 00:15:48,585
And use these events to divide
the time periods into a period before.

215
00:15:48,585 --> 00:15:51,397
For the event and
another after this event.

216
00:15:51,397 --> 00:15:54,417
And then we can compare
the differences of the topics.

217
00:15:54,417 --> 00:15:57,875
The and the variations, etc.

218
00:15:57,875 --> 00:16:02,750
So in this case,
the results show before track the study of

219
00:16:02,750 --> 00:16:07,120
retrieval models was mostly a vector
space model, Boolean model etc.

220
00:16:07,120 --> 00:16:08,800
But the after Trec,

221
00:16:08,800 --> 00:16:13,975
apparently the study of retrieval models
have involved a lot of other words.

222
00:16:13,975 --> 00:16:18,440
That seems to suggest some
different retrieval tasks, so for

223
00:16:18,440 --> 00:16:22,980
example, email was used in
the enterprise search tasks and

224
00:16:22,980 --> 00:16:26,550
subtopical retrieval was another
task later introduced by Trec.

225
00:16:28,200 --> 00:16:32,461
On the bottom,
we see the variations that are correlated

226
00:16:32,461 --> 00:16:36,300
with the propagation of
the language model paper.

227
00:16:36,300 --> 00:16:40,631
Before, we have those classic
probability risk model,

228
00:16:40,631 --> 00:16:44,600
logic model, Boolean etc., but after 1998,

229
00:16:44,600 --> 00:16:50,430
we see clear dominance of language
model as probabilistic models.

230
00:16:50,430 --> 00:16:54,580
And we see words like language model,
estimation of parameters, etc.

231
00:16:54,580 --> 00:17:00,764
So this technique here can use events as
context to understand the impact of event.

232
00:17:00,764 --> 00:17:03,403
Again the technique is generals so

233
00:17:03,403 --> 00:17:07,370
you can use this to analyze
the impact of any event.

234
00:17:07,370 --> 00:17:10,240
Here are some suggested readings.

235
00:17:11,940 --> 00:17:20,090
The first is paper about simple staging of
psi to label cross-collection comparison.

236
00:17:21,270 --> 00:17:24,610
It's to perform comparative
text mining to allow us to

237
00:17:24,610 --> 00:17:27,410
extract common topics shared
by multiple collections.

238
00:17:27,410 --> 00:17:29,930
And there are variations
in each collection.

239
00:17:31,010 --> 00:17:35,540
The second one is the main
paper about the CPLSA model.

240
00:17:35,540 --> 00:17:38,830
Was a discussion of a lot of applications.

241
00:17:38,830 --> 00:17:44,889
The third one has a lot of details
about the special temporal patterns for

242
00:17:44,889 --> 00:17:47,679
the Hurricane Katrina example.

243
00:17:47,679 --> 00:17:57,679
[MUSIC]


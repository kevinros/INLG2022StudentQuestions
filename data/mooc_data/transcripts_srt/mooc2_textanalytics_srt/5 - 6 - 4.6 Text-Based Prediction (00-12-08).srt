1
00:00:00,025 --> 00:00:05,203
[SOUND] This lecture is about

2
00:00:05,203 --> 00:00:11,060
the Text-Based Prediction.

3
00:00:11,060 --> 00:00:15,850
In this lecture, we're going to
start talking about the mining

4
00:00:15,850 --> 00:00:21,420
a different kind of knowledge,
as you can see here on this slide.

5
00:00:21,420 --> 00:00:27,980
Namely we're going to use text
data to infer values of some other

6
00:00:27,980 --> 00:00:33,830
variables in the real world that may
not be directly related to the text.

7
00:00:33,830 --> 00:00:36,660
Or only remotely related to text data.

8
00:00:36,660 --> 00:00:39,810
So this is very different
from content analysis or

9
00:00:39,810 --> 00:00:44,680
topic mining where we directly
characterize the content of text.

10
00:00:44,680 --> 00:00:48,830
It's also different from opinion mining or
sentiment analysis,

11
00:00:48,830 --> 00:00:54,090
which still have to do is
characterizing mostly the content.

12
00:00:54,090 --> 00:00:59,330
Only that we focus more
on the subject of content

13
00:00:59,330 --> 00:01:03,020
which reflects what we know
about the opinion holder.

14
00:01:05,000 --> 00:01:08,630
But this only provides limited
review of what we can predict.

15
00:01:10,320 --> 00:01:15,000
In this lecture and the following
lectures, we're going to talk more about

16
00:01:15,000 --> 00:01:20,830
how we can predict more
Information about the world.

17
00:01:20,830 --> 00:01:26,930
How can we get the sophisticated patterns
of text together with other kind of data?

18
00:01:28,360 --> 00:01:32,590
It would be useful first to take a look
at the big picture of prediction, and

19
00:01:32,590 --> 00:01:36,450
data mining in general, and
I call this data mining loop.

20
00:01:36,450 --> 00:01:41,375
So the picture that you are seeing right
now is that there are multiple sensors,

21
00:01:41,375 --> 00:01:43,067
including human sensors,

22
00:01:43,067 --> 00:01:46,982
to report what we have seen in
the real world in the form of data.

23
00:01:46,982 --> 00:01:50,840
Of course the data in the form
of non-text data, and text data.

24
00:01:51,960 --> 00:01:56,010
And our goal is to see if we
can predict some values of

25
00:01:56,010 --> 00:01:59,480
important real world
variables that matter to us.

26
00:01:59,480 --> 00:02:05,850
For example, someone's house condition,
or the weather, or etc.

27
00:02:05,850 --> 00:02:11,020
And so these variables would be important
because we might want to act on that.

28
00:02:11,020 --> 00:02:14,870
We might want to make
decisions based on that.

29
00:02:14,870 --> 00:02:18,070
So how can we get from the data
to these predicted values?

30
00:02:18,070 --> 00:02:22,650
Well in general we'll first have to do
data mining and analysis of the data.

31
00:02:23,760 --> 00:02:28,540
Because we, in general, should treat
all the data that we collected

32
00:02:30,190 --> 00:02:32,820
in such a prediction problem set up.

33
00:02:32,820 --> 00:02:37,150
We are very much interested in
joint mining of non-text and

34
00:02:37,150 --> 00:02:39,730
text data, which should
combine all the data together.

35
00:02:41,830 --> 00:02:45,850
And then, through analysis,
generally there

36
00:02:45,850 --> 00:02:50,850
are multiple predictors of this
interesting variable to us.

37
00:02:50,850 --> 00:02:52,910
And we call these features.

38
00:02:52,910 --> 00:02:56,490
And these features can then be
put into a predictive model,

39
00:02:56,490 --> 00:03:01,240
to actually predict the value
of any interesting variable.

40
00:03:02,610 --> 00:03:06,850
So this then allows us
to change the world.

41
00:03:06,850 --> 00:03:12,490
And so
this basically is the general process for

42
00:03:12,490 --> 00:03:15,810
making a prediction based on data,
including the test data.

43
00:03:17,010 --> 00:03:20,290
Now it's important to emphasize
that a human actually

44
00:03:20,290 --> 00:03:23,080
plays a very important
role in this process.

45
00:03:24,460 --> 00:03:27,760
Especially because of
the involvement of text data.

46
00:03:27,760 --> 00:03:32,010
So human first would be involved
in the mining of the data.

47
00:03:32,010 --> 00:03:36,030
It would control the generation
of these features.

48
00:03:36,030 --> 00:03:39,200
And it would also help us
understand the text data,

49
00:03:39,200 --> 00:03:43,350
because text data are created
to be consumed by humans.

50
00:03:43,350 --> 00:03:46,890
Humans are the best in consuming or
interpreting text data.

51
00:03:48,280 --> 00:03:52,750
But when there are, of course, a lot of
text data then machines have to help and

52
00:03:52,750 --> 00:03:54,490
that's why we need to do text data mining.

53
00:03:55,670 --> 00:04:00,900
Sometimes machines can see patterns in
a lot of data that humans may not see.

54
00:04:00,900 --> 00:04:03,570
But in general human would
play an important role in

55
00:04:03,570 --> 00:04:07,010
analyzing some text data, or applications.

56
00:04:07,010 --> 00:04:11,000
Next, human also must be involved
in predictive model building and

57
00:04:11,000 --> 00:04:13,250
adjusting or testing.

58
00:04:13,250 --> 00:04:17,530
So in particular, we will have a lot
of domain knowledge about the problem

59
00:04:17,530 --> 00:04:22,170
of prediction that we can build
into this predictive model.

60
00:04:22,170 --> 00:04:28,020
And then next, of course, when we have
predictive values for the variables,

61
00:04:28,020 --> 00:04:32,670
then humans would be involved in
taking actions to change a word or

62
00:04:32,670 --> 00:04:35,100
make decisions based on
these particular values.

63
00:04:36,730 --> 00:04:41,320
And finally it's interesting
that a human could be involved

64
00:04:41,320 --> 00:04:42,720
in controlling the sensors.

65
00:04:43,910 --> 00:04:48,450
And this is so that we can
adjust to the sensors to collect

66
00:04:48,450 --> 00:04:50,850
the most useful data for prediction.

67
00:04:52,490 --> 00:04:54,610
So that's why I call
this data mining loop.

68
00:04:54,610 --> 00:04:58,750
Because as we perturb the sensors,
it'll collect the new data and

69
00:04:58,750 --> 00:05:03,063
more useful data then we will
obtain more data for prediction.

70
00:05:03,063 --> 00:05:07,020
And this data generally will help
us improve the predicting accuracy.

71
00:05:07,020 --> 00:05:08,120
And in this loop,

72
00:05:08,120 --> 00:05:12,780
humans will recognize what additional
data will need to be collected.

73
00:05:12,780 --> 00:05:14,030
And machines, of course,

74
00:05:14,030 --> 00:05:19,040
help humans identify what data
should be collected next.

75
00:05:19,040 --> 00:05:23,640
In general, we want to collect data
that is most useful for learning.

76
00:05:23,640 --> 00:05:28,080
And there was actually a subarea in
machine learning called active learning

77
00:05:28,080 --> 00:05:29,650
that has to do with this.

78
00:05:29,650 --> 00:05:31,110
How do you identify data

79
00:05:32,590 --> 00:05:36,460
points that would be most helpful
in machine learning programs?

80
00:05:36,460 --> 00:05:37,470
If you can label them, right?

81
00:05:38,650 --> 00:05:39,310
So, in general,

82
00:05:39,310 --> 00:05:43,690
you can see there is a loop here from
data acquisition to data analysis.

83
00:05:43,690 --> 00:05:46,160
Or data mining to prediction of values.

84
00:05:46,160 --> 00:05:50,520
And to take actions to change the word,
and then observe what happens.

85
00:05:50,520 --> 00:05:54,780
And then you can then
decide what additional data

86
00:05:54,780 --> 00:05:58,280
have to be collected by
adjusting the sensors.

87
00:05:58,280 --> 00:06:02,630
Or from the prediction arrows,
you can also note what additional data

88
00:06:02,630 --> 00:06:06,160
we need to acquire in order to
improve the accuracy of prediction.

89
00:06:06,160 --> 00:06:09,410
And this big picture is
actually very general and

90
00:06:09,410 --> 00:06:14,660
it's reflecting a lot of important
applications of big data.

91
00:06:16,130 --> 00:06:19,930
So, it's useful to keep that in mind
while we are looking at some text

92
00:06:19,930 --> 00:06:20,690
mining techniques.

93
00:06:22,000 --> 00:06:26,280
So from text mining perspective and
we're interested in text based prediction.

94
00:06:26,280 --> 00:06:29,710
Of course, sometimes texts
alone can make predictions.

95
00:06:29,710 --> 00:06:31,625
And this is most useful for

96
00:06:31,625 --> 00:06:36,820
prediction about human behavior or
human preferences or opinions.

97
00:06:36,820 --> 00:06:40,480
But in general text data will be
put together as non-text data.

98
00:06:40,480 --> 00:06:43,100
So the interesting questions
here would be, first,

99
00:06:43,100 --> 00:06:47,030
how can we design effective predictors?

100
00:06:47,030 --> 00:06:52,190
And how do we generate such
effective predictors from text?

101
00:06:53,730 --> 00:06:57,730
And this question has been addressed to
some extent in some previous lectures

102
00:06:57,730 --> 00:07:03,390
where we talked about what kind of
features we can design for text data.

103
00:07:03,390 --> 00:07:06,860
And it has also been
addressed to some extent by

104
00:07:06,860 --> 00:07:10,430
talking about the other knowledge
that we can mine from text.

105
00:07:10,430 --> 00:07:15,950
So, for example, topic mining can be very
useful to generate the patterns or topic

106
00:07:15,950 --> 00:07:22,340
based indicators or predictors that can
be further fed into a predictive model.

107
00:07:22,340 --> 00:07:26,370
So topics can be intermediate
recognition of text.

108
00:07:26,370 --> 00:07:29,990
That would allow us to do
design high level features or

109
00:07:29,990 --> 00:07:35,780
predictors that are useful for
prediction of some other variable.

110
00:07:35,780 --> 00:07:40,340
It may be also generated from original
text data, it provides a much better

111
00:07:40,340 --> 00:07:45,470
implementation of the problem and
it serves as more effective predictors.

112
00:07:46,650 --> 00:07:50,650
And similarly similar analysis can
lead to such predictors, as well.

113
00:07:50,650 --> 00:07:52,650
So, those other data mining or

114
00:07:52,650 --> 00:07:56,300
text mining algorithms can be
used to generate predictors.

115
00:07:58,470 --> 00:08:03,020
The other question is, how can we join
the mine text and non-text data together?

116
00:08:03,020 --> 00:08:06,680
Now, this is a question that
we have not addressed yet.

117
00:08:06,680 --> 00:08:07,560
So, in this lecture,

118
00:08:07,560 --> 00:08:11,020
and in the following lectures,
we're going to address this problem.

119
00:08:11,020 --> 00:08:16,570
Because this is where we can generate much
more enriched features for prediction.

120
00:08:16,570 --> 00:08:21,580
And allows us to review a lot of
interesting knowledge about the world.

121
00:08:21,580 --> 00:08:23,738
These patterns that
are generated from text and

122
00:08:23,738 --> 00:08:30,520
non-text data themselves can sometimes,
already be useful for prediction.

123
00:08:30,520 --> 00:08:34,900
But, when they are put together
with many other predictors

124
00:08:34,900 --> 00:08:37,820
they can really help
improving the prediction.

125
00:08:39,150 --> 00:08:42,870
Basically, you can see text-based
prediction can actually serve as a unified

126
00:08:42,870 --> 00:08:47,790
framework to combine many text mining and
analysis techniques.

127
00:08:47,790 --> 00:08:54,080
Including topic mining and any content
mining techniques or segment analysis.

128
00:08:55,530 --> 00:09:01,120
The goal here is mainly to evoke
values of real-world variables.

129
00:09:01,120 --> 00:09:07,030
But in order to achieve the goal
we can do some other preparations.

130
00:09:07,030 --> 00:09:08,200
And these are subtasks.

131
00:09:08,200 --> 00:09:14,060
So one subtask could mine the content
of text data, like topic mining.

132
00:09:14,060 --> 00:09:18,360
And the other could be to mine
knowledge about the observer.

133
00:09:18,360 --> 00:09:20,040
So sentiment analysis, opinion.

134
00:09:21,320 --> 00:09:26,110
And both can help provide predictors for
the prediction problem.

135
00:09:27,830 --> 00:09:31,883
And of course we can also add non-text
data directly to the predicted model, but

136
00:09:31,883 --> 00:09:36,960
then non-text data also helps
provide a context for text analyst.

137
00:09:36,960 --> 00:09:42,520
And that further improves the topic
mining and the opinion analysis.

138
00:09:42,520 --> 00:09:48,040
And such improvement often leads to more
effective predictors for our problems.

139
00:09:48,040 --> 00:09:53,210
It would enlarge the space of patterns
of opinions of topics that we can

140
00:09:53,210 --> 00:09:58,400
mine from text and
that we'll discuss more later.

141
00:09:58,400 --> 00:10:00,395
So the joint analysis of text and

142
00:10:00,395 --> 00:10:04,480
non-text data can be actually
understood from two perspectives.

143
00:10:05,900 --> 00:10:10,300
One perspective,
we have non-text can help with testimony.

144
00:10:11,680 --> 00:10:15,310
Because non-text data can
provide a context for

145
00:10:15,310 --> 00:10:19,910
mining text data provide a way to
partition data in different ways.

146
00:10:19,910 --> 00:10:24,950
And this leads to a number of type of
techniques for contextual types of mining.

147
00:10:24,950 --> 00:10:29,730
And that's the mine text in
the context defined by non-text data.

148
00:10:29,730 --> 00:10:34,840
And you see this reference here, for
a large body of work, in this direction.

149
00:10:34,840 --> 00:10:37,730
And I will need to highlight some of them,
in the next lectures.

150
00:10:39,370 --> 00:10:42,207
Now, the other perspective is text data

151
00:10:42,207 --> 00:10:46,315
can help with non-text
data mining as well.

152
00:10:46,315 --> 00:10:49,875
And this is because text
data can help interpret

153
00:10:49,875 --> 00:10:52,635
patterns discovered from non-text data.

154
00:10:52,635 --> 00:10:56,525
Let's say you discover some frequent
patterns from non-text data.

155
00:10:56,525 --> 00:11:01,465
Now we can use the text data
associated with instances

156
00:11:01,465 --> 00:11:06,015
where the pattern occurs as well as
text data that is associated with

157
00:11:06,015 --> 00:11:09,200
instances where the pattern
doesn't look up.

158
00:11:09,200 --> 00:11:11,610
And this gives us two sets of text data.

159
00:11:11,610 --> 00:11:13,700
And then we can see what's the difference.

160
00:11:13,700 --> 00:11:18,180
And this difference in text data is
interpretable because text content is

161
00:11:18,180 --> 00:11:19,780
easy to digest.

162
00:11:19,780 --> 00:11:23,460
And that difference might
suggest some meaning for

163
00:11:23,460 --> 00:11:26,610
this pattern that we
found from non-text data.

164
00:11:26,610 --> 00:11:29,390
So, it helps interpret such patterns.

165
00:11:29,390 --> 00:11:31,890
And this technique is
called pattern annotation.

166
00:11:32,920 --> 00:11:37,160
And you can see this reference
listed here for more detail.

167
00:11:38,400 --> 00:11:40,590
So here are the references
that I just mentioned.

168
00:11:40,590 --> 00:11:43,630
The first is reference for
pattern annotation.

169
00:11:43,630 --> 00:11:49,140
The second is, Qiaozhu Mei's
dissertation on contextual text mining.

170
00:11:49,140 --> 00:11:53,880
It contains a large body of work on
contextual text mining techniques.

171
00:11:56,271 --> 00:12:06,271
[MUSIC]


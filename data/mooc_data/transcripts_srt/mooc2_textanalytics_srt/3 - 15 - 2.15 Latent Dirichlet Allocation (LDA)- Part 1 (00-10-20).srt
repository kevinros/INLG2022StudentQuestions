1
00:00:07,000 --> 00:00:14,567
This lecture is about that Latent Dirichlet Allocation or LDA.

2
00:00:14,567 --> 00:00:17,885
In this lecture, we are going to continue talking about topic models.

3
00:00:17,885 --> 00:00:22,468
In particular, we are going to talk about some extension of PLSA,

4
00:00:22,468 --> 00:00:30,858
and one of them is LDA or Latent Dirichlet Allocation.

5
00:00:30,858 --> 00:00:33,765
So the plan for this lecture is to cover two things.

6
00:00:33,765 --> 00:00:37,730
One is to extend the PLSA with prior knowledge and that

7
00:00:37,730 --> 00:00:41,795
would allow us to have in some sense a user-controlled PLSA,

8
00:00:41,795 --> 00:00:44,329
so it doesn't apply to they just listen to data,

9
00:00:44,329 --> 00:00:47,360
but also would listen to our needs.

10
00:00:47,360 --> 00:00:52,220
The second is to extend the PLSA as a generative model,

11
00:00:52,220 --> 00:00:54,158
a fully generative model.

12
00:00:54,158 --> 00:01:00,887
This has led to the development of Latent Dirichlet Allocation or LDA.

13
00:01:00,887 --> 00:01:04,520
So first, let's talk about the PLSA with prior knowledge.

14
00:01:04,520 --> 00:01:09,060
Now in practice, when we apply PLSA to analyze text data,

15
00:01:09,060 --> 00:01:14,665
we might have additional knowledge that we want to inject to guide the analysis.

16
00:01:14,665 --> 00:01:20,935
The standard PLSA is going to blindly listen to the data by using maximum [inaudible].

17
00:01:20,935 --> 00:01:25,690
We are going to just fit data as much as we can and get some insight about data.

18
00:01:25,690 --> 00:01:27,155
This is also very useful,

19
00:01:27,155 --> 00:01:32,500
but sometimes a user might have some expectations about which topics to analyze.

20
00:01:32,500 --> 00:01:35,723
For example, we might expect to see retrieval models as a topic in

21
00:01:35,723 --> 00:01:40,531
information retrieval or we also may be interesting in certain aspects,

22
00:01:40,531 --> 00:01:43,745
such as battery and memory,

23
00:01:43,745 --> 00:01:46,500
when looking at opinions about a laptop because

24
00:01:46,500 --> 00:01:50,075
the user is particularly interested in these aspects.

25
00:01:50,075 --> 00:01:55,160
A user may also have knowledge about topic coverage and we may

26
00:01:55,160 --> 00:02:00,390
know which topic is definitely not covering which document or is covering the document.

27
00:02:00,390 --> 00:02:03,530
For example, we might have seen those tags,

28
00:02:03,530 --> 00:02:06,520
topic tags assigned to documents.

29
00:02:06,520 --> 00:02:09,800
And those tags could be treated as topics.

30
00:02:09,800 --> 00:02:13,275
If we do that then a document account will be generated using

31
00:02:13,275 --> 00:02:17,420
topics corresponding to the tags already assigned to the document.

32
00:02:17,420 --> 00:02:19,087
If the document is not assigned a tag,

33
00:02:19,087 --> 00:02:25,925
we're going to say there is no way for using that topic to generate document.

34
00:02:25,925 --> 00:02:32,291
The document must be generated by using the topics corresponding to that assigned tags.

35
00:02:32,291 --> 00:02:35,835
So question is how can we incorporate such knowledge into PLSA.

36
00:02:35,835 --> 00:02:39,410
It turns out that there is a very elegant way of doing

37
00:02:39,410 --> 00:02:44,235
that and that would incorporate such knowledge as priors on the models.

38
00:02:44,235 --> 00:02:47,194
And you may recall in Bayesian inference,

39
00:02:47,194 --> 00:02:49,935
we use prior together with data

40
00:02:49,935 --> 00:02:54,185
to estimate parameters and this is precisely what would happen.

41
00:02:54,185 --> 00:02:55,207
So in this case,

42
00:02:55,207 --> 00:02:57,095
we can use maximum

43
00:02:57,095 --> 00:03:02,730
a posteriori estimate also called MAP estimate and the formula is given here.

44
00:03:02,730 --> 00:03:06,230
Basically, this is to maximize the posteriori distribution probability.

45
00:03:06,230 --> 00:03:09,650
And this is a combination of the likelihood of data and the prior.

46
00:03:09,650 --> 00:03:13,895
So what would happen is that we are going to have an estimate

47
00:03:13,895 --> 00:03:19,400
that listens to the data and also listens to our prior preferences.

48
00:03:19,400 --> 00:03:23,195
We can use this prior which is denoted as p of lambda

49
00:03:23,195 --> 00:03:28,070
to encode all kinds of preferences and the constraints.

50
00:03:28,070 --> 00:03:31,615
So for example, we can use this to encode

51
00:03:31,615 --> 00:03:35,870
the need of having precise background of the topic.

52
00:03:35,870 --> 00:03:43,995
Now this could be encoded as a prior because we can say the prior for the parameters is

53
00:03:43,995 --> 00:03:46,565
only a non-zero if

54
00:03:46,565 --> 00:03:52,740
the parameters contain one topic that is equivalent to the background language model.

55
00:03:52,740 --> 00:03:56,060
In other words, in other cases if it is not like that,

56
00:03:56,060 --> 00:03:59,200
we are going to say the prior says it is impossible.

57
00:03:59,200 --> 00:04:07,708
So the probability of that kind of models I think would be zero according to our prior.

58
00:04:07,708 --> 00:04:12,455
So now we can also for example use the prior to

59
00:04:12,455 --> 00:04:19,310
force particular choice of topic to have a probability of a certain number.

60
00:04:19,310 --> 00:04:26,175
For example, we can force document D to choose topic one with probability of

61
00:04:26,175 --> 00:04:33,395
one half or we can prevent topic from being used in generating document.

62
00:04:33,395 --> 00:04:36,440
So we can say the third topic should not be used in generating document D,

63
00:04:36,440 --> 00:04:41,510
we will set to the Pi zero for that topic.

64
00:04:41,510 --> 00:04:44,720
We can also use the prior to favor a set of parameters

65
00:04:44,720 --> 00:04:48,660
with topics that assign high probability to some particular words.

66
00:04:48,660 --> 00:04:52,340
In this case, we are not going to say it is impossible but we can just strongly

67
00:04:52,340 --> 00:04:57,590
favor certain kind of distributions and you will see example later.

68
00:04:57,590 --> 00:05:00,031
The MAP can be computed using a similar EM algorithm as we have used

69
00:05:00,031 --> 00:05:04,345
for the maximum likelihood estimate.

70
00:05:04,345 --> 00:05:06,177
With just some modifications,

71
00:05:06,177 --> 00:05:10,970
most of the parameters would reflect the prior preferences and in

72
00:05:10,970 --> 00:05:16,306
such an estimate if we use a special form of the prior code or conjugate the prior,

73
00:05:16,306 --> 00:05:20,145
then the functional form of the prior will be similar to the data.

74
00:05:20,145 --> 00:05:23,900
As a result, we can combine the two and the consequence is

75
00:05:23,900 --> 00:05:28,707
that you can basically convert the inference of the prior

76
00:05:28,707 --> 00:05:33,184
into the inference of having additional pseudo data

77
00:05:33,184 --> 00:05:37,772
because the two functional forms are the same and they can be combined.

78
00:05:37,772 --> 00:05:44,395
So the effect is as if we had more data and this is convenient for computation.

79
00:05:44,395 --> 00:05:49,680
It does not mean conjugate prior is the best way to define prior.

80
00:05:49,680 --> 00:05:52,780
So now let us look at the specific example.

81
00:05:52,780 --> 00:05:55,444
Suppose the user is particularly interested in

82
00:05:55,444 --> 00:05:59,270
battery life of a laptop and we are analyzing reviews.

83
00:05:59,270 --> 00:06:02,875
So the prior says that the distribution should contain

84
00:06:02,875 --> 00:06:09,551
one distribution that would assign high probability to battery and life.

85
00:06:09,551 --> 00:06:14,770
So we could say well there is distribution that is kind of concentrated on

86
00:06:14,770 --> 00:06:20,421
battery life and prior says that one of distributions should be very similar to this.

87
00:06:20,421 --> 00:06:24,794
Now if we use MAP estimate with conjugate prior,

88
00:06:24,794 --> 00:06:27,463
which is the original prior,

89
00:06:27,463 --> 00:06:32,565
the original distribution based on this preference,

90
00:06:32,565 --> 00:06:38,991
then the only difference in the EM is that when we re-estimate words distributions,

91
00:06:38,991 --> 00:06:44,700
we are going to add additional counts to reflect our prior.

92
00:06:44,700 --> 00:06:49,047
So here you can see the pseudo counts

93
00:06:49,047 --> 00:06:53,826
are defined based on the probability of words in a prior.

94
00:06:53,826 --> 00:06:55,780
So battery obviously would have

95
00:06:55,780 --> 00:07:00,365
high pseudo counts and similarly life would have also high pseudo counts.

96
00:07:00,365 --> 00:07:04,140
All the other words would have zero pseudo counts because their probability is

97
00:07:04,140 --> 00:07:07,900
zero in the prior and we see this is also controlled by

98
00:07:07,900 --> 00:07:15,130
a parameter mu and we are going to add a mu much by the probability of W given

99
00:07:15,130 --> 00:07:20,260
prior distribution to the connected accounts when we

100
00:07:20,260 --> 00:07:26,920
re-estimate this word distribution.

101
00:07:26,920 --> 00:07:30,910
So this is the only step that is changed and the change is happening here.

102
00:07:30,910 --> 00:07:34,570
And before we just connect the counts of words that

103
00:07:34,570 --> 00:07:38,230
we believe have been generated from this topic but now we

104
00:07:38,230 --> 00:07:42,455
force this distribution to give more probabilities

105
00:07:42,455 --> 00:07:47,714
to these words by adding them to the pseudo counts.

106
00:07:47,714 --> 00:07:53,180
So in fact we artificially inflated their probabilities.

107
00:07:53,180 --> 00:07:55,641
To make this distribution,

108
00:07:55,641 --> 00:08:00,190
we also need to add this many pseudo counts to the denominator.

109
00:08:00,190 --> 00:08:02,995
This is total sum of all the pseudo counts we have added

110
00:08:02,995 --> 00:08:07,300
for all the words This would make this a gamma distribution.

111
00:08:07,300 --> 00:08:14,142
Now this is intuitively very reasonable way of modifying EM and theoretically speaking,

112
00:08:14,142 --> 00:08:17,376
this works and it computes the MAP estimate.

113
00:08:17,376 --> 00:08:22,884
It is useful to think about the two specific extreme cases of mu.

114
00:08:22,884 --> 00:08:24,900
Now, [inaudible] the picture.

115
00:08:24,900 --> 00:08:27,555
Think about what would happen if we set mu to zero.

116
00:08:27,555 --> 00:08:29,260
Well that essentially to remove this prior.

117
00:08:29,260 --> 00:08:35,529
So mu in some sense indicates our strengths on prior.

118
00:08:35,529 --> 00:08:38,590
Now what would happen if we set mu to positive infinity.

119
00:08:38,590 --> 00:08:40,687
Well that is to say that prior is so

120
00:08:40,687 --> 00:08:44,140
strong that we are not going to listen to the data at all.

121
00:08:44,140 --> 00:08:46,765
So in the end, you see in this case

122
00:08:46,765 --> 00:08:50,010
we are going to make one of the distributions fixed to the prior. You see why?

123
00:08:50,010 --> 00:08:57,360
When mu is infinitive, we basically let this one dominate.

124
00:08:57,360 --> 00:09:02,961
In fact we are going to set this one to precise this distribution.

125
00:09:02,961 --> 00:09:06,680
So in this case, it is this distribution.

126
00:09:06,680 --> 00:09:08,605
And that is why we said

127
00:09:08,605 --> 00:09:11,384
the background language model is in fact a way to impose the prior

128
00:09:11,384 --> 00:09:17,890
because it would force one distribution to be exactly the same as what we give,

129
00:09:17,890 --> 00:09:19,898
that is background distribution.

130
00:09:19,898 --> 00:09:25,980
So in this case, we can even force the distribution to entirely focus on battery life.

131
00:09:25,980 --> 00:09:29,915
But of course this would not work well because it cannot attract other words.

132
00:09:29,915 --> 00:09:34,240
It would affect the accuracy of counting topics about battery life.

133
00:09:34,240 --> 00:09:38,770
So in practice, mu is set somewhere in between of course.

134
00:09:38,770 --> 00:09:41,625
So this is one way to impose a prior.

135
00:09:41,625 --> 00:09:44,500
We can also impose some other constraints.

136
00:09:44,500 --> 00:09:48,880
For example, we can set any parameters that will constantly include zero as needed.

137
00:09:48,880 --> 00:09:52,515
For example, we may want to set one of the Pi's to

138
00:09:52,515 --> 00:09:56,497
zero and this would mean

139
00:09:56,497 --> 00:10:01,080
we do not allow that topic to participate in generating that document.

140
00:10:01,080 --> 00:10:03,280
And this is only reasonable of course when we

141
00:10:03,280 --> 00:10:08,180
have prior analogy that strongly suggests this.
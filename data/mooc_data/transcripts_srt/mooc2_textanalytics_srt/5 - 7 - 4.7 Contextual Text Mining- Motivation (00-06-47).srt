1
00:00:00,160 --> 00:00:07,187
[SOUND]
This

2
00:00:07,187 --> 00:00:09,830
lecture is about
the contextual text mining.

3
00:00:11,010 --> 00:00:14,400
Contextual text mining
is related to multiple

4
00:00:14,400 --> 00:00:18,880
kinds of knowledge that we mine from
text data, as I'm showing here.

5
00:00:18,880 --> 00:00:23,580
It's related to topic mining because you
can make topics associated with context,

6
00:00:23,580 --> 00:00:25,260
like time or location.

7
00:00:25,260 --> 00:00:29,720
And similarly, we can make opinion
mining more contextualized,

8
00:00:29,720 --> 00:00:32,590
making opinions connected to context.

9
00:00:34,090 --> 00:00:38,320
It's related to text based prediction
because it allows us to combine non-text

10
00:00:38,320 --> 00:00:43,690
data with text data to derive
sophisticated predictors for

11
00:00:43,690 --> 00:00:45,110
the prediction problem.

12
00:00:45,110 --> 00:00:49,290
So more specifically, why are we
interested in contextual text mining?

13
00:00:49,290 --> 00:00:54,510
Well, that's first because text
often has rich context information.

14
00:00:54,510 --> 00:01:01,740
And this can include direct context such
as meta-data, and also indirect context.

15
00:01:01,740 --> 00:01:06,260
So, the direct context can grow
the meta-data such as time,

16
00:01:06,260 --> 00:01:10,030
location, authors, and
source of the text data.

17
00:01:10,030 --> 00:01:12,710
And they're almost always available to us.

18
00:01:14,280 --> 00:01:19,690
Indirect context refers to additional
data related to the meta-data.

19
00:01:19,690 --> 00:01:24,440
So for example, from office,
we can further obtain additional

20
00:01:24,440 --> 00:01:29,280
context such as social network of
the author, or the author's age.

21
00:01:30,300 --> 00:01:34,028
Such information is not in general
directly related to the text, yet

22
00:01:34,028 --> 00:01:37,350
through the process, we can connect them.

23
00:01:37,350 --> 00:01:41,080
There could be other text
data from the same source,

24
00:01:41,080 --> 00:01:46,760
as this one through the other text can
be connected with this text as well.

25
00:01:46,760 --> 00:01:51,400
So in general, any related data
can be regarded as context.

26
00:01:51,400 --> 00:01:53,830
So there could be removed or
rated for context.

27
00:01:55,460 --> 00:01:56,870
And so what's the use?

28
00:01:56,870 --> 00:01:59,100
What is text context used for?

29
00:01:59,100 --> 00:02:05,880
Well, context can be used to partition
text data in many interesting ways.

30
00:02:05,880 --> 00:02:11,490
It can almost allow us to partition
text data in other ways as we need.

31
00:02:11,490 --> 00:02:14,180
And this is very important
because this allows

32
00:02:14,180 --> 00:02:18,060
us to do interesting comparative analyses.

33
00:02:18,060 --> 00:02:21,622
It also in general,
provides meaning to the discovered topics,

34
00:02:21,622 --> 00:02:23,770
if we associate the text with context.

35
00:02:25,290 --> 00:02:30,540
So here's illustration of how context

36
00:02:30,540 --> 00:02:35,520
can be regarded as interesting
ways of partitioning of text data.

37
00:02:35,520 --> 00:02:39,449
So here I just showed some research
papers published in different years.

38
00:02:41,740 --> 00:02:43,122
On different venues,

39
00:02:43,122 --> 00:02:48,150
different conference names here listed on
the bottom like the SIGIR or ACL, etc.

40
00:02:49,640 --> 00:02:53,080
Now such text data can be partitioned

41
00:02:53,080 --> 00:02:55,480
in many interesting ways
because we have context.

42
00:02:56,860 --> 00:03:01,620
So the context here just includes time and
the conference venues.

43
00:03:01,620 --> 00:03:04,930
But perhaps we can include
some other variables as well.

44
00:03:06,480 --> 00:03:08,890
But let's see how we can partition
this interesting of ways.

45
00:03:08,890 --> 00:03:12,840
First, we can treat each
paper as a separate unit.

46
00:03:12,840 --> 00:03:17,987
So in this case, a paper ID and the,
each paper has its own context.

47
00:03:17,987 --> 00:03:22,575
It's independent.

48
00:03:22,575 --> 00:03:27,825
But we can also treat all the papers
within 1998 as one group and

49
00:03:27,825 --> 00:03:32,530
this is only possible because
of the availability of time.

50
00:03:32,530 --> 00:03:34,605
And we can partition data in this way.

51
00:03:34,605 --> 00:03:38,012
This would allow us to compare topics for
example, in different years.

52
00:03:39,840 --> 00:03:42,745
Similarly, we can partition
the data based on the menus.

53
00:03:42,745 --> 00:03:47,740
We can get all the SIGIR papers and
compare those papers with the rest.

54
00:03:47,740 --> 00:03:51,039
Or compare SIGIR papers with KDD papers,
with ACL papers.

55
00:03:52,700 --> 00:03:58,740
We can also partition the data to obtain
the papers written by authors in the U.S.,

56
00:03:58,740 --> 00:04:03,340
and that of course,
uses additional context of the authors.

57
00:04:03,340 --> 00:04:08,240
And this would allow us to then
compare such a subset with

58
00:04:08,240 --> 00:04:12,390
another set of papers written
by also seen in other countries.

59
00:04:13,910 --> 00:04:17,810
Or we can obtain a set of
papers about text mining, and

60
00:04:17,810 --> 00:04:21,890
this can be compared with
papers about another topic.

61
00:04:21,890 --> 00:04:25,310
And note that these
partitionings can be also

62
00:04:25,310 --> 00:04:28,860
intersected with each other to generate
even more complicated partitions.

63
00:04:29,890 --> 00:04:34,280
And so in general, this enables
discovery of knowledge associated with

64
00:04:34,280 --> 00:04:35,890
different context as needed.

65
00:04:37,150 --> 00:04:40,300
And in particular,
we can compare different contexts.

66
00:04:40,300 --> 00:04:43,620
And this often gives us
a lot of useful knowledge.

67
00:04:43,620 --> 00:04:49,350
For example, comparing topics over time,
we can see trends of topics.

68
00:04:49,350 --> 00:04:53,130
Comparing topics in different
contexts can also reveal differences

69
00:04:53,130 --> 00:04:55,230
about the two contexts.

70
00:04:55,230 --> 00:04:59,680
So there are many interesting questions
that require contextual text mining.

71
00:04:59,680 --> 00:05:01,780
Here I list some very specific ones.

72
00:05:01,780 --> 00:05:05,300
For example, what topics have
been getting increasing attention

73
00:05:05,300 --> 00:05:07,420
recently in data mining research?

74
00:05:07,420 --> 00:05:08,570
Now to answer this question,

75
00:05:08,570 --> 00:05:11,724
obviously we need to analyze
text in the context of time.

76
00:05:13,815 --> 00:05:17,455
So time is context in this case.

77
00:05:17,455 --> 00:05:20,675
Is there any difference in the responses
of people in different regions

78
00:05:20,675 --> 00:05:22,885
to the event, to any event?

79
00:05:22,885 --> 00:05:25,515
So this is a very broad
an answer to this question.

80
00:05:25,515 --> 00:05:28,635
In this case of course,
location is the context.

81
00:05:28,635 --> 00:05:31,260
What are the common research
interests of two researchers?

82
00:05:31,260 --> 00:05:34,110
In this case, authors can be the context.

83
00:05:34,110 --> 00:05:38,250
Is there any difference in the research
topics published by authors in the USA and

84
00:05:38,250 --> 00:05:39,920
those outside?

85
00:05:39,920 --> 00:05:43,990
Now in this case,
the context would include the authors and

86
00:05:43,990 --> 00:05:46,030
their affiliation and location.

87
00:05:47,810 --> 00:05:51,700
So this goes beyond just
the author himself or herself.

88
00:05:51,700 --> 00:05:55,520
We need to look at the additional
information connected to the author.

89
00:05:55,520 --> 00:05:58,580
Is there any difference in the opinions
of all the topics expressed on

90
00:05:58,580 --> 00:06:00,720
one social network and another?

91
00:06:00,720 --> 00:06:04,870
In this case, the social network of
authors and the topic can be a context.

92
00:06:06,128 --> 00:06:10,250
Other topics in news data that
are correlated with sudden changes in

93
00:06:10,250 --> 00:06:11,470
stock prices.

94
00:06:11,470 --> 00:06:16,060
In this case, we can use a time series
such as stock prices as context.

95
00:06:17,230 --> 00:06:20,780
What issues mattered in the 2012
presidential campaign, or

96
00:06:20,780 --> 00:06:21,410
presidential election?

97
00:06:21,410 --> 00:06:26,350
Now in this case,
time serves again as context.

98
00:06:26,350 --> 00:06:29,005
So, as you can see,
the list can go on and on.

99
00:06:29,005 --> 00:06:34,911
Basically, contextual text mining
can have many applications.

100
00:06:34,911 --> 00:06:44,911
[MUSIC]


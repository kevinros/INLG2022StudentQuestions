1
00:00:00,025 --> 00:00:05,631
[SOUND] So
now let's talk about the exchanging of

2
00:00:05,631 --> 00:00:10,816
PLSA to of LDA and to motivate that,

3
00:00:10,816 --> 00:00:17,145
we need to talk about some
deficiencies of PLSA.

4
00:00:17,145 --> 00:00:21,085
First, it's not really a generative model
because we can compute the probability of

5
00:00:21,085 --> 00:00:22,335
a new document.

6
00:00:22,335 --> 00:00:26,670
You can see why, and that's because the
pis are needed to generate the document,

7
00:00:26,670 --> 00:00:31,180
but the pis are tied to the document
that we have in the training data.

8
00:00:31,180 --> 00:00:33,790
So we can't compute the pis for
future document.

9
00:00:34,810 --> 00:00:39,030
And there's some heuristic workaround,
though.

10
00:00:39,030 --> 00:00:42,990
Secondly, it has many parameters, and I've
asked you to compute how many parameters

11
00:00:42,990 --> 00:00:47,170
exactly there are in PLSA, and
you will see there are many parameters.

12
00:00:47,170 --> 00:00:49,750
That means that model is very complex.

13
00:00:49,750 --> 00:00:53,010
And this also means that there
are many local maxima and

14
00:00:53,010 --> 00:00:55,090
it's prone to overfitting.

15
00:00:55,090 --> 00:01:01,569
And that means it's very hard to
also find a good local maximum.

16
00:01:02,630 --> 00:01:05,830
And that we are representing
global maximum.

17
00:01:05,830 --> 00:01:09,590
And in terms of explaining future data,
we might find that

18
00:01:09,590 --> 00:01:13,260
it will overfit the training data
because of the complexity of the model.

19
00:01:13,260 --> 00:01:18,010
The model is so flexible to fit precisely
what the training data looks like.

20
00:01:18,010 --> 00:01:22,980
And then it doesn't allow us to generalize
the model for using other data.

21
00:01:23,990 --> 00:01:28,530
This however is not a necessary problem
for text mining because here we're often

22
00:01:28,530 --> 00:01:32,150
only interested in hitting
the training documents that we have.

23
00:01:32,150 --> 00:01:36,980
We are not always interested in modern
future data, but in other cases,

24
00:01:36,980 --> 00:01:40,490
or if we would care about the generality,
we would worry about this overfitting.

25
00:01:42,330 --> 00:01:46,860
So LDA is proposing to improve that,
and basically to make

26
00:01:46,860 --> 00:01:51,470
PLSA a generative model by imposing
a Dirichlet prior on the model parameters.

27
00:01:51,470 --> 00:01:56,130
Dirichlet is just a special distribution
that we can use to specify product.

28
00:01:56,130 --> 00:02:00,120
So in this sense, LDA is just
a Bayesian version of PLSA, and

29
00:02:00,120 --> 00:02:02,290
the parameters are now
much more regularized.

30
00:02:02,290 --> 00:02:05,570
You will see there are many
few parameters and

31
00:02:05,570 --> 00:02:09,260
you can achieve the same goal as PLSA for
text mining.

32
00:02:09,260 --> 00:02:15,130
It means it can compute the top coverage
and topic word distributions as in PLSA.

33
00:02:15,130 --> 00:02:17,440
However, there's no.

34
00:02:17,440 --> 00:02:21,660
Why are the parameters for
PLSA here are much fewer,

35
00:02:21,660 --> 00:02:26,530
there are fewer parameters and
in order to compute a topic coverage and

36
00:02:26,530 --> 00:02:29,650
word distributions,
we again face a problem

37
00:02:29,650 --> 00:02:34,300
of influence of these variables because
they are not parameters of the model.

38
00:02:34,300 --> 00:02:38,190
So the influence part again
face the local maximum problem.

39
00:02:38,190 --> 00:02:41,770
So essentially they are doing something
very similar, but theoretically,

40
00:02:41,770 --> 00:02:48,110
LDA is a more elegant way of looking
at the top and bottom problem.

41
00:02:48,110 --> 00:02:52,810
So let's see how we can
generalize the PLSA to LDA or

42
00:02:52,810 --> 00:02:56,360
a standard PLSA to have LDA.

43
00:02:56,360 --> 00:02:59,753
Now a full treatment of LDA is
beyond the scope of this course and

44
00:02:59,753 --> 00:03:03,285
we just don't have time to go in
depth on that talking about that.

45
00:03:03,285 --> 00:03:07,040
But here, I just want to give you
a brief idea about what's extending and

46
00:03:07,040 --> 00:03:08,590
what it enables, all right.

47
00:03:08,590 --> 00:03:10,831
So this is the picture of LDA.

48
00:03:10,831 --> 00:03:14,940
Now, I remove the background
of model just for simplicity.

49
00:03:15,960 --> 00:03:19,960
Now, in this model, all these
parameters are free to change and

50
00:03:19,960 --> 00:03:22,220
we do not impose any prior.

51
00:03:22,220 --> 00:03:28,650
So these word distributions are now
represented as theta vectors.

52
00:03:28,650 --> 00:03:32,490
So these are word distributions, so here.

53
00:03:32,490 --> 00:03:35,520
And the other set of parameters are pis.

54
00:03:35,520 --> 00:03:37,470
And we would present it as a vector also.

55
00:03:37,470 --> 00:03:40,760
And this is more convenient
to introduce LDA.

56
00:03:40,760 --> 00:03:44,040
And we have one vector for each document.

57
00:03:44,040 --> 00:03:48,820
And in this case, in theta,
we have one vector for each topic.

58
00:03:50,140 --> 00:03:53,470
Now, the difference between LDA and

59
00:03:53,470 --> 00:03:58,390
PLSA is that in LDA, we're not going
to allow them to free the chain.

60
00:03:58,390 --> 00:04:02,170
Instead, we're going to force them to
be drawn from another distribution.

61
00:04:03,400 --> 00:04:04,900
So more specifically,

62
00:04:04,900 --> 00:04:09,760
they will be drawn from two Dirichlet
distributions respectively, but

63
00:04:09,760 --> 00:04:12,880
the Dirichlet distribution is
a distribution over vectors.

64
00:04:12,880 --> 00:04:16,600
So it gives us a probability of
four particular choice of a vector.

65
00:04:16,600 --> 00:04:19,190
Take, for example, pis, right.

66
00:04:19,190 --> 00:04:25,100
So this Dirichlet distribution tells
us which vectors of pi is more likely.

67
00:04:25,100 --> 00:04:29,390
And this distribution in itself is
controlled by another vector of parameters

68
00:04:29,390 --> 00:04:30,040
of alphas.

69
00:04:31,790 --> 00:04:35,130
Depending on the alphas, we can
characterize the distribution in different

70
00:04:35,130 --> 00:04:39,650
ways but with full certain choices of
pis to be more likely than others.

71
00:04:39,650 --> 00:04:40,230
For example,

72
00:04:40,230 --> 00:04:45,910
you might favor the choice of a relatively
uniform distribution of all the topics.

73
00:04:45,910 --> 00:04:51,090
Or you might favor generating
a skewed coverage of topics,

74
00:04:51,090 --> 00:04:53,000
and this is controlled by alpha.

75
00:04:53,000 --> 00:04:56,892
And similarly here, the topic or
word distributions are drawn

76
00:04:56,892 --> 00:05:01,470
from another Dirichlet
distribution with beta parameters.

77
00:05:01,470 --> 00:05:04,450
And note that here,
alpha has k parameters,

78
00:05:04,450 --> 00:05:10,260
corresponding to our inference on
the k values of pis for our document.

79
00:05:10,260 --> 00:05:10,940
Whereas here,

80
00:05:10,940 --> 00:05:16,670
beta has n values corresponding to
controlling the m words in our vocabulary.

81
00:05:17,700 --> 00:05:22,740
Now once we impose this price, then
the generation process will be different.

82
00:05:22,740 --> 00:05:27,667
And we start with joined pis from

83
00:05:27,667 --> 00:05:32,380
the Dirichlet distribution and
this pi will tell us these probabilities.

84
00:05:35,370 --> 00:05:40,990
And then, we're going to use the pi
to further choose which topic

85
00:05:40,990 --> 00:05:45,750
to use, and this is of course
very similar to the PLSA model.

86
00:05:47,250 --> 00:05:51,580
And similar here, we're not going
to have these distributions free.

87
00:05:51,580 --> 00:05:56,900
Instead, we're going to draw one
from the Dirichlet distribution.

88
00:05:56,900 --> 00:06:01,960
And then from this,
then we're going to further sample a word.

89
00:06:01,960 --> 00:06:04,739
And the rest is very similar to the.

90
00:06:04,739 --> 00:06:07,550
The likelihood function now
is more complicated for LDA.

91
00:06:07,550 --> 00:06:12,130
But there's a close connection between the
likelihood function of LDA and the PLSA.

92
00:06:12,130 --> 00:06:15,240
So I'm going to illustrate
the difference here.

93
00:06:15,240 --> 00:06:16,090
So in the top,

94
00:06:16,090 --> 00:06:20,730
you see PLSA likelihood function
that you have already seen before.

95
00:06:20,730 --> 00:06:22,760
It's copied from previous slide.

96
00:06:22,760 --> 00:06:25,820
Only that I dropped the background for
simplicity.

97
00:06:27,160 --> 00:06:32,100
So in the LDA formulas you
see very similar things.

98
00:06:32,100 --> 00:06:34,970
You see the first equation
is essentially the same.

99
00:06:34,970 --> 00:06:39,140
And this is the probability of generating
a word from multiple word distributions.

100
00:06:40,690 --> 00:06:45,440
And this formula is a sum of all
the possibilities of generating a word.

101
00:06:45,440 --> 00:06:50,230
Inside a sum is a product of
the probability of choosing a topic

102
00:06:50,230 --> 00:06:54,080
multiplied by the probability of
observing the word from that topic.

103
00:06:55,180 --> 00:06:59,100
So this is a very important formula,
as I've stressed multiple times.

104
00:06:59,100 --> 00:07:02,800
And this is actually the core
assumption in all the topic models.

105
00:07:02,800 --> 00:07:06,760
And you might see other topic models
that are extensions of LDA or PLSA.

106
00:07:06,760 --> 00:07:08,230
And they all rely on this.

107
00:07:08,230 --> 00:07:11,040
So it's very important to understand this.

108
00:07:11,040 --> 00:07:15,140
And this gives us a probability of
getting a word from a mixture model.

109
00:07:15,140 --> 00:07:20,930
Now, next in the probability of
a document, we see there is a PLSA

110
00:07:20,930 --> 00:07:26,710
component in the LDA formula, but the LDA
formula will add a sum integral here.

111
00:07:26,710 --> 00:07:32,930
And that's to account for
the fact that the pis are not fixed.

112
00:07:32,930 --> 00:07:39,180
So they are drawn from the original
distribution, and that's shown here.

113
00:07:39,180 --> 00:07:43,210
That's why we have to take an integral,
to consider all the possible pis that we

114
00:07:43,210 --> 00:07:48,374
could possibly draw from
this Dirichlet distribution.

115
00:07:48,374 --> 00:07:52,910
And similarly in the likelihood for
the whole collection,

116
00:07:52,910 --> 00:07:56,570
we also see further components added,
another integral here.

117
00:07:58,190 --> 00:07:58,760
Right?

118
00:07:58,760 --> 00:08:03,345
So basically in the area we're just
adding this integrals to account for

119
00:08:03,345 --> 00:08:08,306
the uncertainties and we added of course
the Dirichlet distributions to cover

120
00:08:08,306 --> 00:08:11,480
the choice of this parameters,
pis, and theta.

121
00:08:12,910 --> 00:08:15,276
So this is a likelihood function for LDA.

122
00:08:15,276 --> 00:08:19,760
Now, next to this, let's talk about the
parameter as estimation and inferences.

123
00:08:19,760 --> 00:08:23,730
Now the parameters can be now estimated
using exactly the same approach

124
00:08:23,730 --> 00:08:25,280
maximum likelihood estimate for LDA.

125
00:08:25,280 --> 00:08:31,270
Now you might think about how many
parameters are there in LDA versus PLSA.

126
00:08:31,270 --> 00:08:35,050
You'll see there're a fewer parameters
in LDA because in this case the only

127
00:08:35,050 --> 00:08:37,850
parameters are alphas and the betas.

128
00:08:37,850 --> 00:08:41,330
So we can use the maximum likelihood
estimator to compute that.

129
00:08:41,330 --> 00:08:45,510
Of course, it's more complicated because
the form of likelihood function is

130
00:08:45,510 --> 00:08:46,890
more complicated.

131
00:08:46,890 --> 00:08:51,740
But what's also important
is notice that now these

132
00:08:51,740 --> 00:08:56,350
parameters that we are interested
in name and topics, and

133
00:08:56,350 --> 00:09:00,240
the coverage are no
longer parameters in LDA.

134
00:09:00,240 --> 00:09:04,110
In this case we have to
use basic inference or

135
00:09:04,110 --> 00:09:09,700
posterior inference to compute them based
on the parameters of alpha and the beta.

136
00:09:09,700 --> 00:09:13,900
Unfortunately, this
computation is intractable.

137
00:09:13,900 --> 00:09:17,570
So we generally have to resort
to approximate inference.

138
00:09:18,720 --> 00:09:24,220
And there are many methods available for
that and I'm sure you will

139
00:09:24,220 --> 00:09:29,100
see them when you use different tool kits
for LDA, or when you read papers about

140
00:09:30,800 --> 00:09:35,120
these different extensions of LDA.

141
00:09:35,120 --> 00:09:39,210
Now here we, of course, can't give
in-depth instruction to that, but

142
00:09:39,210 --> 00:09:43,189
just know that they are computed based in

143
00:09:43,189 --> 00:09:50,386
inference by using
the parameters alphas and betas.

144
00:09:50,386 --> 00:09:53,820
But our math [INAUDIBLE],
actually, in the end,

145
00:09:53,820 --> 00:09:57,900
in some of our math list,
it's very similar to PLSA.

146
00:09:57,900 --> 00:10:02,720
And, especially when we use
algorithm called class assembly,

147
00:10:02,720 --> 00:10:06,260
then the algorithm looks very
similar to the Algorithm.

148
00:10:06,260 --> 00:10:08,800
So in the end,
they are doing something very similar.

149
00:10:10,660 --> 00:10:14,950
So to summarize our discussion
of properties of topic models,

150
00:10:14,950 --> 00:10:17,340
these models provide
a general principle or

151
00:10:17,340 --> 00:10:22,300
way of mining and analyzing topics
in text with many applications.

152
00:10:22,300 --> 00:10:27,010
The best basic task setup is
to take test data as input and

153
00:10:27,010 --> 00:10:29,540
we're going to output the k topics.

154
00:10:29,540 --> 00:10:32,610
Each topic is characterized
by word distribution.

155
00:10:32,610 --> 00:10:36,999
And we're going to also output proportions
of these topics covered in each document.

156
00:10:38,990 --> 00:10:45,320
And PLSA is the basic topic model, and
in fact the most basic of the topic model.

157
00:10:45,320 --> 00:10:48,310
And this is often adequate for
most applications.

158
00:10:48,310 --> 00:10:51,800
That's why we spend a lot of
time to explain PLSA in detail.

159
00:10:53,190 --> 00:10:57,050
Now LDA improves over
PLSA by imposing priors.

160
00:10:57,050 --> 00:11:00,650
This has led to theoretically
more appealing models.

161
00:11:00,650 --> 00:11:05,740
However, in practice, LDA and
PLSA tend to give similar performance, so

162
00:11:05,740 --> 00:11:10,890
in practice PLSA and LDA would work
equally well for most of the tasks.

163
00:11:12,290 --> 00:11:16,140
Now here are some suggested readings if
you want to know more about the topic.

164
00:11:16,140 --> 00:11:19,340
First is a nice review of
probabilistic topic models.

165
00:11:20,490 --> 00:11:25,610
The second has a discussion about how
to automatically label a topic model.

166
00:11:25,610 --> 00:11:29,840
Now I've shown you some distributions and
they intuitively suggest a topic.

167
00:11:29,840 --> 00:11:31,690
But what exactly is a topic?

168
00:11:31,690 --> 00:11:35,600
Can we use phrases to label the topic?

169
00:11:35,600 --> 00:11:37,720
To make it the more easy to understand and

170
00:11:37,720 --> 00:11:40,480
this paper is about the techniques for
doing that.

171
00:11:40,480 --> 00:11:45,820
The third one is empirical comparison
of LDA and the PLSA for various tasks.

172
00:11:45,820 --> 00:11:48,985
The conclusion is that they
tend to perform similarly.

173
00:11:48,985 --> 00:11:58,985
[MUSIC]
1
00:00:00,000 --> 00:00:04,714
[SOUND]

2
00:00:06,455 --> 00:00:09,677
In general, we can use the empirical count

3
00:00:09,677 --> 00:00:15,340
of events in the observed data
to estimate the probabilities.

4
00:00:15,340 --> 00:00:19,080
And a commonly used technique is
called a maximum likelihood estimate,

5
00:00:19,080 --> 00:00:22,600
where we simply normalize
the observe accounts.

6
00:00:22,600 --> 00:00:30,330
So if we do that, we can see, we can
compute these probabilities as follows.

7
00:00:30,330 --> 00:00:36,811
For estimating the probability that
we see a water current in a segment,

8
00:00:36,811 --> 00:00:42,773
we simply normalize the count of
segments that contain this word.

9
00:00:42,773 --> 00:00:47,278
So let's first take
a look at the data here.

10
00:00:47,278 --> 00:00:52,970
On the right side, you see a list of some,
hypothesizes the data.

11
00:00:52,970 --> 00:00:55,010
These are segments.

12
00:00:55,010 --> 00:00:59,860
And in some segments you see both words
occur, they are indicated as ones for

13
00:00:59,860 --> 00:01:01,630
both columns.

14
00:01:01,630 --> 00:01:05,830
In some other cases only one will occur,
so only that column has one and

15
00:01:05,830 --> 00:01:07,590
the other column has zero.

16
00:01:07,590 --> 00:01:11,130
And in all, of course, in some other
cases none of the words occur,

17
00:01:11,130 --> 00:01:13,930
so they are both zeros.

18
00:01:13,930 --> 00:01:19,310
And for estimating these probabilities, we
simply need to collect the three counts.

19
00:01:20,340 --> 00:01:23,560
So the three counts are first,
the count of W1.

20
00:01:23,560 --> 00:01:27,337
And that's the total number of
segments that contain word W1.

21
00:01:27,337 --> 00:01:30,950
It's just as the ones in the column of W1.

22
00:01:30,950 --> 00:01:34,470
We can count how many
ones we have seen there.

23
00:01:34,470 --> 00:01:40,460
The segment count is for word 2, and we
just count the ones in the second column.

24
00:01:40,460 --> 00:01:45,425
And these will give us the total
number of segments that contain W2.

25
00:01:45,425 --> 00:01:49,650
The third count is when both words occur.

26
00:01:49,650 --> 00:01:55,370
So this time, we're going to count
the sentence where both columns have ones.

27
00:01:56,580 --> 00:02:00,060
And then, so this would give us
the total number of segments

28
00:02:00,060 --> 00:02:03,510
where we have seen both W1 and W2.

29
00:02:03,510 --> 00:02:08,112
Once we have these counts,
we can just normalize these counts by N,

30
00:02:08,112 --> 00:02:11,019
which is the total number of segments, and

31
00:02:11,019 --> 00:02:16,706
this will give us the probabilities that
we need to compute original information.

32
00:02:16,706 --> 00:02:22,301
Now, there is a small problem,
when we have zero counts sometimes.

33
00:02:22,301 --> 00:02:27,458
And in this case, we don't want a zero
probability because our data may be

34
00:02:27,458 --> 00:02:33,365
a small sample and in general, we would
believe that it's potentially possible for

35
00:02:33,365 --> 00:02:35,806
a [INAUDIBLE] to avoid any context.

36
00:02:35,806 --> 00:02:39,630
So, to address this problem,
we can use a technique called smoothing.

37
00:02:39,630 --> 00:02:43,780
And that's basically to add some
small constant to these counts,

38
00:02:43,780 --> 00:02:48,410
and so that we don't get
the zero probability in any case.

39
00:02:48,410 --> 00:02:54,250
Now, the best way to understand smoothing
is imagine that we actually observed more

40
00:02:54,250 --> 00:03:00,310
data than we actually have, because we'll
pretend we observed some pseudo-segments.

41
00:03:00,310 --> 00:03:04,650
I illustrated on the top,
on the right side on the slide.

42
00:03:04,650 --> 00:03:10,095
And these pseudo-segments would
contribute additional counts

43
00:03:10,095 --> 00:03:15,047
of these words so
that no event will have zero probability.

44
00:03:15,047 --> 00:03:18,169
Now, in particular we introduce
the four pseudo-segments.

45
00:03:18,169 --> 00:03:20,990
Each is weighted at one quarter.

46
00:03:20,990 --> 00:03:25,930
And these represent the four different
combinations of occurrences of this word.

47
00:03:25,930 --> 00:03:30,490
So now each event,
each combination will have

48
00:03:30,490 --> 00:03:35,390
at least one count or at least a non-zero
count from this pseudo-segment.

49
00:03:35,390 --> 00:03:39,380
So, in the actual segments
that we'll observe,

50
00:03:39,380 --> 00:03:44,231
it's okay if we haven't observed
all of the combinations.

51
00:03:44,231 --> 00:03:49,671
So more specifically, you can see
the 0.5 here after it comes from the two

52
00:03:49,671 --> 00:03:55,560
ones in the two pseudo-segments,
because each is weighted at one quarter.

53
00:03:55,560 --> 00:03:59,315
We add them up, we get 0.5.

54
00:03:59,315 --> 00:04:03,319
And similar to this,
0.05 comes from one single

55
00:04:03,319 --> 00:04:08,240
pseudo-segment that indicates
the two words occur together.

56
00:04:09,450 --> 00:04:14,000
And of course in the denominator we add
the total number of pseudo-segments that

57
00:04:14,000 --> 00:04:17,520
we add, in this case,
we added a four pseudo-segments.

58
00:04:17,520 --> 00:04:21,780
Each is weighed at one quarter so
the total of the sum is, after the one.

59
00:04:21,780 --> 00:04:24,110
So, that's why in the denominator
you'll see a one there.

60
00:04:25,990 --> 00:04:31,460
So, this basically concludes
the discussion of how to compute a these

61
00:04:31,460 --> 00:04:33,920
four syntagmatic relation discoveries.

62
00:04:36,090 --> 00:04:42,050
Now, so to summarize,
syntagmatic relation can generally

63
00:04:42,050 --> 00:04:46,240
be discovered by measuring correlations
between occurrences of two words.

64
00:04:46,240 --> 00:04:49,580
We've introduced the three
concepts from information theory.

65
00:04:49,580 --> 00:04:53,230
Entropy, which measures the uncertainty
of a random variable X.

66
00:04:53,230 --> 00:04:59,060
Conditional entropy, which measures
the entropy of X given we know Y.

67
00:04:59,060 --> 00:05:04,530
And mutual information of X and Y,
which matches the entropy reduction of X

68
00:05:04,530 --> 00:05:11,240
due to knowing Y, or
entropy reduction of Y due to knowing X.

69
00:05:11,240 --> 00:05:12,660
They are the same.

70
00:05:12,660 --> 00:05:17,111
So these three concepts are actually very
useful for other applications as well.

71
00:05:17,111 --> 00:05:20,340
That's why we spent some time
to explain this in detail.

72
00:05:20,340 --> 00:05:23,150
But in particular,
they are also very useful for

73
00:05:23,150 --> 00:05:25,960
discovering syntagmatic relations.

74
00:05:25,960 --> 00:05:30,142
In particular,
mutual information is a principal way for

75
00:05:30,142 --> 00:05:32,370
discovering such a relation.

76
00:05:32,370 --> 00:05:37,241
It allows us to have values
computed on different pairs of

77
00:05:37,241 --> 00:05:42,211
words that are comparable and
so we can rank these pairs and

78
00:05:42,211 --> 00:05:48,208
discover the strongest syntagmatic
from a collection of documents.

79
00:05:48,208 --> 00:05:53,700
Now, note that there is some relation
between syntagmatic relation discovery and

80
00:05:53,700 --> 00:05:55,910
[INAUDIBLE] relation discovery.

81
00:05:55,910 --> 00:06:01,835
So we already discussed the possibility
of using BM25 to achieve waiting for

82
00:06:01,835 --> 00:06:06,683
terms in the context to potentially
also suggest the candidates

83
00:06:06,683 --> 00:06:11,187
that have syntagmatic relations
with the candidate word.

84
00:06:11,187 --> 00:06:17,958
But here, once we use mutual information
to discover syntagmatic relations,

85
00:06:17,958 --> 00:06:24,436
we can also represent the context with
this mutual information as weights.

86
00:06:24,436 --> 00:06:29,567
So this would give us
another way to represent

87
00:06:29,567 --> 00:06:33,490
the context of a word, like a cat.

88
00:06:33,490 --> 00:06:37,394
And if we do the same for all the words,
then we can cluster these words or

89
00:06:37,394 --> 00:06:42,320
compare the similarity between these
words based on their context similarity.

90
00:06:42,320 --> 00:06:45,850
So this provides yet
another way to do term weighting for

91
00:06:45,850 --> 00:06:48,800
paradigmatic relation discovery.

92
00:06:48,800 --> 00:06:55,770
And so to summarize this whole part
about word association mining.

93
00:06:55,770 --> 00:06:59,190
We introduce two basic associations,
called a paradigmatic and

94
00:06:59,190 --> 00:07:01,000
a syntagmatic relations.

95
00:07:01,000 --> 00:07:05,710
These are fairly general, they apply
to any items in any language, so

96
00:07:05,710 --> 00:07:10,009
the units don't have to be words,
they can be phrases or entities.

97
00:07:11,120 --> 00:07:16,235
We introduced multiple statistical
approaches for discovering them,

98
00:07:16,235 --> 00:07:20,762
mainly showing that pure
statistical approaches are visible,

99
00:07:20,762 --> 00:07:24,840
are variable for
discovering both kind of relations.

100
00:07:24,840 --> 00:07:28,800
And they can be combined to
perform joint analysis, as well.

101
00:07:28,800 --> 00:07:35,040
These approaches can be applied
to any text with no human effort,

102
00:07:35,040 --> 00:07:39,940
mostly because they are based
on counting of words, yet

103
00:07:39,940 --> 00:07:42,690
they can actually discover
interesting relations of words.

104
00:07:44,360 --> 00:07:47,880
We can also use different ways with
defining context and segment, and

105
00:07:47,880 --> 00:07:51,360
this would lead us to some interesting
variations of applications.

106
00:07:51,360 --> 00:07:56,190
For example, the context can be very
narrow like a few words, around a word, or

107
00:07:56,190 --> 00:08:00,760
a sentence, or maybe paragraphs,
as using differing contexts would

108
00:08:00,760 --> 00:08:05,330
allows to discover different flavors
of paradigmatical relations.

109
00:08:05,330 --> 00:08:09,362
And similarly,
counting co-occurrences using let's say,

110
00:08:09,362 --> 00:08:13,380
visual information to discover
syntagmatical relations.

111
00:08:13,380 --> 00:08:19,110
We also have to define the segment, and
the segment can be defined as a narrow

112
00:08:19,110 --> 00:08:22,560
text window or a longer text article.

113
00:08:22,560 --> 00:08:26,508
And this would give us different
kinds of associations.

114
00:08:26,508 --> 00:08:32,677
These discovery associations can
support many other applications,

115
00:08:32,677 --> 00:08:37,701
in both information retrieval and
text and data mining.

116
00:08:37,701 --> 00:08:44,100
So here are some recommended readings,
if you want to know more about the topic.

117
00:08:44,100 --> 00:08:46,880
The first is a book with
a chapter on collocations,

118
00:08:46,880 --> 00:08:50,810
which is quite relevant to
the topic of these lectures.

119
00:08:50,810 --> 00:08:55,120
The second is an article
about using various

120
00:08:55,120 --> 00:08:58,160
statistical measures to
discover lexical atoms.

121
00:08:58,160 --> 00:09:03,764
Those are phrases that
are non-compositional.

122
00:09:03,764 --> 00:09:07,560
For example,
hot dog is not really a dog that's hot,

123
00:09:08,610 --> 00:09:11,550
blue chip is not a chip that's blue.

124
00:09:11,550 --> 00:09:16,180
And the paper has a discussion about some
techniques for discovering such phrases.

125
00:09:17,400 --> 00:09:23,227
The third one is a new paper on a unified
way to discover both paradigmatical

126
00:09:23,227 --> 00:09:29,441
relations and a syntagmatical relations,
using random works on word graphs.

127
00:09:29,441 --> 00:09:39,441
[SOUND]


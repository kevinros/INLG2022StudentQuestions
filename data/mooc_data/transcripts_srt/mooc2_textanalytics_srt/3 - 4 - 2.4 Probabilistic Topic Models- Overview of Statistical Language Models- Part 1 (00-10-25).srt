1
00:00:00,025 --> 00:00:07,147
[SOUND]
>> This

2
00:00:07,147 --> 00:00:09,992
lecture is about the Overview
of Statistical Language Models,

3
00:00:09,992 --> 00:00:12,001
which cover proper
models as special cases.

4
00:00:12,001 --> 00:00:15,906
In this lecture we're going to give

5
00:00:15,906 --> 00:00:21,320
a overview of Statical Language Models.

6
00:00:21,320 --> 00:00:24,320
These models are general models that cover

7
00:00:24,320 --> 00:00:28,120
probabilistic topic models
as a special cases.

8
00:00:28,120 --> 00:00:30,530
So first off,
what is a Statistical Language Model?

9
00:00:31,780 --> 00:00:36,070
A Statistical Language Model is
basically a probability distribution

10
00:00:36,070 --> 00:00:37,870
over word sequences.

11
00:00:37,870 --> 00:00:41,520
So, for example,
we might have a distribution that gives,

12
00:00:41,520 --> 00:00:44,480
today is Wednesday a probability of .001.

13
00:00:44,480 --> 00:00:49,040
It might give today Wednesday is, which

14
00:00:49,040 --> 00:00:53,560
is a non-grammatical sentence, a very,
very small probability as shown here.

15
00:00:54,580 --> 00:00:56,170
And similarly another sentence,

16
00:00:56,170 --> 00:01:01,430
the eigenvalue is positive might
get the probability of .00001.

17
00:01:01,430 --> 00:01:06,200
So as you can see such a distribution
clearly is Context Dependent.

18
00:01:06,200 --> 00:01:09,830
It depends on the Context of Discussion.

19
00:01:09,830 --> 00:01:15,370
Some Word Sequences might have higher
probabilities than others but the same

20
00:01:15,370 --> 00:01:19,060
Sequence of Words might have different
probability in different context.

21
00:01:20,490 --> 00:01:24,870
And so this suggests that such a
distribution can actually categorize topic

22
00:01:26,960 --> 00:01:31,440
such a model can also be regarded
as Probabilistic Mechanism for

23
00:01:31,440 --> 00:01:32,520
generating text.

24
00:01:33,880 --> 00:01:42,370
And that just means we can view text
data as data observed from such a model.

25
00:01:42,370 --> 00:01:49,225
For this reason,
we call such a model as Generating Model.

26
00:01:49,225 --> 00:01:54,310
So, now given a model we can then
assemble sequences of words.

27
00:01:54,310 --> 00:01:59,790
So, for example, based on the distribution
that I have shown here on this slide,

28
00:01:59,790 --> 00:02:04,240
when matter it say assemble
a sequence like today is Wednesday

29
00:02:04,240 --> 00:02:07,130
because it has a relative
high probability.

30
00:02:07,130 --> 00:02:10,100
We might often get such a sequence.

31
00:02:10,100 --> 00:02:14,470
We might also get the item
value as positive sometimes

32
00:02:14,470 --> 00:02:19,120
with a smaller probability and
very, very occasionally we might

33
00:02:19,120 --> 00:02:22,940
get today is Wednesday because
it's probability is so small.

34
00:02:24,650 --> 00:02:28,960
So in general, in order to categorize such
a distribution we must specify probability

35
00:02:28,960 --> 00:02:33,940
values for
all these different sequences of words.

36
00:02:33,940 --> 00:02:37,827
Obviously, it's impossible
to specify that because it's

37
00:02:37,827 --> 00:02:42,540
impossible to enumerate all of
the possible sequences of words.

38
00:02:42,540 --> 00:02:49,300
So in practice, we will have to
simplify the model in some way.

39
00:02:49,300 --> 00:02:52,710
So, the simplest language model is
called the Unigram Language Model.

40
00:02:52,710 --> 00:02:57,270
In such a case, it was simply a the text

41
00:02:57,270 --> 00:03:01,830
is generated by generating
each word independently.

42
00:03:02,980 --> 00:03:06,660
But in general, the words may
not be generated independently.

43
00:03:06,660 --> 00:03:11,020
But after we make this assumption, we can
significantly simplify the language more.

44
00:03:12,230 --> 00:03:16,700
Basically, now the probability of
a sequence of words, w1 through wn,

45
00:03:16,700 --> 00:03:21,500
will be just the product of
the probability of each word.

46
00:03:24,850 --> 00:03:26,210
So for such a model,

47
00:03:26,210 --> 00:03:30,470
we have as many parameters as
the number of words in our vocabulary.

48
00:03:30,470 --> 00:03:35,260
So here we assume we have n words,
so we have n probabilities.

49
00:03:35,260 --> 00:03:36,590
One for each word.

50
00:03:36,590 --> 00:03:38,700
And then some to 1.

51
00:03:38,700 --> 00:03:43,010
So, now we assume that
our text is a sample

52
00:03:43,010 --> 00:03:46,220
drawn according to this word distribution.

53
00:03:46,220 --> 00:03:50,870
That just means,
we're going to draw a word each time and

54
00:03:50,870 --> 00:03:52,360
then eventually we'll get a text.

55
00:03:53,690 --> 00:03:56,163
So for example, now again,

56
00:03:56,163 --> 00:04:02,050
we can try to assemble words
according to a distribution.

57
00:04:02,050 --> 00:04:05,110
We might get Wednesday often or
today often.

58
00:04:06,610 --> 00:04:11,910
And some other words like eigenvalue
might have a small probability, etcetera.

59
00:04:11,910 --> 00:04:19,370
But with this, we actually can
also compute the probability of

60
00:04:19,370 --> 00:04:25,980
every sequence, even though our model
only specify the probabilities of words.

61
00:04:25,980 --> 00:04:27,780
And this is because of the independence.

62
00:04:27,780 --> 00:04:32,970
So specifically, we can compute
the probability of today is Wednesday.

63
00:04:34,010 --> 00:04:37,740
Because it's just a product
of the probability of today,

64
00:04:37,740 --> 00:04:42,000
the probability of is, and
probability of Wednesday.

65
00:04:42,000 --> 00:04:45,380
For example,
I show some fake numbers here and when you

66
00:04:45,380 --> 00:04:49,650
multiply these numbers together you get
the probability that today's Wednesday.

67
00:04:49,650 --> 00:04:55,900
So as you can see, with N probabilities,
one for each word, we actually

68
00:04:55,900 --> 00:05:02,670
can characterize the probability situation
over all kinds of sequences of words.

69
00:05:02,670 --> 00:05:06,100
And so, this is a very simple model.

70
00:05:06,100 --> 00:05:07,890
Ignore the word order.

71
00:05:07,890 --> 00:05:12,290
So it may not be, in fact, in some
problems, such as for speech recognition,

72
00:05:12,290 --> 00:05:15,410
where you may care about
the order of words.

73
00:05:15,410 --> 00:05:18,310
But it turns out to be
quite sufficient for

74
00:05:18,310 --> 00:05:20,950
many tasks that involve topic analysis.

75
00:05:20,950 --> 00:05:24,590
And that's also what
we're interested in here.

76
00:05:24,590 --> 00:05:31,000
So when we have a model, we generally have
two problems that we can think about.

77
00:05:31,000 --> 00:05:38,520
One is, given a model, how likely are we
to observe a certain kind of data points?

78
00:05:38,520 --> 00:05:41,890
That is,
we are interested in the Sampling Process.

79
00:05:41,890 --> 00:05:44,400
The other is the Estimation Process.

80
00:05:44,400 --> 00:05:49,940
And that, is to think of
the parameters of a model given,

81
00:05:49,940 --> 00:05:53,510
some observe the data and we're
going to talk about that in a moment.

82
00:05:53,510 --> 00:05:56,110
Let's first talk about the sampling.

83
00:05:56,110 --> 00:06:02,480
So, here I show two examples of Water
Distributions or Unigram Language Models.

84
00:06:02,480 --> 00:06:04,760
The first one has higher probabilities for

85
00:06:04,760 --> 00:06:08,530
words like a text mining association,
it's separate.

86
00:06:10,120 --> 00:06:16,030
Now this signals a topic about text mining
because when we assemble words from

87
00:06:16,030 --> 00:06:21,970
such a distribution, we tend to see words
that often occur in text mining contest.

88
00:06:23,710 --> 00:06:27,460
So in this case,
if we ask the question about

89
00:06:27,460 --> 00:06:30,560
what is the probability of
generating a particular document.

90
00:06:30,560 --> 00:06:36,610
Then, we likely will see text that
looks like a text mining paper.

91
00:06:36,610 --> 00:06:42,110
Of course, the text that we
generate by drawing words.

92
00:06:42,110 --> 00:06:45,150
This distribution is unlikely coherent.

93
00:06:45,150 --> 00:06:49,079
Although, the probability
of generating attacks mine

94
00:06:49,079 --> 00:06:53,535
[INAUDIBLE] publishing
in the top conference is

95
00:06:53,535 --> 00:06:59,090
non-zero assuming that no word has
a zero probability in the distribution.

96
00:06:59,090 --> 00:07:02,590
And that just means,
we can essentially generate all kinds of

97
00:07:02,590 --> 00:07:06,560
text documents including very
meaningful text documents.

98
00:07:07,830 --> 00:07:09,660
Now, the second distribution show,

99
00:07:09,660 --> 00:07:14,310
on the bottom, has different than
what was high probabilities.

100
00:07:14,310 --> 00:07:17,940
So food [INAUDIBLE] healthy [INAUDIBLE],
etcetera.

101
00:07:17,940 --> 00:07:20,380
So this clearly indicates
a different topic.

102
00:07:20,380 --> 00:07:23,190
In this case it's probably about health.

103
00:07:23,190 --> 00:07:26,460
So if we sample a word
from such a distribution,

104
00:07:26,460 --> 00:07:31,390
then the probability of observing a text
mining paper would be very, very small.

105
00:07:32,830 --> 00:07:37,020
On the other hand, the probability of
observing a text that looks like a food

106
00:07:37,020 --> 00:07:40,400
nutrition paper would be high,
relatively higher.

107
00:07:41,510 --> 00:07:48,113
So that just means, given a particular
distribution, different than the text.

108
00:07:48,113 --> 00:07:51,830
Now let's look at
the estimation problem now.

109
00:07:51,830 --> 00:07:54,910
In this case, we're going to assume
that we have observed the data.

110
00:07:54,910 --> 00:07:57,410
I will know exactly what
the text data looks like.

111
00:07:57,410 --> 00:07:59,715
In this case,
let's assume we have a text mining paper.

112
00:07:59,715 --> 00:08:06,980
In fact, it's abstract of the paper,
so the total number of words is 100.

113
00:08:06,980 --> 00:08:10,960
And I've shown some counts
of individual words here.

114
00:08:12,550 --> 00:08:16,880
Now, if we ask the question,
what is the most likely

115
00:08:17,950 --> 00:08:22,440
Language Model that has been
used to generate this text data?

116
00:08:22,440 --> 00:08:26,400
Assuming that the text is observed
from some Language Model,

117
00:08:26,400 --> 00:08:28,920
what's our best guess
of this Language Model?

118
00:08:30,740 --> 00:08:35,510
Okay, so the problem now is just to
estimate the probabilities of these words.

119
00:08:35,510 --> 00:08:36,490
As I've shown here.

120
00:08:37,560 --> 00:08:38,370
So what do you think?

121
00:08:38,370 --> 00:08:39,610
What would be your guess?

122
00:08:40,680 --> 00:08:45,590
Would you guess text has
a very small probability, or

123
00:08:45,590 --> 00:08:47,180
a relatively large probability?

124
00:08:48,360 --> 00:08:50,310
What about query?

125
00:08:50,310 --> 00:08:53,200
Well, your guess probably
would be dependent on

126
00:08:53,200 --> 00:08:56,516
how many times we have observed
this word in the text data, right?

127
00:08:56,516 --> 00:09:00,550
And if you think about it for a moment.

128
00:09:00,550 --> 00:09:04,960
And if you are like many others,
you would have guessed that,

129
00:09:04,960 --> 00:09:10,140
well, text has a probability of 10
out of 100 because I've observed

130
00:09:10,140 --> 00:09:15,040
the text 10 times in the text
that has a total of 100 words.

131
00:09:15,040 --> 00:09:19,640
And similarly, mining has 5 out of 100.

132
00:09:19,640 --> 00:09:25,180
And query has a relatively small
probability, just observed for once.

133
00:09:25,180 --> 00:09:27,130
So it's 1 out of 100.

134
00:09:27,130 --> 00:09:32,220
Right, so that, intuitively,
is a reasonable guess.

135
00:09:32,220 --> 00:09:36,440
But the question is, is this our best
guess or best estimate of the parameters?

136
00:09:37,840 --> 00:09:40,000
Of course,
in order to answer this question,

137
00:09:40,000 --> 00:09:45,070
we have to define what do we mean by best,
in this case,

138
00:09:45,070 --> 00:09:50,540
it turns out that our
guesses are indeed the best.

139
00:09:50,540 --> 00:09:54,680
In some sense and this is called
Maximum Likelihood Estimate.

140
00:09:54,680 --> 00:10:00,789
And it's the best thing that, it will give
the observer data our maximum probability.

141
00:10:01,960 --> 00:10:05,740
Meaning that, if you change
the estimate somehow, even slightly,

142
00:10:05,740 --> 00:10:10,760
then the probability of the observed
text data will be somewhat smaller.

143
00:10:10,760 --> 00:10:13,952
And this is called
a Maximum Likelihood Estimate.

144
00:10:13,952 --> 00:10:23,952
[MUSIC]


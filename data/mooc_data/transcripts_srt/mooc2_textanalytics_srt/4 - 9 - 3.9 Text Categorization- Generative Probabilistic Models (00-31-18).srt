1
00:00:00,012 --> 00:00:08,521
[SOUND]
This lecture is

2
00:00:08,521 --> 00:00:13,230
about how to use generative probabilistic
models for text categorization.

3
00:00:14,450 --> 00:00:19,470
There are in general about two kinds
of approaches to text categorization

4
00:00:19,470 --> 00:00:20,580
by using machine learning.

5
00:00:20,580 --> 00:00:22,970
One is by generating probabilistic models.

6
00:00:22,970 --> 00:00:25,750
The other is discriminative approaches.

7
00:00:25,750 --> 00:00:29,740
In this lecture, we're going to
talk about the generative models.

8
00:00:29,740 --> 00:00:34,460
In the next lecture, we're going to
talk about discriminative approaches.

9
00:00:34,460 --> 00:00:36,990
So the problem of text categorization

10
00:00:36,990 --> 00:00:39,400
is actually a very similar
to document clustering.

11
00:00:39,400 --> 00:00:44,820
In that, we'll assume that each document
it belongs to one category or one cluster.

12
00:00:44,820 --> 00:00:48,500
The main difference is that in
clustering we don't really know

13
00:00:48,500 --> 00:00:51,810
what are the predefined categories are,
what are the clusters.

14
00:00:51,810 --> 00:00:54,660
In fact,
that's the goal of text clustering.

15
00:00:55,780 --> 00:00:58,260
We want to find such clusters in the data.

16
00:00:59,280 --> 00:01:02,310
But in the case of categorization,
we are given the categories.

17
00:01:02,310 --> 00:01:07,470
So we kind of have
pre-defined categories and

18
00:01:07,470 --> 00:01:11,810
then based on these categories and
training data, we would like to allocate

19
00:01:11,810 --> 00:01:18,710
a document to one of these categories or
sometimes multiple categories.

20
00:01:18,710 --> 00:01:21,160
But because of the similarity
of the two problems,

21
00:01:21,160 --> 00:01:26,930
we can actually get the document
clustering models for text categorization.

22
00:01:26,930 --> 00:01:30,630
And we understand how we can
use generated models to do

23
00:01:30,630 --> 00:01:35,930
text categorization from
the perspective of clustering.

24
00:01:35,930 --> 00:01:41,620
And so, this is a slide that we've talked
about before, about text clustering,

25
00:01:41,620 --> 00:01:47,550
where we assume there are multiple topics
represented by word distributions.

26
00:01:47,550 --> 00:01:49,620
Each topic is one cluster.

27
00:01:49,620 --> 00:01:52,080
So once we estimated such a model,

28
00:01:52,080 --> 00:01:58,380
we faced a problem of deciding which
cluster document d should belong to.

29
00:01:58,380 --> 00:02:04,260
And this question boils down to decide
which theta i has been used to generate d.

30
00:02:06,440 --> 00:02:14,470
Now, suppose d has L words
represented as xi here.

31
00:02:14,470 --> 00:02:21,220
Now, how can you compute
the probability that a particular

32
00:02:21,220 --> 00:02:25,960
topic word distribution zeta i has
been used to generate this document?

33
00:02:27,050 --> 00:02:32,980
Well, in general, we use base
wall to make this influence and

34
00:02:32,980 --> 00:02:40,167
you can see this prior information here
that we need to consider if a topic or

35
00:02:40,167 --> 00:02:44,846
cluster has a higher prior
then it's more likely

36
00:02:44,846 --> 00:02:49,920
that the document has
been from this cluster.

37
00:02:49,920 --> 00:02:52,470
And so, we should favor such a cluster.

38
00:02:52,470 --> 00:02:54,960
The other is a likelihood part,
it's this part.

39
00:02:56,080 --> 00:02:58,880
And this has to do with whether
the topic word of distribution

40
00:02:58,880 --> 00:03:02,370
can explain the content
of this document well.

41
00:03:02,370 --> 00:03:07,730
And we want to pick a topic
that's high by both values.

42
00:03:07,730 --> 00:03:10,810
So more specifically,
we just multiply them together and

43
00:03:10,810 --> 00:03:14,650
then choose which topic
has the highest product.

44
00:03:14,650 --> 00:03:18,740
So more rigorously,
this is what we'd be doing.

45
00:03:18,740 --> 00:03:22,630
So we're going to choose
the topic that would maximize.

46
00:03:22,630 --> 00:03:27,660
This posterior probability at the top
of a given document gets posterior

47
00:03:27,660 --> 00:03:33,020
because this one,
p of the i, is the prior.

48
00:03:33,020 --> 00:03:36,810
That's our belief about
which topic is more likely,

49
00:03:36,810 --> 00:03:39,470
before we observe any document.

50
00:03:39,470 --> 00:03:43,320
But this conditional probability here is

51
00:03:43,320 --> 00:03:47,850
the posterior probability of the topic
after we have observed the document of d.

52
00:03:49,758 --> 00:03:54,130
And base wall allows us to update this
probability based on the prior and

53
00:03:54,130 --> 00:03:59,040
I have shown the details,
below here you can see how

54
00:03:59,040 --> 00:04:04,792
the prior here is related to
the posterior, on the left-hand side.

55
00:04:05,960 --> 00:04:10,870
And this is related to how
well this word distribution

56
00:04:10,870 --> 00:04:16,200
explains the document here, and
the two are related in this way.

57
00:04:16,200 --> 00:04:21,470
So to find the topic that has the higher

58
00:04:21,470 --> 00:04:26,930
posterior probability here it's
equivalent to maximize this product

59
00:04:26,930 --> 00:04:30,450
as we have seen also,
multiple times in this course.

60
00:04:32,300 --> 00:04:37,159
And we can then change the probability
of document in your product of

61
00:04:37,159 --> 00:04:42,019
the probability of each word, and
that's just because we've made

62
00:04:42,019 --> 00:04:46,382
an assumption about independence
in generating each word.

63
00:04:46,382 --> 00:04:49,640
So this is just something that you
have seen in document clustering.

64
00:04:50,680 --> 00:04:56,760
And we now can see clearly how we
can assign a document to a category

65
00:04:56,760 --> 00:05:02,270
based on the information
about word distributions for

66
00:05:02,270 --> 00:05:05,740
these categories and
the prior on these categories.

67
00:05:05,740 --> 00:05:10,690
So this idea can be directly
adapted to do categorization.

68
00:05:10,690 --> 00:05:16,360
And this is precisely what
a Naive Bayes Classifier is doing.

69
00:05:16,360 --> 00:05:19,290
So here it's most really
the same information except

70
00:05:19,290 --> 00:05:21,910
that we're looking at
the categorization problem now.

71
00:05:21,910 --> 00:05:26,620
So we assume that if theta i

72
00:05:26,620 --> 00:05:31,770
represents category i accurately,
that means the word distribution

73
00:05:31,770 --> 00:05:37,120
characterizes the content of
documents in category i accurately.

74
00:05:37,120 --> 00:05:40,802
Then, what we can do is precisely
like what we did for text clustering.

75
00:05:40,802 --> 00:05:45,580
Namely we're going to assign
document d to the category

76
00:05:45,580 --> 00:05:50,310
that has the highest probability
of generating this document.

77
00:05:50,310 --> 00:05:54,990
In other words, we're going to maximize
this posterior probability as well.

78
00:05:56,620 --> 00:05:59,910
And this is related to the prior and

79
00:05:59,910 --> 00:06:04,290
the [INAUDIBLE] as you have
seen on the previous slide.

80
00:06:04,290 --> 00:06:07,400
And so, naturally we can decompose

81
00:06:07,400 --> 00:06:11,840
this [INAUDIBLE] into
a product as you see here.

82
00:06:11,840 --> 00:06:16,770
Now, here, I change the notation so
that we will write down the product as

83
00:06:16,770 --> 00:06:20,970
product of all the words
in the vocabulary, and

84
00:06:20,970 --> 00:06:25,730
even though the document
doesn't contain all the words.

85
00:06:25,730 --> 00:06:31,810
And the product is still accurately
representing the product

86
00:06:31,810 --> 00:06:35,580
of all the words in the document
because of this count here.

87
00:06:37,150 --> 00:06:39,120
When a word,
it doesn't occur in the document.

88
00:06:39,120 --> 00:06:42,840
The count would be 0, so
this time will just disappear.

89
00:06:42,840 --> 00:06:48,380
So if actively we'll just have the product
over other words in the document.

90
00:06:48,380 --> 00:06:50,940
So basically, with Naive Bayes Classifier,

91
00:06:50,940 --> 00:06:55,680
we're going to score each category for
the document by this function.

92
00:06:56,850 --> 00:07:02,390
Now, you may notice that here it involves
a product of a lot of small probabilities.

93
00:07:02,390 --> 00:07:06,480
And this can cause and the four problem.

94
00:07:06,480 --> 00:07:10,269
So one way to solve the problem is
thru take logarithm of this function,

95
00:07:10,269 --> 00:07:13,530
which it doesn't changes all
the often these categories.

96
00:07:13,530 --> 00:07:15,732
But will helps us preserve precision.

97
00:07:15,732 --> 00:07:20,519
And so, this is often the function
that we actually use to

98
00:07:20,519 --> 00:07:24,611
score each category and
then we're going to choose

99
00:07:24,611 --> 00:07:30,300
the category that has the highest
score by this function.

100
00:07:30,300 --> 00:07:34,870
So this is called an Naive Bayes
Classifier, now the keyword base is

101
00:07:34,870 --> 00:07:39,535
understandable because we are applying
a base rule here when we go from

102
00:07:39,535 --> 00:07:46,380
the posterior probability of the topic to
a product of the likelihood and the prior.

103
00:07:47,560 --> 00:07:52,337
Now, it's also called a naive because
we've made an assumption that every word

104
00:07:52,337 --> 00:07:56,553
in the document is generated
independently, and this is indeed a naive

105
00:07:56,553 --> 00:08:00,932
assumption because in reality they're
not generating independently.

106
00:08:00,932 --> 00:08:05,241
Once you see some word,
then other words will more likely occur.

107
00:08:05,241 --> 00:08:08,235
For example,
if you have seen a word like a text.

108
00:08:08,235 --> 00:08:09,598
Than that mixed category,

109
00:08:09,598 --> 00:08:13,640
they see more clustering more likely to
appear than if you have not the same text.

110
00:08:15,490 --> 00:08:18,740
But this assumption allows
us to simplify the problem.

111
00:08:18,740 --> 00:08:22,854
And it's actually quite effective for
many text categorization tasks.

112
00:08:22,854 --> 00:08:26,370
But you should know that
this kind of model doesn't

113
00:08:26,370 --> 00:08:29,000
have to make this assumption.

114
00:08:29,000 --> 00:08:33,760
We could for example, assume that
words may be dependent on each other.

115
00:08:33,760 --> 00:08:38,411
So that would make it a bigram analogy
model or a trigram analogy model.

116
00:08:38,411 --> 00:08:43,019
And of course you can even use a mixture
model to model what the document looks

117
00:08:43,019 --> 00:08:45,120
like in each category.

118
00:08:45,120 --> 00:08:49,530
So in nature, they will be all using
base rule to do classification.

119
00:08:49,530 --> 00:08:54,760
But the actual generating model for
documents in each category can vary.

120
00:08:54,760 --> 00:08:59,670
And here, we just talk about very
simple case perhaps, the simplest case.

121
00:09:00,980 --> 00:09:05,220
So now the question is,
how can we make sure theta i

122
00:09:05,220 --> 00:09:09,520
actually represents category i accurately?

123
00:09:09,520 --> 00:09:13,700
Now in clustering,
we learned that this category i or

124
00:09:13,700 --> 00:09:17,310
what are the distributions for
category i from the data.

125
00:09:17,310 --> 00:09:20,820
But in our case,
what can we do to make sure

126
00:09:20,820 --> 00:09:24,940
this theta i represents indeed category i.

127
00:09:25,960 --> 00:09:27,510
Well if you think about the question, and

128
00:09:27,510 --> 00:09:33,150
you likely come up with the idea
of using the training data.

129
00:09:34,800 --> 00:09:36,050
Indeed in the textbook,

130
00:09:36,050 --> 00:09:40,140
we typically assume that there
is training data available and

131
00:09:40,140 --> 00:09:47,810
those are the documents that unknown
to have generator from which category.

132
00:09:47,810 --> 00:09:51,680
In other words, these are the documents
with known categories assigned and

133
00:09:51,680 --> 00:09:54,450
of course human experts must do that.

134
00:09:54,450 --> 00:09:58,960
In here, you see that T1
represents the set of documents

135
00:09:58,960 --> 00:10:03,020
that are known to have
the generator from category 1.

136
00:10:03,020 --> 00:10:07,187
And T2 represents the documents
that are known to have been

137
00:10:07,187 --> 00:10:09,699
generated from category 2, etc.

138
00:10:09,699 --> 00:10:14,475
Now if you look at this picture,
you'll see that the model

139
00:10:14,475 --> 00:10:18,872
here is really a simplified
unigram language model.

140
00:10:18,872 --> 00:10:20,452
It's no longer mixed modal, why?

141
00:10:20,452 --> 00:10:25,350
Because we already know which distribution
has been used to generate which documents.

142
00:10:25,350 --> 00:10:29,820
There's no uncertainty here, there's
no mixing of different categories here.

143
00:10:30,980 --> 00:10:35,110
So the estimation problem of
course would be simplified.

144
00:10:35,110 --> 00:10:38,720
But in general,
you can imagine what we want to do is

145
00:10:38,720 --> 00:10:42,380
estimate these probabilities
that I marked here.

146
00:10:42,380 --> 00:10:46,799
And what other probability is that we have
to estimate it in order to do relation.

147
00:10:46,799 --> 00:10:48,553
Well there are two kinds.

148
00:10:48,553 --> 00:10:53,114
So one is the prior,
the probability of theta i and

149
00:10:53,114 --> 00:10:57,349
this indicates how popular
each category is or

150
00:10:57,349 --> 00:11:03,224
how likely will it have observed
the document in that category.

151
00:11:03,224 --> 00:11:05,990
The other kind is
the water distributions and

152
00:11:05,990 --> 00:11:10,300
we want to know what words have high
probabilities for each category.

153
00:11:11,690 --> 00:11:15,570
So the idea then is to just
use observe the training data

154
00:11:15,570 --> 00:11:17,810
to estimate these two probabilities.

155
00:11:18,830 --> 00:11:23,261
And in general, we can do this
separately for the different categories.

156
00:11:23,261 --> 00:11:27,650
That's just because these documents
are known to be generated

157
00:11:27,650 --> 00:11:29,565
from a specific category.

158
00:11:29,565 --> 00:11:30,825
So once we know that,

159
00:11:30,825 --> 00:11:35,660
it's in some sense irrelevant of what
other categories we are also dealing with.

160
00:11:37,470 --> 00:11:41,737
So now this is a statistical
estimation problem.

161
00:11:41,737 --> 00:11:44,209
We have observed some
data from some model and

162
00:11:44,209 --> 00:11:47,220
we want to guess
the parameters of this model.

163
00:11:47,220 --> 00:11:49,580
We want to take our best
guess of the parameters.

164
00:11:51,060 --> 00:11:56,073
And this is a problem that we have seen
also several times in this course.

165
00:11:56,073 --> 00:11:58,728
Now, if you haven't thought
about this problem,

166
00:11:58,728 --> 00:12:00,775
haven't seen life based classifier.

167
00:12:00,775 --> 00:12:04,649
It would be very useful for
you to pause the video for a moment and

168
00:12:04,649 --> 00:12:07,690
to think about how to solve this problem.

169
00:12:07,690 --> 00:12:10,680
So let me state the problem again.

170
00:12:10,680 --> 00:12:13,310
So let's just think about with category 1,

171
00:12:13,310 --> 00:12:18,750
we know there is one word of distribution
that has been used to generate documents.

172
00:12:18,750 --> 00:12:23,110
And we generate each word in the document
independently, and we know that we have

173
00:12:23,110 --> 00:12:29,350
observed a set of n sub 1
documents in the set of Q1.

174
00:12:29,350 --> 00:12:32,980
These documents have been all
generated from category 1.

175
00:12:32,980 --> 00:12:37,500
Namely have been all generated
using this same word distribution.

176
00:12:37,500 --> 00:12:40,420
Now the question is,
what would be your guess or

177
00:12:40,420 --> 00:12:44,369
estimate of the probability of
each word in this distribution?

178
00:12:44,369 --> 00:12:49,710
And what would be your guess of
the entire probability of this category?

179
00:12:49,710 --> 00:12:52,200
Of course,
this singular probability depends on

180
00:12:52,200 --> 00:12:54,840
how likely are you to see
documents in other categories?

181
00:12:55,990 --> 00:13:00,970
So think for a moment, how do you
use all this training data including

182
00:13:00,970 --> 00:13:06,430
all these documents that are known
to be in these k categories,

183
00:13:06,430 --> 00:13:08,950
to estimate all these parameters?

184
00:13:08,950 --> 00:13:11,280
Now, if you spend some time
to think about this and

185
00:13:11,280 --> 00:13:15,770
it would help you understand
the following few slides.

186
00:13:15,770 --> 00:13:20,390
So do spend some time to make sure that
you can try to solve this problem, or

187
00:13:20,390 --> 00:13:23,310
do you best to solve the problem yourself.

188
00:13:23,310 --> 00:13:28,270
Now if you have thought about and
then you will realize the following to it.

189
00:13:29,400 --> 00:13:35,880
First, what's the bases for estimating the
prior or the probability of each category.

190
00:13:35,880 --> 00:13:40,080
Well this has to do with whether you
have observed a lot of documents

191
00:13:40,080 --> 00:13:41,625
form that category.

192
00:13:41,625 --> 00:13:44,870
Intuitively, you have seen a lot
of documents in sports and

193
00:13:44,870 --> 00:13:46,770
very few in medical science.

194
00:13:46,770 --> 00:13:51,870
Then you guess is that the probability
of the sports category is larger or

195
00:13:51,870 --> 00:13:55,800
your prior on the category
would be larger.

196
00:13:57,130 --> 00:14:01,570
And what about the basis for estimating
the probability of where each category?

197
00:14:01,570 --> 00:14:05,929
Well the same, and you'll be just
assuming that words that are observed

198
00:14:05,929 --> 00:14:10,645
frequently in the documents that are known
to be generated from a category will

199
00:14:10,645 --> 00:14:12,799
likely have a higher probability.

200
00:14:12,799 --> 00:14:15,493
And that's just a maximum
Naive Bayes made of.

201
00:14:15,493 --> 00:14:20,707
Indeed, that's what we can do, so this
made the probability of which category and

202
00:14:20,707 --> 00:14:24,990
to answer the question,
which category is most popular?

203
00:14:24,990 --> 00:14:31,210
Then we can simply normalize,
the count of documents in each category.

204
00:14:31,210 --> 00:14:36,470
So here you see N sub i denotes
the number of documents in each category.

205
00:14:37,950 --> 00:14:41,313
And we simply just normalize these
counts to make this a probability.

206
00:14:41,313 --> 00:14:46,929
In other words, we make this
probability proportional to the size

207
00:14:46,929 --> 00:14:52,960
of training intercept in each category
that's a size of the set t sub i.

208
00:14:55,260 --> 00:14:58,010
Now what about the word distribution?

209
00:14:58,010 --> 00:14:59,460
Well, we do the same.

210
00:14:59,460 --> 00:15:03,940
Again this time we can do this for
each category.

211
00:15:03,940 --> 00:15:08,640
So let's say,
we're considering category i or theta i.

212
00:15:08,640 --> 00:15:12,480
So which word has a higher probability?

213
00:15:12,480 --> 00:15:15,850
Well, we simply count the word occurrences

214
00:15:15,850 --> 00:15:19,140
in the documents that are known
to be generated from theta i.

215
00:15:20,290 --> 00:15:25,516
And then we put together all
the counts of the same word in the set.

216
00:15:25,516 --> 00:15:30,565
And then we just normalize these
counts to make this distribution

217
00:15:30,565 --> 00:15:35,660
of all the words make all
the probabilities off these words to 1.

218
00:15:35,660 --> 00:15:39,000
So in this case, you're going to see this
is a proportional through the count of

219
00:15:39,000 --> 00:15:43,870
the word in the collection of
training documents T sub i and

220
00:15:43,870 --> 00:15:48,380
that's denoted by c of w and T sub i.

221
00:15:49,710 --> 00:15:55,110
Now, you may notice that we
often write down probable

222
00:15:55,110 --> 00:16:01,715
estimate in the form of being
proportional for certain numbers.

223
00:16:01,715 --> 00:16:03,529
And this is often sufficient,

224
00:16:03,529 --> 00:16:07,033
because we have some constraints
on these distributions.

225
00:16:07,033 --> 00:16:11,281
So the normalizer is
dictated by the constraint.

226
00:16:11,281 --> 00:16:15,191
So in this case, it will be useful for
you to think about

227
00:16:15,191 --> 00:16:19,711
what are the constraints on these
two kinds of probabilities?

228
00:16:19,711 --> 00:16:22,753
So once you figure out
the answer to this question, and

229
00:16:22,753 --> 00:16:25,410
you will know how to
normalize these accounts.

230
00:16:25,410 --> 00:16:32,940
And so this is a good exercise to
work on if it's not obvious to you.

231
00:16:32,940 --> 00:16:35,940
There is another issue in
Naive Bayes which is a smoothing.

232
00:16:35,940 --> 00:16:41,720
In fact the smoothing is a general problem
in older estimate of language morals.

233
00:16:41,720 --> 00:16:43,420
And this has to do with,

234
00:16:43,420 --> 00:16:47,540
what would happen if you have
observed a small amount of data?

235
00:16:47,540 --> 00:16:51,590
So smoothing is an important technique
to address that outsmarts this.

236
00:16:51,590 --> 00:16:56,620
In our case, the training data can be
small and when the data set is small when

237
00:16:56,620 --> 00:17:01,140
we use maximum likely estimator we often
face the problem of zero probability.

238
00:17:01,140 --> 00:17:03,770
That means if an event is not observed

239
00:17:03,770 --> 00:17:06,440
then the estimated
probability would be zero.

240
00:17:06,440 --> 00:17:11,070
In this case, if we have not seen
a word in the training documents for

241
00:17:11,070 --> 00:17:13,590
let's say, category i.

242
00:17:13,590 --> 00:17:18,770
Then our estimator would be zero for the
probability of this one in this category,

243
00:17:18,770 --> 00:17:20,800
and this is generally not accurate.

244
00:17:20,800 --> 00:17:25,380
So we have to do smoothing to make
sure it's not zero probability.

245
00:17:25,380 --> 00:17:30,990
The other reason for smoothing is that
this is a way to bring prior knowledge,

246
00:17:30,990 --> 00:17:35,330
and this is also generally true for
a lot of situations of smoothing.

247
00:17:35,330 --> 00:17:37,346
When the data set is small,

248
00:17:37,346 --> 00:17:41,827
we tend to rely on some prior
knowledge to solve the problem.

249
00:17:41,827 --> 00:17:46,242
So in this case our [INAUDIBLE] says that
no word should have zero probability.

250
00:17:46,242 --> 00:17:51,035
So smoothing allows us to inject
these to prior initial that no

251
00:17:51,035 --> 00:17:53,810
order has a real zero probability.

252
00:17:54,970 --> 00:17:59,230
There is also a third reason which
us sometimes not very obvious, but

253
00:17:59,230 --> 00:18:00,850
we explain that in a moment.

254
00:18:00,850 --> 00:18:05,480
And that is to help achieve
discriminative weighting of terms.

255
00:18:05,480 --> 00:18:08,160
And this is also called IDF weighting,

256
00:18:08,160 --> 00:18:13,640
inverse document frequency weighting that
you have seen in mining word relations.

257
00:18:14,740 --> 00:18:15,850
So how do we do smoothing?

258
00:18:15,850 --> 00:18:19,220
Well in general we add pseudo
counts to these events,

259
00:18:19,220 --> 00:18:21,607
we'll make sure that no event has 0 count.

260
00:18:22,790 --> 00:18:27,676
So one possible way of smoothing
the probability of the category

261
00:18:27,676 --> 00:18:32,483
is to simply add a small non active
constant delta to the count.

262
00:18:32,483 --> 00:18:37,413
Let's pretend that every category
has actually some extra number of

263
00:18:37,413 --> 00:18:39,880
documents represented by delta.

264
00:18:40,990 --> 00:18:45,660
And in the denominator we also add
a k multiplied by delta because

265
00:18:45,660 --> 00:18:48,730
we want the probability to some to 1.

266
00:18:48,730 --> 00:18:54,427
So in total we've added delta k times
because we have a k categories.

267
00:18:54,427 --> 00:18:59,242
Therefore in this sum,
we have to also add k multiply by

268
00:18:59,242 --> 00:19:04,490
delta as a total pseudocount
that we add up to the estimate.

269
00:19:06,420 --> 00:19:09,568
Now, it's interesting to think
about the influence of that data,

270
00:19:09,568 --> 00:19:11,678
obvious data is a smoothing
parameter here.

271
00:19:11,678 --> 00:19:16,285
Meaning that the larger data is and
the more we will do smoothing and

272
00:19:16,285 --> 00:19:19,505
that means we'll more
rely on pseudocounts.

273
00:19:19,505 --> 00:19:24,238
And we might indeed ignore the actual
counts if they are delta is

274
00:19:24,238 --> 00:19:25,587
set to infinity.

275
00:19:25,587 --> 00:19:30,172
Imagine what would happen if there
are approaches positively to infinity?

276
00:19:30,172 --> 00:19:39,270
Well, we are going to say every category
has an infinite amount of documents.

277
00:19:39,270 --> 00:19:43,300
And then there's no distinction to them so
it become just a uniform.

278
00:19:44,850 --> 00:19:46,200
What if delta is 0?

279
00:19:46,200 --> 00:19:51,050
Well, we just go back to the original
estimate based on the observed training

280
00:19:51,050 --> 00:19:54,880
data to estimate to estimate
the probability of each category.

281
00:19:54,880 --> 00:19:57,610
Now we can do the same for
the word distribution.

282
00:19:57,610 --> 00:20:01,670
But in this case,
sometimes we find it useful

283
00:20:01,670 --> 00:20:05,750
to use a nonuniform seudocount for
the word.

284
00:20:05,750 --> 00:20:09,372
So here you'll see we'll add
a pseudocounts to each word and

285
00:20:09,372 --> 00:20:12,143
that's mule multiplied
by the probability of

286
00:20:12,143 --> 00:20:15,781
the word given by a background
language model, theta sub b.

287
00:20:15,781 --> 00:20:19,772
Now that background model in
general can be estimated by using

288
00:20:19,772 --> 00:20:22,070
a logic collection of tests.

289
00:20:22,070 --> 00:20:26,588
Or in this case we will use the whole
set of all the training data to

290
00:20:26,588 --> 00:20:29,693
estimate this background language model.

291
00:20:29,693 --> 00:20:31,494
But we don't have to use this one,

292
00:20:31,494 --> 00:20:35,050
we can use larger test data that
are available from somewhere else.

293
00:20:36,170 --> 00:20:40,110
Now if we use such a background
language model that has pseudocounts,

294
00:20:40,110 --> 00:20:43,605
we'll find that some words will
receive more pseudocounts.

295
00:20:43,605 --> 00:20:44,848
So what are those words?

296
00:20:44,848 --> 00:20:48,580
Well those are the common words
because they get a high probability by

297
00:20:48,580 --> 00:20:50,328
the background average model.

298
00:20:50,328 --> 00:20:53,314
So the pseudocounts added for
such words will be higher.

299
00:20:53,314 --> 00:20:59,126
Real words on the other hand
will have smaller pseudocounts.

300
00:20:59,126 --> 00:21:03,443
Now this addition of background
model would cause a nonuniform

301
00:21:03,443 --> 00:21:06,382
smoothing of these word distributions.

302
00:21:06,382 --> 00:21:10,630
We're going to bring the probability of
those common words to a higher level,

303
00:21:10,630 --> 00:21:12,880
because of the background model.

304
00:21:12,880 --> 00:21:17,769
Now this helps make the difference
of the probability of

305
00:21:17,769 --> 00:21:21,312
such words smaller across categories.

306
00:21:21,312 --> 00:21:26,005
Because every category has some help
from the background four words, and

307
00:21:26,005 --> 00:21:29,050
I get the, a,
which have high probabilities.

308
00:21:29,050 --> 00:21:33,890
Therefore, it's not always so
important that each category

309
00:21:33,890 --> 00:21:38,320
has documents that contain a lot
of occurrences of such words or

310
00:21:38,320 --> 00:21:41,600
the estimate is more influenced
by the background model.

311
00:21:41,600 --> 00:21:44,360
And the consequence is that
when we do categorization,

312
00:21:44,360 --> 00:21:48,420
such words tend not to influence
the decision that much

313
00:21:48,420 --> 00:21:53,590
as words that have small probabilities
from the background language model.

314
00:21:53,590 --> 00:21:57,070
Those words don't get some help
from the background language model.

315
00:21:57,070 --> 00:22:02,080
So the difference would be primary because
of the differences of the occurrences

316
00:22:02,080 --> 00:22:04,089
in the training documents
in different categories.

317
00:22:05,400 --> 00:22:09,970
We also see another smoothing parameter
mu here, which controls the amount of

318
00:22:09,970 --> 00:22:13,610
smoothing and just like a delta does for
the other probability.

319
00:22:14,920 --> 00:22:19,353
And you can easily understand why we
add mu to the denominator, because that

320
00:22:19,353 --> 00:22:23,587
represents the sum of all the pseudocounts
that we add for all the words.

321
00:22:25,689 --> 00:22:29,051
So view is also a non
negative constant and

322
00:22:29,051 --> 00:22:32,611
it's [INAUDIBLE] set to control smoothing.

323
00:22:32,611 --> 00:22:35,409
Now there are some interesting
special cases to think about as well.

324
00:22:35,409 --> 00:22:39,280
First, let's think about when mu
approaches infinity what would happen?

325
00:22:39,280 --> 00:22:41,319
Well in this case
the estimate would approach

326
00:22:43,170 --> 00:22:47,640
to the background language model we'll
attempt to the background language model.

327
00:22:47,640 --> 00:22:52,365
So we will bring every word distribution
to the same background language model and

328
00:22:52,365 --> 00:22:56,352
that essentially remove the difference
between these categories.

329
00:22:56,352 --> 00:22:57,943
Obviously, we don't want to do that.

330
00:22:57,943 --> 00:23:02,683
The other special case is the thing
about the background model and

331
00:23:02,683 --> 00:23:06,919
suppose, we actually set
the two uniform distribution.

332
00:23:06,919 --> 00:23:10,218
And let's say,
1 over the size of the vocabulary.

333
00:23:10,218 --> 00:23:16,121
So each one has the same probability,
then this smoothing formula is

334
00:23:16,121 --> 00:23:22,030
going to be very similar to the one
on the top when we add delta.

335
00:23:22,030 --> 00:23:25,025
It's because we're going to add
a constant pseudocounts to every word.

336
00:23:29,268 --> 00:23:30,441
So in general,

337
00:23:30,441 --> 00:23:35,240
in Naive Bayes categorization we
have to do such a small thing.

338
00:23:35,240 --> 00:23:38,970
And then once we have these probabilities,

339
00:23:38,970 --> 00:23:42,960
then we can compute the score for
each category.

340
00:23:42,960 --> 00:23:44,030
For a document and

341
00:23:44,030 --> 00:23:47,240
then choose the category where it was
the highest score as we discussed earlier.

342
00:23:49,250 --> 00:23:53,266
Now, it's useful to
further understand whether

343
00:23:53,266 --> 00:23:57,863
the Naive Bayes scoring
function actually makes sense.

344
00:23:57,863 --> 00:24:03,755
So to understand that, and also to
understand why adding a background model

345
00:24:03,755 --> 00:24:10,029
will actually achieve the effect of IDF
weighting and to penalize common words.

346
00:24:10,029 --> 00:24:13,669
So suppose we have just two categories and

347
00:24:13,669 --> 00:24:19,395
we're going to score based on
their ratio of probability, right?

348
00:24:19,395 --> 00:24:21,647
So this is the.

349
00:24:24,563 --> 00:24:29,723
Lets say this is our scoring function for

350
00:24:29,723 --> 00:24:33,072
two categories, right?

351
00:24:33,072 --> 00:24:40,100
So, this is a score of a document for
these two categories.

352
00:24:40,100 --> 00:24:44,409
And we're going to score based
on this probability ratio.

353
00:24:44,409 --> 00:24:47,196
So if the ratio is larger,

354
00:24:47,196 --> 00:24:52,907
then it means it's more
likely to be in category one.

355
00:24:52,907 --> 00:24:57,779
So the larger the score is the more likely

356
00:24:57,779 --> 00:25:01,800
the document is in category one.

357
00:25:01,800 --> 00:25:03,810
So by using Bayes' rule,

358
00:25:03,810 --> 00:25:08,150
we can write down this ratio as follows,
and you have seen this before.

359
00:25:09,190 --> 00:25:15,920
Now, we generally take logarithm of this
ratio, and to avoid small probabilities.

360
00:25:15,920 --> 00:25:21,450
And this would then give us this
formula in the second line.

361
00:25:21,450 --> 00:25:23,520
And here we see something
really interesting,

362
00:25:23,520 --> 00:25:28,300
because this is our scoring function for
deciding between the two categories.

363
00:25:30,280 --> 00:25:34,718
And if you look at this function,
we'll see it has several parts.

364
00:25:34,718 --> 00:25:38,860
The first part here is actually
log of probability ratio.

365
00:25:38,860 --> 00:25:40,409
And so this is a category bias.

366
00:25:41,870 --> 00:25:43,740
It doesn't really depend on the document.

367
00:25:43,740 --> 00:25:48,780
It just says which category is more
likely and then we would then favor

368
00:25:48,780 --> 00:25:53,240
this category slightly, right?

369
00:25:53,240 --> 00:25:58,114
So, the second part has a sum
of all the words, right?

370
00:25:58,114 --> 00:26:03,590
So, these are the words that
are observed in the document but

371
00:26:03,590 --> 00:26:06,510
in general we can consider all
the words in the vocabulary.

372
00:26:06,510 --> 00:26:08,960
So here we're going to
collect the evidence

373
00:26:08,960 --> 00:26:12,280
about which category is more likely,
right?

374
00:26:12,280 --> 00:26:16,920
So inside of the sum you can see
there is product of two things.

375
00:26:16,920 --> 00:26:19,056
The first, is a count of the word.

376
00:26:19,056 --> 00:26:25,660
And this count of the word serves as
a feature to represent the document.

377
00:26:27,080 --> 00:26:30,390
And this is what we can
collect from document.

378
00:26:30,390 --> 00:26:33,645
The second part is
the weight of this feature,

379
00:26:33,645 --> 00:26:37,236
here it's the weight on which word, right?

380
00:26:37,236 --> 00:26:42,500
This weight tells us to
what extent observing

381
00:26:42,500 --> 00:26:47,487
this word helps contribute in our decision

382
00:26:47,487 --> 00:26:51,930
to put this document in category one.

383
00:26:51,930 --> 00:26:54,495
Now remember,
the higher the scoring function is,

384
00:26:54,495 --> 00:26:56,426
the more likely it's in category one.

385
00:26:56,426 --> 00:27:01,493
Now if you look at this ratio, basically,
sorry this weight it's basically based

386
00:27:01,493 --> 00:27:06,025
on the ratio of the probability of the
word from each of the two distributions.

387
00:27:06,025 --> 00:27:09,492
Essentially we're comparing
the probability of the word from the two

388
00:27:09,492 --> 00:27:11,080
distributions.

389
00:27:11,080 --> 00:27:14,780
And if it's a higher according to theta 1,

390
00:27:14,780 --> 00:27:20,289
then according to theta 2,
then this weight would be positive.

391
00:27:20,289 --> 00:27:23,860
And therefore it means when
we observe such a word,

392
00:27:23,860 --> 00:27:27,940
we will say that it's more
likely to be from category one.

393
00:27:27,940 --> 00:27:30,237
And the more we observe such a word,

394
00:27:30,237 --> 00:27:34,155
the more likely the document
will be classified as theta 1.

395
00:27:35,210 --> 00:27:38,940
If, on the other hand,
the probability of the word from

396
00:27:38,940 --> 00:27:42,720
theta 1 is smaller than the probability
of the word from theta 2,

397
00:27:42,720 --> 00:27:45,220
then you can see that
this word is negative.

398
00:27:45,220 --> 00:27:51,390
Therefore, this is negative evidence for
supporting category one.

399
00:27:51,390 --> 00:27:54,010
That means the more we
observe such a word,

400
00:27:54,010 --> 00:27:56,587
the more likely the document
is actually from theta 2.

401
00:27:58,270 --> 00:28:01,350
So this formula now makes a little sense,
right?

402
00:28:01,350 --> 00:28:05,050
So we're going to aggregate all
the evidence from the document,

403
00:28:05,050 --> 00:28:07,010
we take a sum of all the words.

404
00:28:07,010 --> 00:28:09,820
We can call this the features

405
00:28:09,820 --> 00:28:13,530
that we collected from the document
that would help us make the decision.

406
00:28:13,530 --> 00:28:18,550
And then each feature has
a weight that tells us how

407
00:28:19,660 --> 00:28:24,815
does this feature support category one or
just support category two.

408
00:28:24,815 --> 00:28:30,465
And this is estimated as the log of
probability ratio here in naïve Bayes.

409
00:28:32,315 --> 00:28:35,565
And then finally we have
this constant of bias here.

410
00:28:35,565 --> 00:28:39,555
So that formula actually
is a formula that can

411
00:28:39,555 --> 00:28:43,862
be generalized to accommodate
more features and

412
00:28:43,862 --> 00:28:48,704
that's why I have introduce
some other symbols here.

413
00:28:48,704 --> 00:28:54,546
To introduce beta 0 to denote the Bayes
and fi to denote the each feature and

414
00:28:54,546 --> 00:28:58,269
beta sub i to denote
the weight on each feature.

415
00:28:58,269 --> 00:29:03,154
Now we do this generalisation,
what we see is that in

416
00:29:03,154 --> 00:29:08,815
general we can represent
the document by feature vector fi,

417
00:29:08,815 --> 00:29:13,960
here of course in this case
fi is the count of a word.

418
00:29:13,960 --> 00:29:17,700
But in general, we can put any features
that we think are relevant for

419
00:29:17,700 --> 00:29:18,760
categorization.

420
00:29:18,760 --> 00:29:20,650
For example, document length or

421
00:29:20,650 --> 00:29:25,720
font size or
count of other patterns in the document.

422
00:29:27,310 --> 00:29:35,120
And then our scoring function can be
defined as a sum of a constant beta 0 and

423
00:29:35,120 --> 00:29:40,430
the sum of the feature
weights of all the features.

424
00:29:42,156 --> 00:29:46,890
So if each f sub i is a feature
value then we multiply the value

425
00:29:46,890 --> 00:29:51,140
by the corresponding weight,
beta sub i, and we just take the sum.

426
00:29:51,140 --> 00:29:54,860
And this is the aggregate of all evidence
that we can collect from all these

427
00:29:54,860 --> 00:29:56,360
features.

428
00:29:56,360 --> 00:29:57,960
And of course there are parameters here.

429
00:29:57,960 --> 00:29:59,060
So what are the parameters?

430
00:29:59,060 --> 00:30:00,510
Well, these are the betas.

431
00:30:00,510 --> 00:30:02,690
These betas are weights.

432
00:30:02,690 --> 00:30:07,040
And with a proper setting of the weights,
then we can expect such a scoring function

433
00:30:07,040 --> 00:30:13,470
to work well to classify documents,
just like in the case of naive Bayes.

434
00:30:13,470 --> 00:30:16,770
We can clearly see naive Bayes
classifier as a special case of

435
00:30:16,770 --> 00:30:18,760
this general classifier.

436
00:30:18,760 --> 00:30:23,940
Actually, this general form is very close
to a classifier called a logistical

437
00:30:23,940 --> 00:30:28,800
regression, and this is actually one
of those conditional approaches or

438
00:30:28,800 --> 00:30:31,210
discriminative approaches
to classification.

439
00:30:32,340 --> 00:30:36,980
And we're going to talk more
about such approaches later, but

440
00:30:36,980 --> 00:30:40,600
here I want you to note that
there is a strong connection,

441
00:30:40,600 --> 00:30:44,350
a close connection between
the two kinds of approaches.

442
00:30:44,350 --> 00:30:48,930
And this slide shows how naive Bayes
classifier can be connected to

443
00:30:48,930 --> 00:30:50,663
a logistic regression.

444
00:30:50,663 --> 00:30:55,692
And you can also see that in
discriminative classifiers

445
00:30:55,692 --> 00:31:00,079
that tend to use more
general form on the bottom,

446
00:31:00,079 --> 00:31:05,116
we can accommodate more
features to solve the problem.

447
00:31:05,116 --> 00:31:15,116
[MUSIC]
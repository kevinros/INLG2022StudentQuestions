1
00:00:06,750 --> 00:00:12,040
This lecture is about Probabilistic Topic
Models for topic mining and analysis.

2
00:00:13,350 --> 00:00:14,110
In this lecture,

3
00:00:14,110 --> 00:00:16,909
we're going to continue talking
about the topic mining and analysis.

4
00:00:18,190 --> 00:00:20,490
We're going to introduce
probabilistic topic models.

5
00:00:22,410 --> 00:00:26,140
So this is a slide that
you have seen earlier,

6
00:00:26,140 --> 00:00:30,640
where we discussed the problems
with using a term as a topic.

7
00:00:30,640 --> 00:00:35,370
So, to solve these problems
intuitively we need to use

8
00:00:35,370 --> 00:00:37,950
more words to describe the topic.

9
00:00:37,950 --> 00:00:43,110
And this will address the problem
of lack of expressive power.

10
00:00:43,110 --> 00:00:45,040
When we have more words that we
can use to describe the topic,

11
00:00:45,040 --> 00:00:49,880
that we can describe complicated topics.

12
00:00:49,880 --> 00:00:54,030
To address the second problem we
need to introduce weights on words.

13
00:00:54,030 --> 00:00:59,140
This is what allows you to distinguish
subtle differences in topics, and

14
00:00:59,140 --> 00:01:04,600
to introduce semantically
related words in a fuzzy manner.

15
00:01:04,600 --> 00:01:09,240
Finally, to solve the problem of
word ambiguity, we need to split

16
00:01:09,240 --> 00:01:14,700
ambiguous word, so
that we can disambiguate its topic.

17
00:01:15,720 --> 00:01:21,060
It turns out that all these can be done
by using a probabilistic topic model.

18
00:01:21,060 --> 00:01:25,520
And that's why we're going to spend a lot
of lectures to talk about this topic.

19
00:01:25,520 --> 00:01:28,130
So the basic idea here is that,

20
00:01:28,130 --> 00:01:32,600
improve the replantation of
topic as one distribution.

21
00:01:32,600 --> 00:01:35,650
So what you see now is
the older replantation.

22
00:01:35,650 --> 00:01:40,730
Where we replanted each topic, it was just
one word, or one term, or one phrase.

23
00:01:40,730 --> 00:01:45,240
But now we're going to use a word
distribution to describe the topic.

24
00:01:45,240 --> 00:01:47,110
So here you see that for sports.

25
00:01:47,110 --> 00:01:50,220
We're going to use
the word distribution over

26
00:01:50,220 --> 00:01:53,160
theoretical speaking all
the words in our vocabulary.

27
00:01:54,650 --> 00:01:59,150
So for example, the high
probability words here are sports,

28
00:01:59,150 --> 00:02:03,880
game, basketball,
football, play, star, etc.

29
00:02:03,880 --> 00:02:06,100
These are sports related terms.

30
00:02:06,100 --> 00:02:10,150
And of course it would also give
a non-zero probability to some other word

31
00:02:10,150 --> 00:02:15,430
like Trouble which might be
related to sports in general,

32
00:02:15,430 --> 00:02:17,420
not so much related to topic.

33
00:02:18,900 --> 00:02:23,030
In general we can imagine a non
zero probability for all the words.

34
00:02:23,030 --> 00:02:27,890
And some words that are not read and
would have very, very small probabilities.

35
00:02:27,890 --> 00:02:29,830
And these probabilities will sum to one.

36
00:02:31,780 --> 00:02:34,500
So that it forms a distribution
of all the words.

37
00:02:36,650 --> 00:02:41,440
Now intuitively, this distribution
represents a topic in that if we assemble

38
00:02:41,440 --> 00:02:46,780
words from the distribution, we tended
to see words that are ready to dispose.

39
00:02:48,470 --> 00:02:53,236
You can also see, as a very special case,
if the probability of the mass

40
00:02:53,236 --> 00:02:57,387
is concentrated in entirely on
just one word, it's sports.

41
00:02:57,387 --> 00:03:01,670
And this basically degenerates
to the symbol foundation

42
00:03:01,670 --> 00:03:03,270
of a topic was just one word.

43
00:03:04,640 --> 00:03:10,420
But as a distribution,
this topic of representation can,

44
00:03:10,420 --> 00:03:13,980
in general,
involve many words to describe a topic and

45
00:03:13,980 --> 00:03:17,970
can model several differences
in semantics of a topic.

46
00:03:17,970 --> 00:03:24,500
Similarly we can model Travel and Science
with their respective distributions.

47
00:03:24,500 --> 00:03:30,120
In the distribution for Travel we see top
words like attraction, trip, flight etc.

48
00:03:31,670 --> 00:03:36,110
Whereas in Science we see scientist,
spaceship, telescope, or

49
00:03:36,110 --> 00:03:39,820
genomics, and, you know,
science related terms.

50
00:03:39,820 --> 00:03:43,260
Now that doesn't mean sports related terms

51
00:03:43,260 --> 00:03:46,330
will necessarily have zero
probabilities for science.

52
00:03:46,330 --> 00:03:51,860
In general we can imagine all of these
words we have now zero probabilities.

53
00:03:51,860 --> 00:03:55,250
It's just that for a particular
topic in some words we have very,

54
00:03:55,250 --> 00:03:56,620
very small probabilities.

55
00:03:58,200 --> 00:04:02,770
Now you can also see there are some
words that are shared by these topics.

56
00:04:02,770 --> 00:04:07,600
When I say shared it just means even
with some probability threshold,

57
00:04:07,600 --> 00:04:10,990
you can still see one word
occurring much more topics.

58
00:04:10,990 --> 00:04:13,140
In this case I mark them in black.

59
00:04:13,140 --> 00:04:17,110
So you can see travel, for example,
occurred in all the three topics here, but

60
00:04:17,110 --> 00:04:19,420
with different probabilities.

61
00:04:19,420 --> 00:04:23,237
It has the highest probability for
the Travel topic, 0.05.

62
00:04:23,237 --> 00:04:29,050
But with much smaller probabilities for
Sports and Science, which makes sense.

63
00:04:29,050 --> 00:04:32,450
And similarly, you can see a Star
also occurred in Sports and

64
00:04:32,450 --> 00:04:35,420
Science with reasonably
high probabilities.

65
00:04:35,420 --> 00:04:39,690
Because they might be actually
related to the two topics.

66
00:04:39,690 --> 00:04:43,420
So with this replantation it addresses the
three problems that I mentioned earlier.

67
00:04:43,420 --> 00:04:46,750
First, it now uses multiple
words to describe a topic.

68
00:04:46,750 --> 00:04:50,700
So it allows us to describe
a fairly complicated topics.

69
00:04:50,700 --> 00:04:53,400
Second, it assigns weights to terms.

70
00:04:53,400 --> 00:04:57,060
So now we can model several
differences of semantics.

71
00:04:57,060 --> 00:05:02,390
And you can bring in related
words together to model a topic.

72
00:05:02,390 --> 00:05:07,930
Third, because we have probabilities for
the same word in different topics,

73
00:05:07,930 --> 00:05:12,210
we can disintegrate the sense of word.

74
00:05:12,210 --> 00:05:16,930
In the text to decode
it's underlying topic,

75
00:05:16,930 --> 00:05:22,480
to address all these three problems with
this new way of representing a topic.

76
00:05:22,480 --> 00:05:27,650
So now of course our problem definition
has been refined just slightly.

77
00:05:27,650 --> 00:05:32,090
The slight is very similar to what
you've seen before except we have

78
00:05:32,090 --> 00:05:34,920
added refinement for what our topic is.

79
00:05:34,920 --> 00:05:41,180
Now each topic is word distribution,
and for each word distribution we know

80
00:05:41,180 --> 00:05:45,460
that all the probabilities should sum to
one with all the words in the vocabulary.

81
00:05:45,460 --> 00:05:47,640
So you see a constraint here.

82
00:05:47,640 --> 00:05:53,060
And we still have another constraint
on the topic coverage, namely pis.

83
00:05:53,060 --> 00:05:58,180
So all the Pi sub ij's must sum to one for
the same document.

84
00:05:59,620 --> 00:06:01,250
So how do we solve this problem?

85
00:06:01,250 --> 00:06:05,470
Well, let's look at this problem
as a computation problem.

86
00:06:05,470 --> 00:06:07,560
So we clearly specify it's input and

87
00:06:07,560 --> 00:06:11,190
output and
illustrate it here on this side.

88
00:06:11,190 --> 00:06:12,920
Input of course is our text data.

89
00:06:12,920 --> 00:06:18,620
C is our collection but we also generally
assume we know the number of topics, k.

90
00:06:18,620 --> 00:06:22,940
Or we hypothesize a number and
then try to bind k topics,

91
00:06:22,940 --> 00:06:27,820
even though we don't know the exact
topics that exist in the collection.

92
00:06:27,820 --> 00:06:32,960
And V is the vocabulary that has
a set of words that determines what

93
00:06:32,960 --> 00:06:38,880
units would be treated as
the basic units for analysis.

94
00:06:38,880 --> 00:06:44,780
In most cases we'll use words
as the basis for analysis.

95
00:06:44,780 --> 00:06:46,429
And that means each word is a unique.

96
00:06:47,610 --> 00:06:53,560
Now the output would consist of as first
a set of topics represented by theta I's.

97
00:06:53,560 --> 00:06:55,280
Each theta I is a word distribution.

98
00:06:56,430 --> 00:07:02,860
And we also want to know the coverage
of topics in each document.

99
00:07:02,860 --> 00:07:03,520
So that's.

100
00:07:03,520 --> 00:07:06,250
That the same pi ijs
that we have seen before.

101
00:07:07,470 --> 00:07:13,460
So given a set of text data we would
like compute all these distributions and

102
00:07:13,460 --> 00:07:16,980
all these coverages as you
have seen on this slide.

103
00:07:18,130 --> 00:07:21,520
Now of course there may be many
different ways of solving this problem.

104
00:07:21,520 --> 00:07:24,670
In theory, you can write the [INAUDIBLE]
program to solve this problem,

105
00:07:24,670 --> 00:07:27,050
but here we're going to introduce

106
00:07:27,050 --> 00:07:32,200
a general way of solving this
problem called a generative model.

107
00:07:32,200 --> 00:07:35,770
And this is, in fact,
a very general idea and

108
00:07:35,770 --> 00:07:41,390
it's a principle way of using statistical
modeling to solve text mining problems.

109
00:07:41,390 --> 00:07:46,190
And here I dimmed the picture
that you have seen before

110
00:07:46,190 --> 00:07:49,470
in order to show the generation process.

111
00:07:49,470 --> 00:07:55,960
So the idea of this approach is actually
to first design a model for our data.

112
00:07:55,960 --> 00:08:02,070
So we design a probabilistic model
to model how the data are generated.

113
00:08:02,070 --> 00:08:04,180
Of course,
this is based on our assumption.

114
00:08:04,180 --> 00:08:08,060
The actual data aren't
necessarily generating this way.

115
00:08:08,060 --> 00:08:11,930
So that gave us a probability
distribution of the data

116
00:08:11,930 --> 00:08:13,980
that you are seeing on this slide.

117
00:08:13,980 --> 00:08:18,840
Given a particular model and
parameters that are denoted by lambda.

118
00:08:18,840 --> 00:08:22,040
So this template of actually consists of

119
00:08:22,040 --> 00:08:24,380
all the parameters that
we're interested in.

120
00:08:24,380 --> 00:08:27,780
And these parameters in general
will control the behavior of

121
00:08:27,780 --> 00:08:29,370
the probability risk model.

122
00:08:29,370 --> 00:08:32,530
Meaning that if you set these
parameters with different values and

123
00:08:32,530 --> 00:08:36,820
it will give some data points
higher probabilities than others.

124
00:08:36,820 --> 00:08:39,910
Now in this case of course,
for our text mining problem or

125
00:08:39,910 --> 00:08:44,100
more precisely topic mining problem
we have the following plans.

126
00:08:44,100 --> 00:08:49,450
First of all we have theta i's which
is a word distribution snd then we have

127
00:08:49,450 --> 00:08:52,070
a set of pis for each document.

128
00:08:52,070 --> 00:08:58,980
And since we have n documents, so we have
n sets of pis, and each set the pi up.

129
00:08:58,980 --> 00:09:01,430
The pi values will sum to one.

130
00:09:01,430 --> 00:09:06,370
So this is to say that we
first would pretend we already

131
00:09:06,370 --> 00:09:10,640
have these word distributions and
the coverage numbers.

132
00:09:10,640 --> 00:09:18,010
And then we can see how we can generate
data by using such distributions.

133
00:09:18,010 --> 00:09:21,950
So how do we model the data in this way?

134
00:09:21,950 --> 00:09:25,280
And we assume that the data
are actual symbols

135
00:09:25,280 --> 00:09:29,530
drawn from such a model that
depends on these parameters.

136
00:09:29,530 --> 00:09:31,290
Now one interesting question here is to

137
00:09:32,320 --> 00:09:35,080
think about how many
parameters are there in total?

138
00:09:35,080 --> 00:09:41,360
Now obviously we can already see
n multiplied by K parameters.

139
00:09:41,360 --> 00:09:42,140
For pi's.

140
00:09:42,140 --> 00:09:44,530
We also see k theta i's.

141
00:09:44,530 --> 00:09:49,110
But each theta i is actually a set
of probability values, right?

142
00:09:49,110 --> 00:09:51,580
It's a distribution of words.

143
00:09:51,580 --> 00:09:54,000
So I leave this as an exercise for

144
00:09:54,000 --> 00:09:59,980
you to figure out exactly how
many parameters there are here.

145
00:09:59,980 --> 00:10:04,690
Now once we set up the model then
we can fit the model to our data.

146
00:10:04,690 --> 00:10:07,900
Meaning that we can
estimate the parameters or

147
00:10:07,900 --> 00:10:11,010
infer the parameters based on the data.

148
00:10:11,010 --> 00:10:14,930
In other words we would like to
adjust these parameter values.

149
00:10:14,930 --> 00:10:20,330
Until we give our data set
the maximum probability.

150
00:10:20,330 --> 00:10:22,880
I just said,
depending on the parameter values,

151
00:10:22,880 --> 00:10:27,090
some data points will have higher
probabilities than others.

152
00:10:27,090 --> 00:10:28,620
What we're interested in, here,

153
00:10:28,620 --> 00:10:33,420
is what parameter values will give
our data set the highest probability?

154
00:10:33,420 --> 00:10:37,620
So I also illustrate the problem
with a picture that you see here.

155
00:10:37,620 --> 00:10:41,720
On the X axis I just illustrate lambda,
the parameters,

156
00:10:41,720 --> 00:10:44,260
as a one dimensional variable.

157
00:10:44,260 --> 00:10:49,360
It's oversimplification, obviously,
but it suffices to show the idea.

158
00:10:49,360 --> 00:10:53,370
And the Y axis shows the probability
of the data, observe.

159
00:10:53,370 --> 00:10:57,780
This probability obviously depends
on this setting of lambda.

160
00:10:57,780 --> 00:11:01,480
So that's why it varies as you
change the value of lambda.

161
00:11:01,480 --> 00:11:04,830
What we're interested here
is to find the lambda star.

162
00:11:05,880 --> 00:11:09,259
That would maximize the probability
of the observed data.

163
00:11:10,440 --> 00:11:15,470
So this would be, then,
our estimate of the parameters.

164
00:11:15,470 --> 00:11:17,040
And these parameters,

165
00:11:17,040 --> 00:11:21,720
note that are precisely what we
hoped to discover from text data.

166
00:11:21,720 --> 00:11:25,405
So we'd treat these parameters
as actually the outcome or

167
00:11:25,405 --> 00:11:28,046
the output of the data mining algorithm.

168
00:11:28,046 --> 00:11:32,966
So this is the general idea of using

169
00:11:32,966 --> 00:11:38,231
a generative model for text mining.

170
00:11:38,231 --> 00:11:42,762
First, we design a model with
some parameter values to fit

171
00:11:42,762 --> 00:11:44,804
the data as well as we can.

172
00:11:44,804 --> 00:11:47,207
After we have fit the data,
we will recover some parameter value.

173
00:11:47,207 --> 00:11:48,827
We will use the specific
parameter value And

174
00:11:48,827 --> 00:11:50,910
those would be the output
of the algorithm.

175
00:11:50,910 --> 00:11:55,880
And we'll treat those as actually
the discovered knowledge from text data.

176
00:11:55,880 --> 00:11:59,460
By varying the model of course we
can discover different knowledge.

177
00:11:59,460 --> 00:12:03,840
So to summarize, we introduced
a new way of representing topic,

178
00:12:03,840 --> 00:12:09,020
namely representing as word distribution
and this has the advantage of using

179
00:12:09,020 --> 00:12:14,039
multiple words to describe a complicated
topic.It also allow us to assign

180
00:12:14,039 --> 00:12:19,390
weights on words so we have more than
several variations of semantics.

181
00:12:19,390 --> 00:12:23,390
We talked about the task of topic mining,
and answers.

182
00:12:23,390 --> 00:12:26,430
When we define a topic as distribution.

183
00:12:26,430 --> 00:12:30,140
So the importer is a clashing of text
articles and a number of topics and

184
00:12:30,140 --> 00:12:33,000
a vocabulary set and
the output is a set of topics.

185
00:12:33,000 --> 00:12:35,470
Each is a word distribution and

186
00:12:35,470 --> 00:12:38,730
also the coverage of all
the topics in each document.

187
00:12:38,730 --> 00:12:43,870
And these are formally represented
by theta i's and pi i's.

188
00:12:43,870 --> 00:12:48,710
And we have two constraints here for
these parameters.

189
00:12:48,710 --> 00:12:53,320
The first is the constraints
on the worded distributions.

190
00:12:53,320 --> 00:12:56,820
In each worded distribution
the probability of all the words

191
00:12:56,820 --> 00:12:59,400
must sum to 1,
all the words in the vocabulary.

192
00:12:59,400 --> 00:13:03,960
The second constraint is on
the topic coverage in each document.

193
00:13:03,960 --> 00:13:08,600
A document is not allowed to recover
a topic outside of the set of topics that

194
00:13:08,600 --> 00:13:10,200
we are discovering.

195
00:13:10,200 --> 00:13:17,220
So, the coverage of each of these k
topics would sum to one for a document.

196
00:13:17,220 --> 00:13:21,580
We also introduce a general idea of using
a generative model for text mining.

197
00:13:21,580 --> 00:13:27,920
And the idea here is, first we're design
a model to model the generation of data.

198
00:13:27,920 --> 00:13:30,780
We simply assume that they
are generative in this way.

199
00:13:30,780 --> 00:13:34,730
And inside the model we embed some
parameters that we're interested in

200
00:13:34,730 --> 00:13:35,650
denoted by lambda.

201
00:13:36,770 --> 00:13:40,605
And then we can infer the most
likely parameter values lambda star,

202
00:13:40,605 --> 00:13:41,935
given a particular data set.

203
00:13:43,095 --> 00:13:48,975
And we can then take the lambda star as
knowledge discovered from the text for

204
00:13:48,975 --> 00:13:49,495
our problem.

205
00:13:50,555 --> 00:13:53,115
And we can adjust
the design of the model and

206
00:13:53,115 --> 00:13:58,855
the parameters to discover various
kinds of knowledge from text.

207
00:13:58,855 --> 00:14:04,999
As you will see later
in the other lectures.

208
00:14:04,999 --> 00:14:14,999
[MUSIC]


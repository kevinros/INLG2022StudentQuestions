1
00:00:00,025 --> 00:00:07,935
[SOUND]
This

2
00:00:07,935 --> 00:00:14,253
lecture is about
the Paradigmatics Relation Discovery.

3
00:00:14,253 --> 00:00:19,131
In this lecture we are going to talk about
how to discover a particular kind of word

4
00:00:19,131 --> 00:00:22,160
association called
a paradigmatical relation.

5
00:00:25,400 --> 00:00:30,307
By definition,
two words are paradigmatically

6
00:00:30,307 --> 00:00:34,503
related if they share a similar context.

7
00:00:34,503 --> 00:00:39,086
Namely, they occur in
similar positions in text.

8
00:00:39,086 --> 00:00:44,280
So naturally our idea of discovering such
a relation is to look at the context

9
00:00:44,280 --> 00:00:49,080
of each word and then try to compute
the similarity of those contexts.

10
00:00:50,160 --> 00:00:54,360
So here is an example of
context of a word, cat.

11
00:00:55,800 --> 00:01:01,690
Here I have taken the word
cat out of the context and

12
00:01:01,690 --> 00:01:08,080
you can see we are seeing some remaining
words in the sentences that contain cat.

13
00:01:09,610 --> 00:01:12,479
Now, we can do the same thing for
another word like dog.

14
00:01:13,660 --> 00:01:18,370
So in general we would like to capture
such a context and then try to assess

15
00:01:18,370 --> 00:01:23,340
the similarity of the context of cat and
the context of a word like dog.

16
00:01:24,790 --> 00:01:29,970
So now the question is how can we
formally represent the context and

17
00:01:29,970 --> 00:01:31,458
then define the similarity function.

18
00:01:33,340 --> 00:01:38,560
So first, we note that the context
actually contains a lot of words.

19
00:01:38,560 --> 00:01:43,637
So, they can be regarded as
a pseudo document, a imagine

20
00:01:43,637 --> 00:01:49,370
document, but there are also different
ways of looking at the context.

21
00:01:49,370 --> 00:01:57,470
For example, we can look at the word
that occurs before the word cat.

22
00:01:57,470 --> 00:02:00,440
We can call this context Left1 context.

23
00:02:00,440 --> 00:02:04,980
All right, so in this case you
will see words like my, his, or

24
00:02:04,980 --> 00:02:07,430
big, a, the, et cetera.

25
00:02:07,430 --> 00:02:12,690
These are the words that can
occur to left of the word cat.

26
00:02:12,690 --> 00:02:19,280
So we say my cat, his cat,
big cat, a cat, et cetera.

27
00:02:19,280 --> 00:02:24,180
Similarly, we can also collect the words
that occur right after the word cat.

28
00:02:24,180 --> 00:02:28,156
We can call this context Right1, and

29
00:02:28,156 --> 00:02:34,128
here we see words like eats,
ate, is, has, et cetera.

30
00:02:34,128 --> 00:02:35,907
Or, more generally,

31
00:02:35,907 --> 00:02:41,253
we can look at all the words in
the window of text around the word cat.

32
00:02:41,253 --> 00:02:46,960
Here, let's say we can take a window
of 8 words around the word cat.

33
00:02:46,960 --> 00:02:48,720
We call this context Window8.

34
00:02:49,850 --> 00:02:54,680
Now, of course, you can see all
the words from left or from right, and

35
00:02:54,680 --> 00:02:58,829
so we'll have a bag of words in
general to represent the context.

36
00:03:01,270 --> 00:03:06,410
Now, such a word based representation
would actually give us

37
00:03:06,410 --> 00:03:12,230
an interesting way to define the
perspective of measuring the similarity.

38
00:03:12,230 --> 00:03:15,911
Because if you look at just
the similarity of Left1,

39
00:03:15,911 --> 00:03:21,750
then we'll see words that share
just the words in the left context,

40
00:03:21,750 --> 00:03:27,650
and we kind of ignored the other words
that are also in the general context.

41
00:03:27,650 --> 00:03:32,380
So that gives us one perspective to
measure the similarity, and similarly,

42
00:03:32,380 --> 00:03:34,244
if we only use the Right1 context,

43
00:03:34,244 --> 00:03:38,420
we will capture this narrative
from another perspective.

44
00:03:38,420 --> 00:03:43,040
Using both the Left1 and
Right1 of course would allow us to capture

45
00:03:43,040 --> 00:03:47,720
the similarity with even
more strict criteria.

46
00:03:49,910 --> 00:03:54,744
So in general, context may contain
adjacent words, like eats and

47
00:03:54,744 --> 00:03:59,575
my, that you see here, or
non-adjacent words, like Saturday,

48
00:03:59,575 --> 00:04:02,961
Tuesday, or
some other words in the context.

49
00:04:05,461 --> 00:04:10,174
And this flexibility also allows us
to match the similarity in somewhat

50
00:04:10,174 --> 00:04:11,660
different ways.

51
00:04:11,660 --> 00:04:13,500
Sometimes this is useful,

52
00:04:13,500 --> 00:04:19,130
as we might want to capture
similarity base on general content.

53
00:04:19,130 --> 00:04:25,270
That would give us loosely
related paradigmatical relations.

54
00:04:25,270 --> 00:04:29,340
Whereas if you use only the words
immediately to the left and

55
00:04:29,340 --> 00:04:35,520
to the right of the word, then you
likely will capture words that are very

56
00:04:35,520 --> 00:04:39,950
much related by their syntactical
categories and semantics.

57
00:04:41,170 --> 00:04:46,304
So the general idea of discovering
paradigmatical relations

58
00:04:46,304 --> 00:04:50,754
is to compute the similarity
of context of two words.

59
00:04:50,754 --> 00:04:55,264
So here, for example,
we can measure the similarity of cat and

60
00:04:55,264 --> 00:04:59,110
dog based on the similarity
of their context.

61
00:04:59,110 --> 00:05:02,890
In general, we can combine all
kinds of views of the context.

62
00:05:02,890 --> 00:05:06,395
And so the similarity function is,
in general,

63
00:05:06,395 --> 00:05:10,336
a combination of similarities
on different context.

64
00:05:10,336 --> 00:05:14,849
And of course, we can also assign
weights to these different

65
00:05:14,849 --> 00:05:20,170
similarities to allow us to focus
more on a particular kind of context.

66
00:05:20,170 --> 00:05:24,395
And this would be naturally
application specific, but again,

67
00:05:24,395 --> 00:05:28,935
here the main idea for discovering
pardigmatically related words is

68
00:05:28,935 --> 00:05:32,470
to computer the similarity
of their context.

69
00:05:32,470 --> 00:05:37,670
So next let's see how we exactly
compute these similarity functions.

70
00:05:37,670 --> 00:05:42,235
Now to answer this question,
it is useful to think of bag of words

71
00:05:42,235 --> 00:05:46,520
representation as vectors
in a vector space model.

72
00:05:48,340 --> 00:05:53,016
Now those of you who have been
familiar with information retrieval or

73
00:05:53,016 --> 00:05:57,936
textual retrieval techniques would
realize that vector space model has

74
00:05:57,936 --> 00:06:02,711
been used frequently for
modeling documents and queries for search.

75
00:06:02,711 --> 00:06:08,115
But here we also find it convenient
to model the context of a word for

76
00:06:08,115 --> 00:06:11,130
paradigmatic relation discovery.

77
00:06:11,130 --> 00:06:15,440
So the idea of this
approach is to view each

78
00:06:15,440 --> 00:06:20,140
word in our vocabulary as defining one
dimension in a high dimensional space.

79
00:06:20,140 --> 00:06:23,615
So we have N words in
total in the vocabulary,

80
00:06:23,615 --> 00:06:27,462
then we have N dimensions,
as illustrated here.

81
00:06:27,462 --> 00:06:34,311
And on the bottom, you can see a frequency
vector representing a context,

82
00:06:34,311 --> 00:06:39,855
and here we see where eats
occurred 5 times in this context,

83
00:06:39,855 --> 00:06:43,140
ate occurred 3 times, et cetera.

84
00:06:43,140 --> 00:06:48,003
So this vector can then be placed
in this vector space model.

85
00:06:48,003 --> 00:06:53,347
So in general,
we can represent a pseudo document or

86
00:06:53,347 --> 00:06:58,933
context of cat as one vector,
d1, and another word,

87
00:06:58,933 --> 00:07:04,045
dog, might give us a different context,
so d2.

88
00:07:04,045 --> 00:07:07,880
And then we can measure
the similarity of these two vectors.

89
00:07:07,880 --> 00:07:10,980
So by viewing context in
the vector space model,

90
00:07:10,980 --> 00:07:15,100
we convert the problem of
paradigmatical relation discovery

91
00:07:15,100 --> 00:07:18,820
into the problem of computing
the vectors and their similarity.

92
00:07:20,300 --> 00:07:24,170
So the two questions that we
have to address are first,

93
00:07:24,170 --> 00:07:28,750
how to compute each vector, and
that is how to compute xi or yi.

94
00:07:31,050 --> 00:07:33,579
And the other question is how
do you compute the similarity.

95
00:07:35,580 --> 00:07:40,515
Now in general, there are many approaches
that can be used to solve the problem, and

96
00:07:40,515 --> 00:07:43,795
most of them are developed for
information retrieval.

97
00:07:43,795 --> 00:07:47,821
And they have been shown to work well for

98
00:07:47,821 --> 00:07:52,712
matching a query vector and
a document vector.

99
00:07:52,712 --> 00:07:57,555
But we can adapt many of
the ideas to compute a similarity

100
00:07:57,555 --> 00:08:01,378
of context documents for our purpose here.

101
00:08:01,378 --> 00:08:05,829
So let's first look at
the one plausible approach,

102
00:08:05,829 --> 00:08:10,481
where we try to match
the similarity of context based on

103
00:08:10,481 --> 00:08:15,150
the expected overlap of words,
and we call this EOWC.

104
00:08:17,020 --> 00:08:22,495
So the idea here is to represent
a context by a word vector

105
00:08:22,495 --> 00:08:28,438
where each word has a weight
that's equal to the probability

106
00:08:28,438 --> 00:08:35,336
that a randomly picked word from
this document vector, is this word.

107
00:08:35,336 --> 00:08:39,956
So in other words,
xi is defined as the normalized

108
00:08:39,956 --> 00:08:43,476
account of word wi in the context, and

109
00:08:43,476 --> 00:08:48,756
this can be interpreted as
the probability that you would

110
00:08:48,756 --> 00:08:54,600
actually pick this word from d1
if you randomly picked a word.

111
00:08:56,760 --> 00:09:01,620
Now, of course these xi's would sum to one
because they are normalized frequencies,

112
00:09:02,930 --> 00:09:05,750
and this means the vector is

113
00:09:05,750 --> 00:09:08,193
actually probability of
the distribution over words.

114
00:09:10,500 --> 00:09:15,883
So, the vector d2 can be also
computed in the same way, and

115
00:09:15,883 --> 00:09:23,540
this would give us then two probability
distributions representing two contexts.

116
00:09:24,840 --> 00:09:28,220
So, that addresses the problem
how to compute the vectors, and

117
00:09:28,220 --> 00:09:31,760
next let's see how we can define
similarity in this approach.

118
00:09:31,760 --> 00:09:35,668
Well, here, we simply define
the similarity as a dot product of two

119
00:09:35,668 --> 00:09:39,890
vectors, and
this is defined as a sum of the products

120
00:09:41,410 --> 00:09:43,960
of the corresponding
elements of the two vectors.

121
00:09:46,630 --> 00:09:51,847
Now, it's interesting to see
that this similarity function

122
00:09:51,847 --> 00:09:57,360
actually has a nice interpretation,
and that is this.

123
00:09:57,360 --> 00:10:02,548
Dot product, in fact that gives
us the probability that two

124
00:10:02,548 --> 00:10:08,570
randomly picked words from
the two contexts are identical.

125
00:10:08,570 --> 00:10:12,630
That means if we try to pick a word
from one context and try to pick another

126
00:10:12,630 --> 00:10:17,860
word from another context, we can then
ask the question, are they identical?

127
00:10:17,860 --> 00:10:22,650
If the two contexts are very similar,
then we should expect we frequently will

128
00:10:22,650 --> 00:10:27,390
see the two words picked from
the two contexts are identical.

129
00:10:27,390 --> 00:10:30,900
If they are very different,
then the chance of seeing

130
00:10:30,900 --> 00:10:34,890
identical words being picked from
the two contexts would be small.

131
00:10:34,890 --> 00:10:39,865
So this intuitively makes sense, right,
for measuring similarity of contexts.

132
00:10:41,490 --> 00:10:46,819
Now you might want to also take
a look at the exact formulas and

133
00:10:46,819 --> 00:10:51,627
see why this can be interpreted
as the probability that

134
00:10:51,627 --> 00:10:55,410
two randomly picked words are identical.

135
00:10:57,440 --> 00:11:04,550
So if you just stare at the formula
to check what's inside this sum,

136
00:11:04,550 --> 00:11:12,034
then you will see basically in each
case it gives us the probability that

137
00:11:12,034 --> 00:11:17,170
we will see an overlap on
a particular word, wi.

138
00:11:17,170 --> 00:11:23,661
And where xi gives us a probability that
we will pick this particular word from d1,

139
00:11:23,661 --> 00:11:28,503
and yi gives us the probability
of picking this word from d2.

140
00:11:28,503 --> 00:11:32,024
And when we pick the same
word from the two contexts,

141
00:11:32,024 --> 00:11:34,920
then we have an identical pick, right so.

142
00:11:34,920 --> 00:11:42,380
That's one possible approach, EOWC,
extracted overlap of words in context.

143
00:11:42,380 --> 00:11:49,440
Now as always, we would like to assess
whether this approach it would work well.

144
00:11:49,440 --> 00:11:52,880
Now of course, ultimately we have to
test the approach with real data and

145
00:11:52,880 --> 00:11:56,259
see if it gives us really
semantically related words.

146
00:11:57,730 --> 00:12:01,010
Really give us paradigmatical relations,
but

147
00:12:01,010 --> 00:12:05,380
analytically we can also analyze
this formula a little bit.

148
00:12:05,380 --> 00:12:11,020
So first, as I said,
it does make sense, right, because this

149
00:12:11,020 --> 00:12:15,802
formula will give a higher score if there
is more overlap between the two contexts.

150
00:12:15,802 --> 00:12:17,988
So that's exactly what we want.

151
00:12:17,988 --> 00:12:21,170
But if you analyze
the formula more carefully,

152
00:12:21,170 --> 00:12:24,286
then you also see there might
be some potential problems,

153
00:12:24,286 --> 00:12:27,735
and specifically there
are two potential problems.

154
00:12:27,735 --> 00:12:33,935
First, it might favor matching
one frequent term very well,

155
00:12:33,935 --> 00:12:35,795
over matching more distinct terms.

156
00:12:36,825 --> 00:12:44,300
And that is because in the dot product,
if one element has a high value and this

157
00:12:44,300 --> 00:12:50,190
element is shared by both contexts and
it contributes a lot to the overall sum,

158
00:12:51,250 --> 00:12:55,710
it might indeed make the score
higher than in another case,

159
00:12:55,710 --> 00:13:01,150
where the two vectors actually have
a lot of overlap in different terms.

160
00:13:01,150 --> 00:13:06,878
But each term has a relatively low
frequency, so this may not be desirable.

161
00:13:06,878 --> 00:13:09,586
Of course, this might be
desirable in some other cases.

162
00:13:09,586 --> 00:13:14,527
But in our case, we should intuitively
prefer a case where we match

163
00:13:14,527 --> 00:13:19,645
more different terms in the context,
so that we have more confidence

164
00:13:19,645 --> 00:13:24,253
in saying that the two words
indeed occur in similar context.

165
00:13:24,253 --> 00:13:27,020
If you only rely on one term and

166
00:13:27,020 --> 00:13:32,465
that's a little bit questionable,
it may not be robust.

167
00:13:34,675 --> 00:13:38,795
Now the second problem is that it
treats every word equally, right.

168
00:13:38,795 --> 00:13:42,131
So if you match a word like the and

169
00:13:42,131 --> 00:13:47,443
it will be the same as
matching a word like eats, but

170
00:13:47,443 --> 00:13:52,388
intuitively we know
matching the isn't really

171
00:13:52,388 --> 00:13:57,816
surprising because the occurs everywhere.

172
00:13:57,816 --> 00:14:02,787
So matching the is not as such
strong evidence as matching what

173
00:14:02,787 --> 00:14:07,956
a word like eats,
which doesn't occur frequently.

174
00:14:07,956 --> 00:14:11,216
So this is another
problem of this approach.

175
00:14:13,426 --> 00:14:19,003
In the next chapter we are going to talk
about how to address these problems.

176
00:14:19,003 --> 00:14:29,003
[MUSIC]


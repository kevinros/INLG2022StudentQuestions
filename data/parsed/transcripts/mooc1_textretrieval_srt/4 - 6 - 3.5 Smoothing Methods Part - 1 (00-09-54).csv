name,id,from,to,text
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,1,0.0,5.545,[SOUND] 
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,2,7.837,9.888,This lecture is about the specific 
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,3,9.888,14.71,smoothing methods for language models used in Probabilistic Retrieval Model. 
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,4,16.56,21.1,In this lecture we will continue the discussion of language models for 
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,5,21.1,26.01,_information retrieval, particularly the query likelihood retrieval method. _
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,6,26.01,29.91,And we're going to talk about the specific smoothing methods used for 
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,7,29.91,31.04,such a retrieval function. 
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,8,33.93,37.99,_So, this is a slide from a previous lecture where we show that with _
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,9,37.99,42.42,query likelihood ranking and the smoothing with the collection language model. 
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,10,42.42,48.841,We end up having a retrieval function that looks like the following. 
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,11,49.98,53.51,_So, this is the retrieval function, _
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,12,53.51,57.395,based on these assumptions that we have discussed. 
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,13,57.395,60.7,You can see it's a sum of all the matched query terms here. 
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,14,60.7,66.79,_And inside the sum it's a count of term in the query, _
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,15,66.79,71.218,and some weight for the term in the document. 
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,16,71.218,73.92,_We have TFI, TF weight here. _
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,17,73.92,78.15,_And then we have another constant here, in n. _
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,18,80.3,85.32,_So clearly, if we want to implement this function using a programming language, _
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,19,85.32,87.65,we'll still need to figure out a few variables. 
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,20,87.65,92.8,_In particular, we're going to need to know how to estimate the, _
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,21,92.8,96.69,probability of would exactly. 
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,22,96.69,99.0,And how do we set alpha? 
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,23,100.27000000000001,104.4,_So in order to answer these questions, we have to think about this very specific _
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,24,104.4,107.75999999999999,_smoothing methods, and that is the main topic of this lecture. _
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,25,108.9,110.78,We're going to talk about two smoothing methods. 
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,26,110.78,117.42,_The first is the simple linear interpolation, with a fixed coefficient. _
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,27,117.42,119.91,And this is also called a Jelinek and Mercer smoothing. 
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,28,121.17,124.09,So the idea is actually very simple. 
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,29,124.09,126.33,This picture shows how we estimate 
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,30,128.27,132.44,_document language model by using maximum [INAUDIBLE] method, _
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,31,132.44,137.97,that gives us word counts normalized by the total number of words in the text. 
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,32,137.97,142.7,The idea of using this method is to 
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,33,142.7,146.48,maximize the probability of the observed text. 
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,34,146.48,153.09,_As a result, if a word like network, is not observed in the text. _
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,35,153.09,156.22,_It's going to get zero probability, as shown here. _
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,36,157.81,162.6,_So the idea of smoothing, then, is to rely on collection average model, _
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,37,162.6,167.45,where this word is not going to have a zero probability to help us decide 
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,38,167.45,170.85,what non-zero probability should be assigned to such a word. 
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,39,170.85,175.56,_So, we can know that network as a non-zero probability here. _
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,40,175.56,181.2,_So, in this approach what we do is, we do a linear interpolation between _
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,41,181.2,185.0,the maximum likelihood or estimate here and the collection language model. 
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,42,185.0,187.86,_And this controlled by the smoothing parameter, lambda. _
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,43,187.86,192.82,Which is between 0 and 1. 
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,44,192.82,194.968,So this is a smoothing parameter. 
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,45,194.968,199.92000000000002,_The larger lambda is the two the more smoothing we have, we will have. _
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,46,201.03,207.2,_So by mixing them together, we achieve the goal of assigning non-zero probability. _
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,47,207.2,209.06,And these two are word in our network. 
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,48,209.06,211.41,So let's see how it works for some of the words here. 
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,49,212.44,216.8,For example if we compute to the smallest probability for text. 
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,50,217.96,221.61,_Now, the next one right here is made give us 10 over 100, _
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,51,221.61,223.21,and that's going to be here. 
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,52,224.32,227.35399999999998,_But the connection probability is this, so _
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,53,227.35399999999998,231.29500000000002,we just combine them together with this simple formula. 
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,54,233.42000000000002,238.095,_We can also see a, the word network. _
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,55,238.095,242.785,Which used to have zero probability now is getting a non-zero 
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,56,242.785,245.305,probability of this value. 
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,57,245.305,252.132,_And that's because the count is going to be zero for network here, but _
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,58,252.132,258.282,this part is non zero and that's basically how this method works. 
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,59,259.412,266.15,If you think about this and you can easily see now the alpha sub d 
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,60,266.15,271.11,_in this smoothing method is basically lambda because that's, remember, _
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,61,271.11,276.57,the coefficient in front of the probability of the word given by 
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,62,276.57,279.0,_the collection language model here, right? _
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,63,280.44,282.109,_Okay, so this is the first smoothing method. _
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,64,283.4,289.88,_The second one is similar, but it has a find end for manual interpretation. _
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,65,289.88,294.54,It's often called a duration of the ply or Bayesian smoothing. 
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,66,294.54,300.955,_So again here, we face the problem of zero probability for like network. _
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,67,300.955,305.745,_Again we'll use the collection language model, but _
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,68,305.745,308.96,in this case we're going to combine them in a somewhat different ways. 
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,69,308.96,315.751,The formula first can be seen as a interpolation of the maximum 
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,70,315.751,320.72,and the collection language model as before. 
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,71,320.72,323.58,As in the J M's [INAUDIBLE]. 
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,72,323.58,328.47,_Only and after the coefficient [INAUDIBLE] is not the lambda, a fixed lambda, but _
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,73,328.47,331.43,_a dynamic coefficient in this form, _
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,74,331.43,336.76,_when mu is a parameter, it's a non, negative value. _
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,75,336.76,340.55,_And you can see if we set mu to a constant, _
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,76,340.55,344.63,the effect is that a long document would actually get smaller coefficient here. 
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,77,346.07,347.9,Right? Because a long document 
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,78,347.9,349.93,we have a longer length. 
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,79,349.93,353.14,_Therefore, the coefficient is actually smaller. _
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,80,353.14,357.31,And so a long document would have less smoothing as we would expect. 
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,81,359.64,365.78,So this seems to make more sense than a fixed coefficient smoothing. 
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,82,365.78,370.18,_Of course, this part would be of this form, so _
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,83,370.18,372.08,that the two coefficients would sum to 1. 
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,84,372.08,376.4,_Now, this is one way to understand that this is smoothing. _
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,85,376.4,381.08,_Basically, it means that it's a dynamic coefficient interpolation. _
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,86,382.79,385.31,There is another way to understand this formula. 
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,87,387.09,391.62,Which is even easier to remember and that's this side. 
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,88,393.31,398.44,So it's easy to see we can rewrite this modern method in this form. 
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,89,398.44,403.73,_Now, in this form, we can easily see what change we have made to the maximum _
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,90,403.73,405.94,_estimator, which would be this part, right? _
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,91,407.31,410.1,So it normalizes the count by the top elements. 
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,92,412.3,416.07,_So, in this form, we can see what we did, _
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,93,416.07,420.75,is we add this to the count of every word. 
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,94,421.8,423.21,_So, what does this mean? _
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,95,423.21,424.95,_Well, this is basically _
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,96,426.02,430.39,something relative to the probability of the word in the collection.. 
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,97,430.39,433.26,And we multiply that by the parameter mu. 
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,98,434.51,438.21,_And when we combine this with the count here, _
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,99,438.21,443.806,essentially we are adding pseudo counts to the observed text. 
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,100,443.806,450.07,_We pretend every word, has got this many pseudocount. _
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,101,451.09,455.29,So the total count would be the sum of these pseudocount and 
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,102,455.29,458.73,the actual count of the word in the document. 
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,103,459.95,466.05,_As a result, in total, we would have added this minute pseudocount. _
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,104,466.05,468.99,_Why? Because if you take a sum of this, _
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,105,468.99,472.24,_this one, move over all the words and _
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,106,472.24,477.38,_we'll see the probability of the words would sum to 1, and that gives us just mu. _
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,107,477.38,480.2,So this is the total number of pseudo counters that we added. 
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,108,481.54,485.26,_And, and so these probabilities would still sum to 1. _
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,109,485.26,492.51,_So in this case, we can easily see the method is essentially to _
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,110,493.92,498.13,add these as a pseudocount to this data. 
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,111,498.13,501.32,Pretend we actually augment the data 
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,112,501.32,506.48,by including by some pseudo data defined by the collection language model. 
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,113,506.48,508.33,_As a result, we have more counts. _
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,114,509.47,515.71,_It's the, the total counts for, for word, a word that would be like this. _
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,115,515.71,519.88,_And, as a result, even if a word has zero counts here. _
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,116,519.88,523.58,_And say if we have zero come here and that it would still have none, _
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,117,523.58,527.3,_zero count because of this part, right? _
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,118,527.3,530.04,And so this is how this method works. 
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,119,530.04,532.56,Let's also take a look at this specific example here. 
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,120,532.56,538.58,_All right, so for text again, we will have 10 as original count. _
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,121,538.58,541.635,That we actually observe but we also added some pseudocount. 
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,122,543.0,545.94,_And so, the probability of text would be of this form. _
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,123,545.94,550.1,Naturally the probability of network would be just this part. 
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,124,551.3,554.41,_And so, here you can also see what's alpha sub d here. _
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,125,555.6,556.85,Can you see it? 
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,126,556.85,559.05,If you want to think about you can pause the video. 
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,127,560.61,565.83,Have you noticed that this part is basically of a sub t? 
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,128,565.83,572.64,_So we can see this case of our sub t does depend on the document, right? _
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,129,572.64,579.211,Because this lens depends on the document whereas in the linear interpolation. 
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,130,579.211,582.545,The James move method this is the constant. 
4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt,131,582.545,592.545,[MUSIC] 

name,id,from,to,text
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,1,0.025,6.458,[SOUND] This lecture is about the feedback 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,2,6.458,12.33,in the language modeling approach. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,3,12.33,17.52,In this lecture we will continue the discussion of feedback in text retrieval. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,4,17.52,20.8,In particular we're going to talk about the feedback in language modeling 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,5,20.8,21.36,approaches. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,6,23.45,29.28,So we derive the query likelihood ranking function by making various assumptions. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,7,30.41,35.86,_As a basic retrieval function, that formula, or those formulas worked well. _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,8,35.86,39.91,_But if we think about the feedback information, it's a little bit awkward to _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,9,39.91,44.73,use query likelihood to perform feedback because 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,10,44.73,49.62,a lot of times the feedback information is additional information about the query. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,11,49.62,53.26,But we assume the query is generated by assembling words 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,12,53.26,56.85,from a language model in the query likelihood method. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,13,56.85,63.17,_It's kind of unnatural to sample, words that, form feedback documents. _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,14,63.17,66.46,_As a result, then research is proposed, _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,15,66.46,70.36,a way to generalize query likelihood function. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,16,70.36,74.06,It's called a Kullback-Leibler divergence retrieval model. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,17,75.45,80.96000000000001,_And this model is actually, going to make the query likelihood, _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,18,80.96000000000001,85.78,our retrieval function much closer to vector space model. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,19,85.78,92.66,_Yet this, form of the language model can be, regarded as a generalization of query _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,20,92.66,96.6,likelihood in the sense that if it can cover query likelihood as a special case. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,21,98.18,101.82,And in this case the feedback can be achieved through 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,22,101.82,104.18,simply query model estimation or updating. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,23,104.18,108.12,This is very similar to Rocchio which updates the query vector. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,24,110.0,115.02199999999999,_So let's see what the, is the scale of divergence, which we will model. _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,25,115.02199999999999,119.529,_So, on the top, what you see is query _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,26,119.529,125.087,_likelihood retrieval function, all right, this one. _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,27,125.087,131.417,And then KL-divergence or also called cross entropy retrieval 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,28,131.417,136.808,_model is basically to generalize the frequency part, _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,29,136.808,141.15,_here, into a layered model. _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,30,141.15,144.975,_So basically it's the difference, _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,31,144.975,149.91,given by the probabilistic model here 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,32,149.91,154.64,to characterize what the user's looking for versus the kind of query words there. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,33,155.81,162.61,And this difference allows us to plotting various different ways to estimate this. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,34,162.61,168.27,So this can be estimated in many different ways including using feedback information. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,35,168.27,171.41,Now this is called a KL-divergence because 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,36,171.41,176.231,this can be interpreted as measuring the KL-divergence of two distributions. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,37,176.231,182.935,One is the query model denoted by this distribution. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,38,182.935,187.38,_One is the talking, the language model here. _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,39,187.38,191.605,_And [INAUDIBLE] though is a [INAUDIBLE] language model, of course. _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,40,191.605,195.647,_And we are not going to talk about the detail of that, and _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,41,195.647,198.361,you'll find the things in references. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,42,198.361,202.769,_It's also called cross entropy, because, in, in fact, _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,43,202.769,207.407,we can ignore some terms in the KL-divergence function and we will end up 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,44,207.407,212.75,_having actually cross entropy, and that, both are terms in information theory. _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,45,214.41,215.32999999999998,_But, anyway for _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,46,216.97,222.82,_our purposes here you can just see the two formulas look almost identical, _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,47,222.82,228.22,except that here we have a probability of a word given by a query language model. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,48,229.44,232.59,_This, and here, _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,49,232.59,237.48,_the sum is over all the words that are in the document, _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,50,237.48,242.34,and also with the non-zero probability for the query model. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,51,242.34,247.51,_So it's kind of, again, a generalization of sum over all the matching query words. _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,52,249.94,254.47,_Now you can also, easy to see, we can recover the query likelihood, _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,53,254.47,258.45,which we will find here by as simple as setting this query model to 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,54,258.45,263.44,_the relative frequency of a word in the query, right? _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,55,263.44,265.95,This is very to easy see once you practice this. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,56,265.95,269.94,_And to here, you can eliminate this query lens, that's a constant, _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,57,269.94,272.40999999999997,and then you get exactly like that. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,58,273.83,276.2,So you can see the equivalence. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,59,276.2,281.84000000000003,And that's also why this KL-divergence model can be regarded as a generalization 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,60,281.84000000000003,287.48,_of query likelihood because we can cover query likelihood as a special case, _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,61,287.48,289.469,but it would also allow it to do much more than that. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,62,290.76,296.35,So this is how we use the KL-divergence model to then do feedback. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,63,296.35,300.48,_The picture shows that we first estimate a document language model, _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,64,300.48,302.84,then we estimate a query language model and 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,65,302.84,307.06,_we compute the KL-divergence, this is often denoted by a D here. _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,66,309.54,314.29,_But this basically means, this was exactly like in vector space _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,67,314.29,318.28,model because we compute the vector for the document in the computer and 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,68,318.28,322.44,_not the vector for the query, and then we compute the distance. _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,69,322.44,325.05,_Only that these vectors are of special forms, _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,70,325.05,326.58,they have probability distributions. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,71,327.95,331.68,_And then we get the results, and we can find some feedback documents. _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,72,331.68,337.42,_Let's assume they are more selective sorry, mostly positive documents. _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,73,337.42,340.4,Although we could also consider both kinds of documents. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,74,340.4,342.881,_So what we could do is, like in Rocchio, _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,75,342.881,348.57,we can compute another language model called feedback language model here. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,76,348.57,352.992,_Again, this is going to be another vector just like a computing centroid vector in _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,77,352.992,353.592,Rocchio. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,78,353.592,356.472,And then this model can be combined with the original 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,79,356.472,358.899,query model using a linear interpolation. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,80,360.49,364.63,_And this would then give us an updated model, just like again in Rocchio. _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,81,365.81,370.676,_Right, so here, we can see the parameter of our controlling amount of feedback if _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,82,370.676,374.075,_it's set to 0, then it says here there's no feedback. _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,83,374.075,379.05,_After set to 1, we've got full feedback, we can ignore the original query. _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,84,379.05,381.82,_And this is generally not desirable, right. _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,85,381.82,387.64,So this unless you are absolutely sure you have seen a lot of relevant documents and 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,86,387.64,389.29,the query terms are not important. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,87,391.18,394.87,So of course the main question here is how do you compute this theta F? 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,88,394.87,396.79,This is the big question here. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,89,396.79,399.34000000000003,_And once you can do that, the rest is easy. _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,90,399.34000000000003,401.64,So here we'll talk about one of the approaches. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,91,401.64,403.27,And there are many approaches of course. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,92,403.27,408.05,This approach is based on generative model and I'm going to show you how it works. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,93,408.05,410.55,This is a user generative mixture model. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,94,410.55,415.03,_So this picture shows that the we have this model here, _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,95,415.03,418.08,the feedback model that we want to estimate. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,96,418.08,420.49,And we the basis is the feedback options. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,97,420.49,424.11,Let's say we are observing the positive documents. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,98,424.11,428.54,_These are the collected documents by users, or random documents judged by _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,99,428.54,432.67,_users, or simply top ranked documents that we assumed to be random. _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,100,434.71,437.36,Now imagine how we can compute a centroid for 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,101,437.36,440.63,these documents by using language model. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,102,440.63,443.33,One approach is simply to assume 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,103,443.33,448.21,these documents are generated from this language model as we did before. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,104,448.21,452.12,_What we could do is do it, just normalize the word frequency here. _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,105,452.12,454.94,_And then we, we'll get this word distribution. _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,106,456.21,459.87,Now the question is whether this distribution is good for feedback. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,107,459.87,464.61,Well you can imagine well the top rank of the words would be what? 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,108,465.7,466.2,What do you think? 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,109,468.28,471.241,_Well those words would be common words, right? _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,110,471.241,473.68,_As well we see in, in the language model, _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,111,473.68,477.85,_in the top right, the words are actually common words like, the, et cetera. _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,112,477.85,482.61,_So, it's not very good for feedback, because we will be adding a lot of such _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,113,482.61,487.35,_words to our query when we interpret, this was the original query model. _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,114,488.87,493.69,_So, this is not good, so we need to do something, in particular, _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,115,493.69,497.33,we are trying to get rid of those common words. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,116,497.33,501.23,_And we all, we have seen actually one way to do that, by using background language _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,117,501.23,507.47,_model in the case of learning the associations with of words, right. _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,118,507.47,509.61,The words that are related to the word computer. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,119,510.83,513.62,_We could do that, and that would be another way to do this. _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,120,513.62,516.29,_But here, we're going to talk about another approach, _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,121,516.29,519.16,which is a more principled approach. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,122,519.16,523.47,_In this case, we're going to say, well, you, you said that there are common words _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,123,523.47,529.3,_here in this, these documents that should not belong to this top model, right? _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,124,530.31,534.99,_So now, what we can do is to assume that, well, those words are, generally, _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,125,534.99,538.02,_from background language model, so they will generate a, _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,126,538.02,541.26,_those words like the, for example. _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,127,542.43,545.079,_And if we use maximum likelihood estimated, _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,128,545.079,551.3,_note that if all the words here must be generated from this model, _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,129,551.3,556.83,_then this model is forced to assign high probabilities to a word like the, _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,130,556.83,559.7,because it occurs so frequently here. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,131,559.7,564.8,_Note that in order to reduce its probability in this model, we have to _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,132,564.8,571.28,_have another model, which is this one to help explain the word, the, here. _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,133,571.28,572.86,_And in this case, _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,134,572.86,577.55,it's not appropriate to use the background language model to achieve this goal 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,135,577.55,582.31,because this model will assign high probabilities to these common words. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,136,583.37,586.74,_So in this approach then, we assume _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,137,586.74,590.81,this machine that which generated these words would work as follows. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,138,590.81,593.62,We have a source controller here. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,139,593.62,599.1,Imagine we flip a coin here to decide what distribution to use. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,140,599.1,602.25,With the probability of lambda the coin shows up as head. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,141,602.25,605.39,And then we're going to use the background language model. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,142,605.39,608.54,And we can do then sample word from that model. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,143,608.54,611.522,_With probability of 1 minus lambda now, _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,144,611.522,617.44,we now decide to use a unknown topic model here that we will try to estimate. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,145,617.44,620.1,And we're going to then generate a word here. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,146,620.1,625.06,_If we make this assumption, and this whole thing will be just one model, and _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,147,625.06,627.18,_we call this a mixture model, _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,148,627.18,630.4,because there are two distributions that are mixed here together. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,149,630.4,633.94,And we actually don't know when each distribution is used. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,150,635.15,640.05,_Right, so again think of this whole thing as one model. _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,151,640.05,642.904,_And we can still ask it for words, and _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,152,642.904,647.91,_it will still give us a word in a random method, right? _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,153,647.91,652.0,And of course which word will show up will depend on both this distribution and 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,154,652.0,653.295,that distribution. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,155,653.295,656.2,_In addition, it would also depend on this lambda, _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,156,656.2,658.97,_because if you say, lambda is very high and _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,157,658.97,662.92,_it's going to always use the background distribution, you'll get different words. _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,158,662.92,667.244,_If you say, well our lambda is very small, we're going to use this, all right? _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,159,667.244,672.562,_So all these are parameters, in this model. _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,160,672.562,675.189,_And then, if you're thinking this way, _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,161,675.189,680.068,_basically we can do exactly the same as what we did before, we're going to use _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,162,680.068,685.76,maximum likelihood estimator to adjust this model to estimate the parameters. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,163,685.76,690.58,_Basically we're going to adjust, well, this parameter so _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,164,690.58,692.9,that we can best explain all the data. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,165,692.9,701.2,The difference now is that we are not asking this model alone to explain this. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,166,701.2,705.54,_But rather we're going to ask this whole model, mixture model, _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,167,705.54,710.53,to explain the data because it has got some help from the background model. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,168,710.53,714.08,_It doesn't have to assign high probabilities towards like the, _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,169,714.08,715.15,as a result. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,170,715.15,721.136,It would then assign high probabilities to other words that are common here but 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,171,721.136,724.95,not having high probability here. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,172,724.95,727.063,So those would be common here. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,173,730.093,731.393,Right? 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,174,731.393,735.069,_And if they're common they would have to have high probabilities, _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,175,735.069,737.658,according to a maximum likelihood estimator. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,176,737.658,743.512,_And if they are rare here, all right, so if they are rare here, _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,177,743.512,749.62,then you don't get much help from this background model. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,178,749.62,753.94,_As a result, this topic model must assign high probabilities. _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,179,753.94,757.37,So the higher probability words according to the topic model 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,180,757.37,761.71,_will be those that are common here, but rare in the background. _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,181,763.37,769.52,_Okay, so, this is basically a little bit like a idea for weighting here. _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,182,769.52,773.8,This would allow us to achieve the effect of removing these top words 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,183,773.8,776.79,that are meaningless in the feedback. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,184,776.79,781.553,_So mathematically what we have is to compute the likelihood again, _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,185,781.553,785.15,local likelihood of the feedback documents. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,186,785.15,788.72,_And, and note that, we also have another parameter, lambda here. _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,187,788.72,793.15,But we assume that lambda denotes noise in the feedback document. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,188,793.15,796.51,_So we are going to, let's say, set this to a parameter, let's say, _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,189,796.51,801.8,_say 50% of the words are noise, or 90% are noise. _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,190,801.8,804.6,_And this can then be, assume it will be fixed. _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,191,804.6,811.081,_If we assume this is fixed, then we only have these probabilities as parameters _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,192,811.081,817.12,_just like in the simplest unigram language model, we have n parameters. _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,193,817.12,821.289,_n is the number of words and, then, the likelihood function will look like this. _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,194,822.76,826.61,_It's very similar to the likelihood function, normal likelihood _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,195,826.61,832.1,function we see before except that inside the logarithm there's a sum in here. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,196,832.1,837.07,And this sum is because we can see the two distributions. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,197,837.07,841.3,And which ones used would depend on lambda and that's why we have this form. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,198,842.46,848.79,_But mathematically this is the function with theta as unknown variables, right? _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,199,848.79,850.51,_So, this is just a function. _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,200,850.51,853.91,_All the other variables are known, except for this guy. _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,201,855.0,859.658,_So, we can then choose this probability distribution to _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,202,859.658,862.287,maximize this log likelihood. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,203,862.287,865.03,The same idea as the maximum likelihood estimator. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,204,865.03,867.304,_As a mathematical problem which is to, _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,205,867.304,870.06,we just have to solve this optimization problem. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,206,870.06,873.048,_We said we would try all of the theta values, and _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,207,873.048,877.471,here we find one that gives this whole thing the maximum probability. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,208,877.471,880.682,_So, it's a well-defined math problem. _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,209,880.682,883.49,_Once we have done that, we obtain this theta F, _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,210,883.49,887.86,that can be the interpreter with the original query model to do feedback. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,211,890.99,896.02,So here are some examples of the feedback model learned from a web 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,212,896.02,899.51,_document collection, and we do pseudo-feedback. _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,213,899.51,903.76,_We just use the top 10 documents, and we use this mixture model. _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,214,903.76,906.09,So the query is airport security. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,215,906.09,911.36,What we do is we first retrieve ten documents from the web database. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,216,911.36,913.929,_And this is of course pseudo-feedback, right? _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,217,913.929,920.872,_And then we're going to feed to that mixture model, to this ten document set. _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,218,920.872,925.5,And these are the words learned using this approach. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,219,925.5,930.22,This is the probability of a word given by the feedback model in both cases. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,220,931.6,935.7,_So, in both cases, you can see the highest probability of words _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,221,935.7,938.48,include very random words to the query. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,222,938.48,940.2,_So, airport security for example, _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,223,940.2,943.74,these query words still show up as high probabilities 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,224,943.74,948.85,in each case naturally because they occur frequently in the top rank of documents. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,225,948.85,953.5,_But we also see beverage, alcohol, bomb, terrorist, et cetera. _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,226,953.5,958.92,_Right, so these are relevant to this topic, and they, _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,227,958.92,965.28,_if combined with original query can help us match more accurately, on documents. _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,228,965.28,970.7,_And also they can help us bring up documents that only managing the, _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,229,970.7,972.71,some of these other words. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,230,972.71,977.78,And maybe for example just airport and then bomb for example. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,231,977.78,980.68,_These so, this is how pseudo-feedback works. _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,232,980.68,982.93,It shows that this model really works and 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,233,982.93,986.79,_picks up mm, some related words to the query. _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,234,986.79,991.32,_What's also interesting is that if you look at the two tables here, and _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,235,991.32,996.05,_you compare them, and you see in this case, when lambda is set to a small value, _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,236,996.05,1001.773,_and we'll still see some common words here, and that means. _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,237,1001.773,1005.732,_When we don't use the background model often, remember lambda can _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,238,1005.732,1010.955,use the probability of using the background model to generate to the text. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,239,1010.955,1013.245,_If we don't rely much on background model, _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,240,1013.245,1018.1,we still have to use this topped model to account for the common words. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,241,1018.1,1023.062,Whereas if we set lambda to a very high value we would use the background 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,242,1023.062,1026.98,_model very often to explain these words, then there is no burden on _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,243,1026.98,1031.8,expanding those common words in the feedback documents by the topping model. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,244,1031.8,1037.4,_So, as a result, the top of the model here is very discriminative. _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,245,1037.4,1040.07,It contains all the relevant words without common words. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,246,1041.2,1046.1,So this can be added to the original query to achieve feedback. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,247,1048.18,1052.3,So to summarize in this lecture we have talked about the feedback in 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,248,1052.3,1054.39,language model approach. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,249,1054.39,1058.29,_In general, feedback is to learn from examples. _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,250,1058.29,1062.914,_These examples can be assumed examples, can be pseudo-examples, _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,251,1062.914,1068.268,_like assume the, the top ten documents are assumed to be random. _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,252,1068.268,1072.277,_They could be based on using fractions like feedback, _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,253,1072.277,1075.26,based on quick sorts or implicit feedback. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,254,1075.26,1078.878,_We talked about the three major feedback scenarios, relevance feedback, _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,255,1078.878,1082.01,_pseudo-feedback, and implicit feedback. _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,256,1082.01,1087.62,We talked about how to use Rocchio to do feedback in vector-space model and 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,257,1087.62,1094.26,how to use query model estimation for feedback in language model. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,258,1094.26,1097.73,And we briefly talked about the mixture model and 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,259,1097.73,1101.65,the basic idea and there are many other methods. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,260,1101.65,1105.08,For example the relevance model is a very effective model for 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,261,1105.08,1106.96,estimating query model. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,262,1106.96,1109.1,_So, you can read more about the, _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,263,1109.1,1114.69,these methods in the references that are listed at the end of this lecture. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,264,1116.18,1118.4,So there are two additional readings here. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,265,1118.4,1122.92,_The first one is a book that has a systematic, review and _
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,266,1122.92,1125.055,discussion of language models of more information retrieval. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,267,1126.56,1131.728,And the second one is an important research paper that's about relevance 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,268,1131.728,1136.989,based language models and it's a very effective way of computing query model. 
4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt,269,1136.989,1146.989,[MUSIC] 

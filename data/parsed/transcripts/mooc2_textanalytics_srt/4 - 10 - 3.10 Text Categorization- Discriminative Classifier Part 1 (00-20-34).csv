name,id,from,to,text
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,1,0.012,7.093,[SOUND] This 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,2,7.093,11.83,lecture is about the discriminative classifiers for text categorization. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,3,13.0,15.84,In this lecture we're going to continue talking about how to 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,4,15.84,20.22,do text categorization and cover discriminative approaches. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,5,20.22,24.76,_This is a slide that you have seen from the discussion of Naive Bayes Classifier, _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,6,24.76,29.12,where we have shown that although Naive Bayes Classifier tries to model 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,7,29.12,34.09,_the generation of text data, from each categories, we can actually use Bayes' _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,8,34.09,40.9,rule to eventually rewrite the scoring function as you see on this slide. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,9,40.9,45.52,And this scoring function is basically a weighted combination of a lot 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,10,45.52,50.53,_of word features, where the feature values are word counts, and the feature weights _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,11,50.53,55.72,are the log of probability ratios of the word given by two distributions here. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,12,57.28,62.67,Now this kind of scoring function can be actually a general scoring 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,13,62.67,68.57,function where we can in general present text data as a feature vector. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,14,68.57,72.34,Of course the features don't have to be all the words. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,15,72.34,76.28,Their features can be other signals that we want to use. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,16,76.28,82.88,And we mentioned that this is precisely similar to logistic regression. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,17,82.88,87.57,_So, in this lecture we're going to introduce some discriminative classifiers. _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,18,87.57,91.45,They try to model the conditional distribution of 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,19,91.45,96.99000000000001,labels given the data directly rather than using Bayes' rule 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,20,96.99000000000001,101.55,to compute that interactively as we have seen in naive Bayes. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,21,101.55,107.15,So the general idea of logistic regression is to model 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,22,107.15,112.35,the dependency of a binary response variable Y 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,23,112.35,116.36,on some predictors that are denoted as X. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,24,116.36,121.72,So here we have also changed the notation 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,25,121.72,126.12,to X for future values. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,26,127.14,130.12,You may recall in the previous slides we have used 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,27,130.12,132.81,FI to represent the future values. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,28,133.91,138.762,_And here we use the notation of X factor, _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,29,138.762,143.331,which is more common when we introduce 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,30,143.331,147.64,such discriminative algorithms. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,31,147.64,149.69,_So, X is our input. _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,32,149.69,157.93,It's a vector with n features and each feature has a value x sub i here. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,33,157.93,162.92000000000002,And I will go with a model that dependency of this binary response variable of 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,34,162.92000000000002,164.36,these features. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,35,164.36,169.897,So in our categorization problem when I have two categories theta 1 and 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,36,169.897,175.183,_theta 2, and we can use the Y value to denote the two categories when Y is 1, _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,37,175.183,180.08,_it means the category of the document, the first class, is theta 1. _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,38,180.08,187.225,_Now, the goal here is the model, the conditional property of Y given X directly _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,39,187.225,193.465,as opposed to model of the generation of X and Y as in the case of Naive Bayes. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,40,193.465,195.985,And another advantage of this kind of approach is that 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,41,195.985,199.88,it would allow many other features than words to be used in this vector 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,42,199.88,203.49,since we're not modeling the generation of this vector. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,43,203.49,205.82999999999998,And we can plug in any signals that we want. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,44,205.82999999999998,211.41,So this is potentially advantageous for doing text categorization. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,45,211.41,214.51,_So more specifically, in logistic regression, _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,46,214.51,220.76,assume the functional form of Y depending on X is the following. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,47,220.76,226.61,And this is very closely related to the log 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,48,226.61,231.29,odds that I introduced in the Naive Bayes or log of probability ratio 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,49,231.29,236.25,of the two categories that you have seen on the previous slide. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,50,237.9,240.23,So this is what I meant. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,51,240.23,245.43,_So in the case of Naive Bayes, we compute this by using those words and _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,52,245.43,251.37,eventually we have reached a formula that looks like this. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,53,252.99,258.29,But here we actually would assume explicitly 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,54,258.29,263.001,that we with the model our 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,55,263.001,267.4,probability of Y given X 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,56,269.84,276.43,directly as a function of these features. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,57,277.58,286.26,_So, most specifically we assume that the ratio of the probability of Y equals 1 and _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,58,286.26,292.79,the probability of Y equals 0 is a function of X. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,59,294.46,296.58,_All right, so it's a function of x and _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,60,296.58,300.91,it's a linear combination of these feature values controlled by theta values. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,61,302.39,306.79,And it seems we know that the probability of Y equals zero 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,62,306.79,311.1,is one minus probability of Y equals one and 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,63,311.1,316.03,this can be also written in this way. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,64,316.03,320.02,So this is a log out ratio here. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,65,322.04,323.25,_And so in logistic regression, _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,66,323.25,327.49,we're basically assuming that the probability of Y equals 1. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,67,327.49,334.57,Okay my X is dependent on this linear combination of all these features. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,68,334.57,339.96,_So it's just one of the many possible ways, assuming that the dependency. _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,69,339.96,342.88,But this particular form has been quite useful and 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,70,342.88,345.90999999999997,it also has some nice properties. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,71,347.76,353.69,So if we rewrite this equation to actually express the probability of Y given X. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,72,353.69,358.77,_In terms of X by getting rid of the logarithm we get this functional form, _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,73,358.77,361.98,and this is called a logistical function. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,74,361.98,367.03,_It's a transformation of X into Y, as you see _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,75,368.12,374.09,_on the right side here, so that the X's will be map _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,76,374.09,379.31,_into a range of values from 0 to 1.0, you can see. _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,77,379.31,383.28,And that's precisely what we want since we have a probability here. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,78,384.35,386.71,And the function form looks like this. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,79,388.17,391.79,So this is the basic idea of logistic regression. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,80,391.79,394.57,And it's a very useful classifier that 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,81,394.57,399.231,can be used to do a lot of classification tasks including text categorization. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,82,401.75,407.1,So as in all cases of model we would be interested in estimating the parameters. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,83,407.1,410.78,_And in fact in all of the machine running programs, once you set up with the model, _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,84,410.78,414.98,_set up object and function to model the file, _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,85,414.98,420.12,then the next step is to compute the parameter values. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,86,420.12,422.68,_In general, we're going to adjust to these parameter values. _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,87,422.68,426.641,Optimize the performance of classify on the training data. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,88,426.641,433.41,_So in our case just assume we have the training data here, xi and yi, and _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,89,433.41,440.81,each pair is basically a future vector of x and a known label for that x. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,90,440.81,443.53,Y is either 1 or 0. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,91,443.53,449.9,So in our case we are interested maximize this conditional likelihood. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,92,451.31,456.02,The conditional likelihood here is 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,93,456.02,461.829,_basically to model why given observe the x, _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,94,461.829,466.382,_so it's not like a moderate x, but _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,95,466.382,470.787,rather we're going to model this. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,96,470.787,475.589,Note that this is a conditional probability of Y given X and 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,97,475.589,480.494,this is also precisely what we wanted For classification. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,98,480.494,486.266,Now so the likelihood function would be just a product of all the training cases. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,99,486.266,487.383,_And in each case, _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,100,487.383,492.99,this is the model of the probability of observing this particular training case. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,101,492.99,499.96,_So given a particular Xi, how likely we are to observe the corresponding Yi? _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,102,499.96,503.228,_Of course, Yi could be 1 or 0, and in fact, _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,103,503.228,508.31,the function found here would vary depending on whether Yi is 1 or 0. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,104,508.31,513.374,_If it's a 1, we'll be taking this form. _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,105,513.374,516.276,And that's basically the logistic regression function. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,106,516.276,518.723,_But what about this, if it's 0? _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,107,518.723,525.42,_Well, if it's 0, then we have to use a different form, and that's this one. _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,108,528.299,530.31,_Now, how do we get this one? _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,109,530.31,534.87,_Well, that's just a 1 minus the probability of Y=1, right? _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,110,535.99,538.2,And you can easily see this. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,111,538.2,544.22,Now the key point in here is that the function form here depends on the observer 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,112,544.22,549.34,_Yi, if it's a 1, it has a different form than when it's 0. _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,113,549.34,553.852,_And if you think about when we want to maximize this probability, _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,114,553.852,559.033,we're basically going to want this probability to be as high as possible. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,115,559.033,566.519,_When the label is 1, that means the document is in probability 1. _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,116,566.519,571.925,_But if the document is not, we're going to maximize this value, _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,117,571.925,576.821,and what's going to happen is actually to make this value as 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,118,576.821,580.5,small as possible because this sum's 1. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,119,580.5,585.67,_When I maximize this one, it's equivalent to minimize this one. _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,120,588.07,593.275,_So you can see basically, if we maximize the conditional likelihood, we're going _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,121,593.275,598.568,to basically try to make the prediction on the training data as accurate as possible. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,122,600.957,604.97,_So as another occasion, when you compute the maximum likelihood data, _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,123,604.97,607.075,_basically you'll find a beta value, _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,124,607.075,611.05,a set of beta values that would maximize this conditional likelihood. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,125,612.19,615.93,_And this, again, then gives us a standard optimization problem. _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,126,615.93,620.13,_In this case, it can be also solved in many ways. _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,127,620.13,622.87,_Newton's method is a popular way to solve this problem, _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,128,622.87,625.05,there are other methods as well. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,129,625.05,629.27,_But in the end, we will look at a set of data values. _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,130,629.27,634.59,_Once we have the beta values, then we have a way to find the scoring _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,131,634.59,638.6,function to help us classify a document. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,132,639.62,640.58,So what's the function? 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,133,640.58,642.399,_Well, it's this one. _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,134,642.399,647.33,_See, if we have all the beta values, are they known? _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,135,647.33,652.81,All we need is to compute the Xi for that document and then plug in those values. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,136,652.81,658.01,That will give us an estimated probability that the document is in category one. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,137,659.17,662.93,_Okay so, so much for logistical regression. _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,138,662.93,666.71,Let's also introduce another discriminative classifier 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,139,666.71,668.23,called K-Nearest Neighbors. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,140,668.23,672.34,_Now in general, I should say there are many such approaches, and _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,141,672.34,677.517,a thorough introduction to all of them is clearly beyond the scope of this course. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,142,677.517,680.169,And you should take a machine learning course or 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,143,680.169,683.5,read more about machine learning to know about them. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,144,683.5,687.95,_Here, I just want to include the basic introduction to some of the most commonly _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,145,687.95,692.345,_used classifiers, since you might use them often for text calculation. _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,146,692.345,696.61,So the second classifier is called K-Nearest Neighbors. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,147,696.61,700.83,_In this approach, we're going to also estimate _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,148,700.83,705.615,_the conditional probability of label given data, but in a very different way. _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,149,705.615,709.36,So the idea is to keep all the training examples and 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,150,709.36,713.9,_then once we see a text object that we want to classify, we're going to find _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,151,713.9,719.29,the K examples in the training set and that are most similar to this text object. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,152,719.29,723.981,_Basically, this is to find the neighbors of this text objector in _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,153,723.981,725.7,the training data set. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,154,725.7,728.314,So once we found the neighborhood and 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,155,728.314,734.132,_we found the object that are close to the object we are interested in classifying, _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,156,734.132,738.62,and let's say we have found the K-Nearest Neighbors. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,157,738.62,741.46,That's why this method is called K-Nearest Neighbors. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,158,741.46,746.23,Then we're going to assign the category that's most common in these neighbors. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,159,746.23,748.87,Basically we're going to allow these neighbors to vote for 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,160,748.87,752.05,the category of the objective that we're interested in classifying. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,161,753.56,758.24,Now that means if most of them have a particular category and it's a category 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,162,758.24,761.79,_one, they're going to say this current object will have category one. _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,163,763.1,767.82,This approach can also be improved by considering the distance of a neighbor and 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,164,767.82,769.24,of a current object. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,165,769.24,773.56,_Basically, we can assume a closed neighbor would have more say _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,166,773.56,775.11,about the category of the subject. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,167,775.11,780.626,_So, we can give such a neighbor more influence on the vote. _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,168,780.626,784.65,And we can take away some of the votes based on the distances. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,169,786.12,788.52,_But the general idea is look at the neighborhood, and _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,170,788.52,793.27,then try to assess the category based on the categories of the neighbors. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,171,793.27,795.745,_Intuitively, this makes a lot of sense. _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,172,795.745,801.17,_But mathematically, this can also be regarded as a way to directly estimate _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,173,801.17,806.87,_there's a conditional probability of label given data, that is p of Y given X. _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,174,808.19,813.64,_Now I'm going to explain this intuition in a moment, but before we proceed, let me _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,175,813.64,820.53,emphasize that we do need a similarity function here in order for this to work. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,176,820.53,823.874,_Note that in naive base class five, we did not need a similarity function. _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,177,823.874,828.16,_And in logistical regression, we did not talk about those similarity function _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,178,828.16,832.57,_either, but here we explicitly require a similarity function. _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,179,832.57,837.5,Now this similarity function actually is a good opportunity for 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,180,837.5,842.288,us to inject any of our insights about the features. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,181,842.288,847.42,Basically effective features are those that would 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,182,847.42,852.77,_make the objects that are on the same category look more similar, but _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,183,852.77,856.6,distinguishing objects in different categories. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,184,856.6,861.1,So the design of this similarity function is closely tied it to the design 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,185,861.1,865.34,of the features in logistical regression and other classifiers. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,186,865.34,868.35,So let's illustrate how K-NN works. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,187,868.35,872.36,Now suppose we have a lot of training instances here. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,188,872.36,878.612,And I've colored them differently and to show just different categories. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,189,878.612,883.69,Now suppose we have a new object in the center that we want to classify. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,190,883.69,886.53,_So according to this approach, you work on finding the neighbors. _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,191,886.53,890.73,_Now, let's first think of a special case of finding just one neighbor, _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,192,890.73,891.62,the closest neighbor. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,193,893.1,899.264,_Now in this case, let's assume the closest neighbor is the box filled with diamonds. _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,194,899.264,904.191,_And so then we're going to say, well, since this is in this _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,195,904.191,909.25,_object that is in category of diamonds, let's say. _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,196,909.25,911.945,_Then we're going to say, well, _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,197,911.945,917.25,we're going to assign the same category to our text object. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,198,917.25,922.346,_But let's also look at another possibility of finding a larger neighborhood, _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,199,922.346,924.73,so let's think about the four neighbors. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,200,926.06,931.09,_In this case, we're going to include a lot of other solid field boxes in red or _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,201,931.09,932.97,_pink, right? _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,202,932.97,938.182,_So in this case now, we're going to notice that among the four neighbors, _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,203,938.182,941.59,there are three neighbors in a different category. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,204,941.59,943.02,_So if we take a vote, _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,205,943.02,948.001,then we'll conclude the object is actually of a different category. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,206,948.001,952.252,So this both illustrates how can nearest neighbor works and 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,207,952.252,957.021,also it illustrates some potential problems of this classifier. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,208,957.021,960.867,_Basically, the results might depend on the K and indeed, _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,209,960.867,963.703,k's an important parameter to optimize. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,210,963.703,967.871,_Now, you can intuitively imagine if we have a lot of neighbors _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,211,967.871,971.8,_around this object, and then we'd be okay because we have _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,212,971.8,976.36,a lot of neighbors who will help us decide the categories. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,213,976.36,981.14,_But if we have only a few, then the decision may not be reliable. _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,214,981.14,985.22,_So on the one hand, we want to find more neighbor, right? _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,215,985.22,986.85,And then we have more votes. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,216,986.85,991.77,_But on the other hand, as we try to find more neighbors we actually could risk _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,217,991.77,996.99,on getting neighbors that are not really similar to this instance. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,218,996.99,1000.21,They might actually be far away as you try to get more neighbors. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,219,1000.21,1004.52,So although you get more neighbors but those neighbors aren't necessarily so 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,220,1004.52,1007.65,helpful because they are not very similar to the object. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,221,1007.65,1011.15,So the parameter still has to be set empirically. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,222,1011.15,1015.996,_And typically, you can optimize such a parameter by using cross validation. _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,223,1015.996,1021.378,_Basically, you're going to separate your training data into two parts and _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,224,1021.378,1025.803,then you're going to use one part to actually help you choose 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,225,1025.803,1030.778,the parameter k here or some other parameters in other class files. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,226,1030.778,1035.913,And then you're going to assume this number that works well on your 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,227,1035.913,1041.063,training that will be actually be the best for your future data. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,228,1043.103,1044.257,_So as I mentioned, _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,229,1044.257,1049.234,K-NN can be actually regarded as estimate of conditional problem within y given x 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,230,1049.234,1054.6,an that's why we put this in the category of discriminative approaches. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,231,1054.6,1059.47,So the key assumption that we made in this approach is that the distribution 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,232,1059.47,1064.027,of the label given the document probability a category given for 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,233,1064.027,1071.62,example probability of theta i given document d is locally smooth. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,234,1071.62,1076.89,And that just means we're going to assume that this probability is the same for 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,235,1076.89,1081.57,all the documents in these region R here. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,236,1081.57,1085.26,And suppose we draw a neighborhood and we're going to assume in this neighborhood 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,237,1085.26,1090.32,since the data instances are very similar we're going to assume that 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,238,1090.32,1095.53,the conditional distribution of the label given the data will be roughly the same. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,239,1095.53,1099.408,If these are very different then we're going to assume that 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,240,1099.408,1103.136,the probability of c doc given d would be also similar. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,241,1103.136,1104.976,So that's a very key assumption. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,242,1104.976,1109.481,And that's actually important assumption 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,243,1109.481,1114.82,that would allow us to do a lot of machinery. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,244,1114.82,1115.73,_But in reality, _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,245,1115.73,1119.56,_whether this is true of course, would depend on how we define similarity. _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,246,1119.56,1123.61,Because neighborhood is largely determined by our similarity function. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,247,1123.61,1128.18,If our similarity function captures objects that do follow similar 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,248,1128.18,1131.29,distributions then these assumptions are okay but 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,249,1131.29,1135.24,_if our similarity function could not capture that, obviously these _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,250,1135.24,1138.11,assumption would be a problem and then the classifier would not be accurate. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,251,1139.32,1141.68,_Okay, let's proceed with these assumption. _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,252,1141.68,1143.31,_Then what we are saying is that, _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,253,1143.31,1147.57,in order to estimate the probability of category given a document. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,254,1147.57,1154.23,We can try to estimate the probability of the category given that entire region. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,255,1154.23,1156.73,_Now, this has a benefit, of course, _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,256,1156.73,1160.34,of bringing additional data points to help us estimate this probability. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,257,1162.66,1165.41,And so this is precisely the idea of K-NN. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,258,1165.41,1169.91,Basically now we can use the known categories of 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,259,1169.91,1173.87,all the documents in this region to estimate this probability. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,260,1173.87,1180.34,And I have even given a formula here where you can see we just count the topics in 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,261,1180.34,1184.91,this region and then normalize that by the total number of documents in the region. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,262,1184.91,1189.51,_So the numerator that you see here, c of theta i and r, _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,263,1189.51,1195.025,is a counter of the documents in region R was category theta i. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,264,1195.025,1197.91,Since these are training document and we know they are categories. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,265,1197.91,1201.394,We can simply count how many times it was since here. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,266,1201.394,1203.491,_How many times we have the same signs, etc. _
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,267,1203.491,1207.269,And then the denominator is just the total number of training 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,268,1207.269,1208.981,documents in this region. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,269,1208.981,1212.781,So this gives us a rough estimate of which categories most popular in this 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,270,1212.781,1213.661,neighborhood. 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,271,1213.661,1217.539,And we are going to assign the popular category 
4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt,272,1217.539,1221.821,to our data object since it falls into this region. 

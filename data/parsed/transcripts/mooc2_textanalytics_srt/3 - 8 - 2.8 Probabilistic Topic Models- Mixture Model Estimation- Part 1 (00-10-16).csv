name,id,from,to,text
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,1,0.012,8.031,[SOUND] This 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,2,8.031,10.57,lecture is about mixture model estimation. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,3,12.24,16.26,In this lecture we're going to continue discussing probabilistic topic models. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,4,16.26,16.83,_In particular, _
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,5,16.83,20.38,we're going to talk about how to estimate the parameters of a mixture model. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,6,23.01,26.87,So let's first look at our motivation for using a mixture model. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,7,26.87,30.026,And we hope to factor out the background words. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,8,30.026,33.48,From the top-words equation. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,9,33.48,39.99,The idea is to assume that the text data actually contained two kinds of words. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,10,39.99,44.817,One kind is from the background here. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,11,44.817,48.8,_So, the is, we, etc. _
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,12,48.8,53.82,And the other kind is from our pop board distribution that we are interested in. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,13,56.31,61.42,_So in order to solve this problem of factoring out background words, _
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,14,61.42,65.78,we can set up our mixture model as false. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,15,65.78,69.25,We're going to assume that we already know the parameters of 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,16,69.25,74.01,_all the values for all the parameters in the mixture model, _
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,17,74.01,79.11,except for the water distribution of which is our target. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,18,80.13,85.13,So this is a case of customizing a probabilist model so 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,19,85.13,89.5,that we embedded a known variable that we are interested in. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,20,89.5,91.265,But we're going to simplify other things. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,21,91.265,94.18,We're going to assume we have knowledge above others. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,22,94.18,97.75999999999999,And this is a powerful way of customizing a model. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,23,97.75999999999999,99.5,For a particular need. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,24,99.5,100.32,_Now you can imagine, _
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,25,100.32,105.0,we could have assumed that we also don't know the background words. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,26,105.0,106.22999999999999,_But in this case, _
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,27,106.22999999999999,111.81,our goal is to factor out precisely those high probability background words. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,28,111.81,115.53,So we assume the background model is already fixed. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,29,116.68,121.67,And one problem here is how can we adjust theta sub d 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,30,121.67,126.27,in order to maximize the probability of the observed document here and 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,31,126.27,127.92,we assume all the other perimeters are now. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,32,129.47,132.53,Now although we designed the model holistically. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,33,132.53,136.23,To try to factor out these background words. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,34,136.23,141.056,_It's unclear whether, if we use maximum write or estimator. _
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,35,141.056,145.85,We will actually end up having a whole distribution where the Common 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,36,145.85,149.94,words like the would indeed have smaller probabilities than before. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,37,151.22,156.88,Now in this case it turns out the answer is yes. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,38,156.88,161.07,_And when we set up the probability in this way, _
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,39,161.07,165.72,when we use maximum likelihood or we will end up having a word distribution 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,40,166.89,169.99,where the use common words would be factored out. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,41,169.99,172.57,By the use of the background rule of distribution. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,42,173.57999999999998,176.71,_So to understand why this is so, _
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,43,176.71,180.6,it's useful to examine the behavior of a mixture model. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,44,180.6,183.91,So we're going to look at a very very simple case. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,45,183.91,188.86,In order to understand some interesting behaviors of a mixture model. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,46,188.86,195.13,The observed pattern here actually are generalizable to mixture model in general. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,47,195.13,197.92000000000002,But it's much easier to understand this behavior 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,48,197.92000000000002,201.75,when we use A very simple case like what we are seeing here. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,49,201.75,205.29,_So specifically in this case, let's assume that _
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,50,205.29,209.67000000000002,the probability choosing each of the two models is exactly the same. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,51,209.67000000000002,213.4,So we're going to flip a fair coin to decide which model to use. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,52,214.42000000000002,216.61,_Furthermore, we're going to assume there are. _
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,53,216.61,219.51,_Precisely two words, the and text. _
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,54,219.51,226.12,_Obviously this is a very naive oversimplification of the actual text, _
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,55,226.12,232.11,_but again, it's useful to examine the behavior in such a special case. _
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,56,233.69,238.18,So we further assume that the background model gives probability of 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,57,238.18,243.059,0.9 towards the end text 0.1. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,58,243.059,248.34,_Now, lets also assume that our data is extremely simple the document has just _
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,59,248.34,253.82,two words text and the so now lets right down the likeable function in such a case. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,60,253.82,258.35,_First, what's the probability of text, and what's the probably of the. _
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,61,259.55,262.34,I hope by this point you'll be able to write it down. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,62,263.76,268.644,_So the probability of text is basically the sum over two cases, _
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,63,268.644,273.46,where each case corresponds with to each of the order distribution 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,64,274.48,278.06,and it accounts for the two ways of generating text. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,65,279.49,283.58,_And inside each case, we have the probability of choosing the model, _
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,66,283.58,290.36,which is 0.5 multiplied by the probability of observing text from that model. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,67,290.36,294.98,_Similarly, the, would have a probability of the same form, _
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,68,294.98,297.48,just what is different is the exact probabilities. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,69,298.9,303.49,So naturally our lateral function is just a product of the two. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,70,303.49,307.11,_So It's very easy to see that, _
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,71,308.14,311.0,once you understand what's the probability of each word. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,72,311.0,315.45,Which is also why it's so important to understand what's exactly 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,73,315.45,319.87,the probability of observing each word from such a mixture model. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,74,319.87,325.69,_Now, the interesting question now is, how can we then optimize this likelihood? _
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,75,325.69,329.42,_Well, you will note that there are only two variables. _
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,76,329.42,332.27,They are precisely the two probabilities of the two words. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,77,332.27,335.95,Text [INAUDIBLE] given by theta sub d. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,78,335.95,339.65999999999997,And this is because we have assumed that all the other parameters are known. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,79,341.24,345.46,_So, now the question is a very simple algebra question. _
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,80,345.46,348.45,_So, we have a simple expression with two variables and _
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,81,348.45,353.14,we hope to choose the values of these two variables to maximize this function. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,82,354.27,358.90999999999997,And the exercises that we have seen some simple algebra problems. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,83,360.15,364.65,_Note that the two probabilities must sum to one, so there's some constraint. _
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,84,366.34,368.08,_If there were no constraint of course, _
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,85,368.08,372.02,_we would set both probabilities to their maximum value which would be one, _
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,86,372.02,378.0,_to maximize, But we can't do that because text then the must sum to one. _
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,87,378.0,380.24,We can't give both a probability of one. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,88,381.84,385.15,_So, now the question is how should we allocate the probability and _
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,89,385.15,387.09,the math between the two words. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,90,387.09,388.4,What do you think? 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,91,388.4,392.32,_Now, it would be useful to look at this formula For a moment, and _
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,92,392.32,396.54,_to see what, intuitively, what we do in order to _
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,93,396.54,399.94,do set these probabilities to maximize the value of this function. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,94,402.42,404.31,_Okay, if we look into this further, _
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,95,404.31,410.07,then we see some interesting behavior of The two component models in that 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,96,410.07,414.73,they will be collaborating to maximize the probability of the observed data. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,97,414.73,417.79,Which is dictated by the maximum likelihood estimator. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,98,417.79,422.02,_But they are also competing in some way, and in particular, _
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,99,422.02,425.35,they would be competing on the words. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,100,425.35,429.14,And they would tend to back high probabilities on different words 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,101,429.14,434.68,to avoid this competition in some sense or to gain advantages in this competition. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,102,434.68,436.97,_So again, looking at this objective function and _
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,103,436.97,440.22,we have a constraint on the two probabilities. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,104,441.36,445.46,_Now, if you look at the formula intuitively, _
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,105,445.46,450.509,you might feel that you want to set the probability of text to be somewhat larger. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,106,452.13,458.15999999999997,_And this inducing can be work supported by mathematical fact, which is when _
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,107,458.15999999999997,463.28,the sum of two variables is a constant then the product of them 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,108,463.28,469.15,_which is maximum when they are equal, and this is a fact we know from algebra. _
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,109,469.15,472.90999999999997,Now if we plug that [INAUDIBLE] It would mean that we have to make the two 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,110,472.90999999999997,475.1,probabilities equal. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,111,476.17,477.83,And when we make them equal and 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,112,477.83,482.18,_then if we consider the constraint it will be easy to solve this problem, and _
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,113,482.18,489.31,the solution is the probability of tax will be .09 and probability is .01. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,114,489.31,494.15,_The probability of text is now much larger than probability of the, and _
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,115,494.15,497.2,this is not the case when have just one distribution. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,116,497.2,501.04,_And this is clearly because of the use of the background model, _
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,117,501.04,506.48,which assigned the very high probability to the and low probability to text. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,118,506.48,510.27,And if you look at the equation you will see obviously 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,119,510.27,513.3,some interaction of the two distributions here. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,120,515.07,519.09,_In particular, you will see in order to make them equal. _
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,121,519.09,526.35,And then the probability assigned by theta sub d must be higher for 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,122,526.35,530.849,a word that has a smaller probability given by the background. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,123,533.38,536.69,This is obvious from examining this equation. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,124,536.69,539.85,Because the background part is weak for text. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,125,539.85,540.71,It's small. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,126,540.71,544.9,_So in order to compensate for that, we must make the probability for _
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,127,544.9,551.27,_text given by theta sub D somewhat larger, so that the two sides can be balanced. _
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,128,551.27,557.28,So this is in fact a very general behavior of this model. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,129,557.28,561.78,_And that is, if one distribution assigns a high probability to one word than another, _
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,130,561.78,565.54,then the other distribution would tend to do the opposite. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,131,565.54,568.96,Basically it would discourage other distributions to do the same And 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,132,568.96,574.65,this is to balance them out so we can account for all kinds of words. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,133,574.65,578.811,And this also means that by using a background model that is fixed into 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,134,578.811,582.341,assigned high probabilities through background words. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,135,582.341,587.194,We can indeed encourages the unknown topical one of this to assign smaller 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,136,587.194,590.035,probabilities for such common words. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,137,590.035,594.302,_Instead put more probability than this on the content words, _
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,138,594.302,598.17,that cannot be explained well by the background model. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,139,598.17,602.754,Meaning that they have a very small probability from the background motor like 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,140,602.754,603.452,text here. 
3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt,141,603.452,613.452,[MUSIC] 

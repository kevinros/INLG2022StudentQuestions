filename,lecture in question,lecture,raw from,raw to,from,to,text
week3_allquestions,3.6,3 - 9 - 2.8 Evaluation of TR Systems- Practical Issues (00-15-14).csv, 3'50",4'50",230,290, What are examples of good MAP and gMAP values when measuring precision?
week3_allquestions,3.6,3 - 9 - 2.8 Evaluation of TR Systems- Practical Issues (00-15-14).csv, 3'0",9'0",180,540, What are the most popular methods for statistical significance testing?
week3_allquestions,3.4,3 - 7 - 2.6 Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01) .csv, 4'52" , 6'28",292,388,$ Why is this a special case? How's it different from the normal method? I understand that the more documents the average precision doesn't change as much but why would be want to measure precision, if there's only one relevant document in the first place.$
week3_allquestions,3.2,3 - 5 - 2.5 Evaluation of TR Systems- Basic Measures (00-12-54).csv, 7'22",9'30",442,570, For the Parameter Variable what makes it usually set to one and when would it not be 1?
week3_allquestions,3.3,3 - 6 - 2.6 Evaluation of TR Systems- Evaluating Ranked Lists Part 1 (00-12-51).csv, 8'00",9'30",480,570, Is it possible to use both algorithms and at runtime decide which one should be used?
week3_allquestions,3.6,3 - 9 - 2.8 Evaluation of TR Systems- Practical Issues (00-15-14).csv, 6'45",7'22",405,442, Where did they get the values p=1 and p=0.9375?
week3_allquestions,3.2,3 - 5 - 2.5 Evaluation of TR Systems- Basic Measures (00-12-54).csv, 9' 16",10' 00",556,600, Why do we have to use an F measure to combine precision and recall?
week3_allquestions,3.6,3 - 9 - 2.8 Evaluation of TR Systems- Practical Issues (00-15-14).csv, 5'15",9'20",315,560, Could we get more examples of Statistical Significance Testing?
week3_allquestions,3.4,3 - 7 - 2.6 Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01) .csv, 7'02",8'55",422,535, Still confused about this 1/r - why take the mean of the 1/r values instead of using just one?</pre>
week3_allquestions,3.4,3 - 7 - 2.6 Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01) .csv, 7'12'',9'09'',432,549, Can you explain why we use 1/r with a concrete example since I am still confused
week3_allquestions,3.6,3 - 9 - 2.8 Evaluation of TR Systems- Practical Issues (00-15-14).csv, 7'23",8'50",443,530, which distribution are we using in this model? is it normal distribution? If it is why we choose normal?
week3_allquestions,3.2,3 - 5 - 2.5 Evaluation of TR Systems- Basic Measures (00-12-54).csv, 3'35" , 7'00",215,420,$ Are the documents that are not retrieved and not relevant used in any sort of metric? For example, working with homophones or something, could those irrelevant and non retrieved documents be used to measure accuracy?$
week3_allquestions,3.6,3 - 9 - 2.8 Evaluation of TR Systems- Practical Issues (00-15-14).csv, 5'50'',7'00'',350,420, How do you get a p-value from the sign test since it is only pluses and minuses?
week3_allquestions,3.2,3 - 5 - 2.5 Evaluation of TR Systems- Basic Measures (00-12-54).csv, 2'25",3'33",145,213, What is the value of the denominator when calculating recall in test collection evaluation?
week3_allquestions,3.6,3 - 9 - 2.8 Evaluation of TR Systems- Practical Issues (00-15-14).csv, 9'15",12'25",555,745, What is the benefit of following the pooling strategy?
week3_allquestions,3.2,3 - 5 - 2.5 Evaluation of TR Systems- Basic Measures (00-12-54).csv, 5'20",6'30",320,390,$ If there is a trade-off between precision and recall, is there a better ranking-based system for accuracy in a selection of documents?$
week3_allquestions,3.3,3 - 6 - 2.6 Evaluation of TR Systems- Evaluating Ranked Lists Part 1 (00-12-51).csv, 0'00",2'52",0,172,$ When you calculate the precision in a collection, does the denominator increase each time you look at any document, even the non-relevant ones? And relevant document means positive ones, right? A little example with comparison to a google search would be really helpful!$
week3_allquestions,3.5,3 - 8 - 2.7 Evaluation of TR Systems- Multi-Level Judgements (00-10-48).csv, 9'0",9'30",540,570, How does dividing by the Ideal DCG actually normalize the DCG?
week3_allquestions,3.6,3 - 9 - 2.8 Evaluation of TR Systems- Practical Issues (00-15-14).csv, 2'35",9'00",155,540, What is the difference between Wilcoxon Test and he Sign Test?
week3_allquestions,3.5,3 - 8 - 2.7 Evaluation of TR Systems- Multi-Level Judgements (00-10-48).csv, 4'28",5'55",268,355, Why do we normalise discounted cumulative gain with the log of the document rank instead of just the rank?
week3_allquestions,3.5,3 - 8 - 2.7 Evaluation of TR Systems- Multi-Level Judgements (00-10-48).csv, 04'25",04'50",265,290, What is the base of the log used for nDCG? Does the base even matter here?
week3_allquestions,3.2,3 - 5 - 2.5 Evaluation of TR Systems- Basic Measures (00-12-54).csv, 8'45",9'11",525,551, what would be the effect if we set the parameter larger or less than 1?
week3_allquestions,3.3,3 - 6 - 2.6 Evaluation of TR Systems- Evaluating Ranked Lists Part 1 (00-12-51).csv, 13'58",14'40",838,880, Why the standard method for evaluating a ranked list is quite sensitive to a small change of precision of random document?
week3_allquestions,3.2,3 - 5 - 2.5 Evaluation of TR Systems- Basic Measures (00-12-54).csv, 7'05",9'30",425,570,$ When calculating values for F-measure, it's necessary to have non-zero precision and recall. What would happen if the system doesnot respond well and give zero retrieved docs?$
week3_allquestions,3.6,3 - 9 - 2.8 Evaluation of TR Systems- Practical Issues (00-15-14).csv,10'20'',11'20'',620,680,What's the meaning of "combine all the top-k sets"?
week3_allquestions,3.2,3 - 5 - 2.5 Evaluation of TR Systems- Basic Measures (00-12-54).csv, 9'50",10'40",590,640, What is the trade-off between precision and recall while calculating F-Measure?
week3_allquestions,3.6,3 - 9 - 2.8 Evaluation of TR Systems- Practical Issues (00-15-14).csv, 7'19",8'55",439,535, Why is B better? Does it have less random fluctuations?
week3_allquestions,3.5,3 - 8 - 2.7 Evaluation of TR Systems- Multi-Level Judgements (00-10-48).csv, 7'00",9'00",420,540, Why do we have @k in DCG@k? Shouldn't the ranking system return a ranked list of all documents?
week3_allquestions,3.5,3 - 8 - 2.7 Evaluation of TR Systems- Multi-Level Judgements (00-10-48).csv, 6'42",7'17",402,437, Why did we assume we have 9 documents rated 3 and 1 rated 2 for ideal DCG if in the example for actual DCG all the documents are ranked differently? Should we be assuming all documents but 1 are ranked 3 in every ideal situation?
week3_allquestions,3.6,3 - 9 - 2.8 Evaluation of TR Systems- Practical Issues (00-15-14).csv, 10'09'',10'20'',609,620,What's the meaning of "return top-k document"?
week3_allquestions,3.5,3 - 8 - 2.7 Evaluation of TR Systems- Multi-Level Judgements (00-10-48).csv, 6'54",8'12",414,492, How do you calculate the IdealDCG@10 again? Do you put a score of 3 (very relevant) for every document except the last one?
week3_allquestions,3.6,3 - 9 - 2.8 Evaluation of TR Systems- Practical Issues (00-15-14).csv, 7’15”,7’25”,435,445, What is the Wilcoxon method?
week3_allquestions,3.2,3 - 5 - 2.5 Evaluation of TR Systems- Basic Measures (00-12-54).csv, 7’03”,7’29”,423,449, Why do we combine the precision and recall?
week3_allquestions,3.2,3 - 5 - 2.5 Evaluation of TR Systems- Basic Measures (00-12-54).csv, 7’03”,7’14”,423,434, Why we need a parameter here?
week3_allquestions,3.5,3 - 8 - 2.7 Evaluation of TR Systems- Multi-Level Judgements (00-10-48).csv, 4'30",5'30",270,330, Why is the discounted cumulative gain calculated by dividing the log of the position?
week3_allquestions,3.5,3 - 8 - 2.7 Evaluation of TR Systems- Multi-Level Judgements (00-10-48).csv, 6'21",7'24",381,444,$ Is the discounting function always supposed to be division by log, or is this just a type of discounting function?$
week3_allquestions,3.2,3 - 5 - 2.5 Evaluation of TR Systems- Basic Measures (00-12-54).csv, 3'40",6'20",220,380, What exactly does recall refer to?
week3_allquestions,3.2,3 - 5 - 2.5 Evaluation of TR Systems- Basic Measures (00-12-54).csv, 2'30",4'30",150,270,How does recall be assumed that there are 10 relevant documents in the collection? Is this a labeled dataset to evaluate the system because won't multiple documents pieced together make something relevant as well?
week3_allquestions,3.1,3 - 4 - 2.4 Evaluation of TR Systems (00-10-10).csv, 0'45",0'46",45,46, How often do engineers use evaluation in the real world scenario
week3_allquestions,3.4,3 - 7 - 2.6 Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01) .csv, 3'40",4'00",220,240, What is the difficulty of a query? Why the situation affects the choice of MAP and gMAP
week3_allquestions,3.6,3 - 9 - 2.8 Evaluation of TR Systems- Practical Issues (00-15-14).csv, 11'22",11'45",682,705,$ For example, let's have a system that ranks only the top five percent of relevant documents correctly and blindly marks everything else as non-relevant. It seems like it will pass the pooling test strategy although it is not a good TR system. Therefore, I am confused about how the pooling strategy can work if it simply assumes all unjudged documents are non-relevant? It seems like this would be problematice for small and large datasets.$
week3_allquestions,3.4,3 - 7 - 2.6 Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01) .csv, 3'45",4'34",225,274, Why don't we do another operation like deciding which query vectors are likely to be more difficult and rare (like we did in L2) and then based on that choose to do either MAP or GMAP?
week3_allquestions,3.2,3 - 5 - 2.5 Evaluation of TR Systems- Basic Measures (00-12-54).csv, 6'40",7'03",400,423, Why is precision most important for the top ten resulted documents? Should precision affect the majority of documents affected?
week3_allquestions,3.2,3 - 5 - 2.5 Evaluation of TR Systems- Basic Measures (00-12-54).csv, 4'00",4'20",240,260,$ Which measure should be given more priority, precision or recall$
week3_allquestions,3.6,3 - 9 - 2.8 Evaluation of TR Systems- Practical Issues (00-15-14).csv, 5'10",8'47",310,527, Examples of how to carry out those tests and what does the result mean?
week3_allquestions,3.4,3 - 7 - 2.6 Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01) .csv, 0'15'',0'30'',15,30,$ What is meant by having variance across queries, do you really mean bias?$
week3_allquestions,3.6,3 - 9 - 2.8 Evaluation of TR Systems- Practical Issues (00-15-14).csv, 13'00",13'33",780,813, Is it a good idea to use two completely different methods and then return the "average" of the results? Would this perhaps result in increased relevance and accuracy?
week3_allquestions,3.5,3 - 8 - 2.7 Evaluation of TR Systems- Multi-Level Judgements (00-10-48).csv, 9'42",10'30",582,630, Is the NDCG used to rank different methods of retrieval or is it used to actually select individual document collections?
week3_allquestions,3.5,3 - 8 - 2.7 Evaluation of TR Systems- Multi-Level Judgements (00-10-48).csv, 6'50",7'25",410,445,$ For the ideal DCG, do we set up an arbitrary standard of what's ideal, or the ideal situation is always when n-1 documents are very relevant?$
week3_allquestions,3.5,3 - 8 - 2.7 Evaluation of TR Systems- Multi-Level Judgements (00-10-48).csv, 6'30",7'16",390,436, Why is the ideal discounted cumulative gain based on 9 documents being relevant with 10 documents rather than all 10 documents being relevant?
week3_allquestions,3.4,3 - 7 - 2.6 Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01) .csv, 0'50",4'00",50,240, Can you go into more depth of the differences between MAP and gMAP?
week3_allquestions,1.2,MISSING, 2'20",2'35",140,155, Why do we have to use Precision and Recall over raw accuracy?
week3_allquestions,3.3,3 - 6 - 2.6 Evaluation of TR Systems- Evaluating Ranked Lists Part 1 (00-12-51).csv, 8'30",8'40",510,520,$ I don't get why the Ideal System would be a horizontal line, shouldn't precision get higher?$
week3_allquestions,3.6,3 - 9 - 2.8 Evaluation of TR Systems- Practical Issues (00-15-14).csv, 9'48",12'25",588,745, What is the risk associated with discarding documents that are potentially relevant?
week3_allquestions,3.1,3 - 4 - 2.4 Evaluation of TR Systems (00-10-10).csv, 6'00",6'50",360,410,$ By "reusable", is the method independent of data/text content?$
week3_allquestions,3.2,3 - 5 - 2.5 Evaluation of TR Systems- Basic Measures (00-12-54).csv, 7’03”,8’42”,423,522, What is the difference between an F-measure and an F1-measure besides adjusting the Beta parameter?
week3_allquestions,3.6,3 - 9 - 2.8 Evaluation of TR Systems- Practical Issues (00-15-14).csv, 10'13",11'27",613,687,$ How is the K determined for the judging of top-K documents, since it can vary from system to system?$
week3_allquestions,3.5,3 - 8 - 2.7 Evaluation of TR Systems- Multi-Level Judgements (00-10-48).csv, 5'55",9'30",355,570, Are there situations where we don't want to normalize the DCG and instead have some queries contribute more to the average than others?
week3_allquestions,1.1,MISSING, 16'03",16'08",963,968, What is the difference between MAP and gMAP
week3_allquestions,3.1,3 - 4 - 2.4 Evaluation of TR Systems (00-10-10).csv, 7'35",8'30",455,510,$ When performing test collection, are the initial relevance judgements determined by humans or some emperical method?$
week3_allquestions,3.3,3 - 6 - 2.6 Evaluation of TR Systems- Evaluating Ranked Lists Part 1 (00-12-51).csv, 11'40",15'41",700,941, Is there any particular reason for not computing the average recall? I'm afraid I might be overlooking something is I want to use it to test the performance of any TR algorithm.
week3_allquestions,3.2,3 - 5 - 2.5 Evaluation of TR Systems- Basic Measures (00-12-54).csv, 1'02",2'10",62,130, How is it possible to measure the Recall Value of a system if the number of relevant documents out of a complete set is unknown?
week3_allquestions,3.2,3 - 5 - 2.5 Evaluation of TR Systems- Basic Measures (00-12-54).csv, 7'44",8'10",464,490, How does parameter beta in F-Measure works?
week3_allquestions,3.4,3 - 7 - 2.6 Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01) .csv, 4'33" , 4'50",273,290, how is gMAP calculated?
week3_allquestions,3.5,3 - 8 - 2.7 Evaluation of TR Systems- Multi-Level Judgements (00-10-48).csv, 9'50" , 10'10",590,610, what stuffs can nDCG do but DCG cannot?
week3_allquestions,3.4,3 - 7 - 2.6 Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01) .csv, 1'30",2'10",90,130, Could you please give an example of gMAP? I find the concept a bit abstract.
week3_allquestions,3.3,3 - 6 - 2.6 Evaluation of TR Systems- Evaluating Ranked Lists Part 1 (00-12-51).csv, 11'37",15'26",697,926, Why is non changing recall count as zero when calculating the average position instead of using updated precision?
week3_allquestions,3.5,3 - 8 - 2.7 Evaluation of TR Systems- Multi-Level Judgements (00-10-48).csv, 1'15",2'30",75,150,$ If a user ends up reading documents that were through to be non-relevant, wouldn't that make those documents relevant and they should then have a higher gain? Why is the list not sorted by most relevant?$
week3_allquestions,2.6,MISSING, 10'15'',10'27'',615,627, What is the purpose of making human assessors judge a collection of top-K documents?
week3_allquestions,3.5,3 - 8 - 2.7 Evaluation of TR Systems- Multi-Level Judgements (00-10-48).csv, 5'20",5'50",320,350, Is there only one way of discounting?

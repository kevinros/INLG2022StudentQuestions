filename,lecture in question,lecture,raw from,raw to,from,to,text
week4_allquestions,4.3,4 - 3 - 3.3 Query Likelihood Retrieval Function (00-12-07).csv, 10'38",11'38",638,698, How do you determine which estimation method would be most effective to solve a particular problem?
week4_allquestions,4.7,4 - 7 - 3.5 Smoothing Methods Part - 2 (00-13-17).csv, 1'00",7'00",60,420, How do we pick the best values to use for lambda and mu when doing the smoothing?
week4_allquestions,4.1,4 - 1 - 3.1 Probabilistic Retrieval Model- Basic Idea (00-12-44).csv, 11'22" , 12'44",682,764, I am having trouble understanding why or how the R value helps NLP?
week4_allquestions,4.5,4 - 5 - 3.4 Smoothing of Language Model - Part 2 (00-09-36).csv, 1'30",1'52",90,112, Why is the log term of p(q|d) independent of the document?
week4_allquestions,4.3,4 - 3 - 3.3 Query Likelihood Retrieval Function (00-12-07).csv, 10’15”,10’20”,615,620, Why can we assume all words in a query are independent?
week4_allquestions,4.5,4 - 5 - 3.4 Smoothing of Language Model - Part 2 (00-09-36).csv, 2' 10",2' 50",130,170, Why do we ignore the last part of the log p(q | d) formula when ranking documents?
week4_allquestions,4.3,4 - 3 - 3.3 Query Likelihood Retrieval Function (00-12-07).csv, 2'20",3'23",140,203,$ Making the assumtion that each query word is generated independently seems weird because when a user creates a query, usually the words are not indpendent. Is anything used to try and bridge the gap between indpendently generated query words and actual queries?$
week4_allquestions,4.5,4 - 5 - 3.4 Smoothing of Language Model - Part 2 (00-09-36).csv, 4'48'',5'50'',288,350, Could you further explain the doc length normalization a little bit since I do remember it should be a term as a mutiply factor instead of addition factor.
week4_allquestions,4.2,4 - 2 - 3.2 Statistical Language Models (00-17-53).csv, 5'43",5'50",343,350, what is the probability exactly?
week4_allquestions,4.5,4 - 5 - 3.4 Smoothing of Language Model - Part 2 (00-09-36).csv, 4'15'',5'40'',255,340, How exactly does nlog(alpha_d) relate to doc length normalization?
week4_allquestions,4.2,4 - 2 - 3.2 Statistical Language Models (00-17-53).csv, 4'45",5'00",285,300, Why are there only n-1 parameters in the generative model?
week4_allquestions,4.7,4 - 7 - 3.5 Smoothing Methods Part - 2 (00-13-17).csv, 0'15",9'40",15,580,$ Between JM Smoothing and Dirichlet Prior Smoothing, which is better in which situation and what is the main difference between their ranking functions?$
week4_allquestions,4.3,4 - 3 - 3.3 Query Likelihood Retrieval Function (00-12-07).csv, 0'30",2'00",30,120,$ If the user poses a query not related to the document they just observed, wouldn't the query likelihood retrieval model be less accurate than basing results off the query itself?$
week4_allquestions,4.1,4 - 1 - 3.1 Probabilistic Retrieval Model- Basic Idea (00-12-44).csv, 8'40",9'00",520,540,$ In Query Likelihood Retrieval Model, does the language model ensure that the assumed probabilities for "imaginary documents" are sufficient?$
week4_allquestions,4.2,4 - 2 - 3.2 Statistical Language Models (00-17-53).csv, 5'00",5'08",300,308,$Why would we need the statistical language model to generate words for us, instead of sequences?$
week4_allquestions,4.6,4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).csv, 0'35",1'46",35,106, Query Likelihood+smoothing
week4_allquestions,4.2,4 - 2 - 3.2 Statistical Language Models (00-17-53).csv, 4'00" , 4'30",240,270, does the sequence of words matter in the query? Is it suggesting that querying "baseball game yesterday" is the same as querying "baseball yesterday game"?
week4_allquestions,4.5,4 - 5 - 3.4 Smoothing of Language Model - Part 2 (00-09-36).csv, 4'15",4'17",255,257, So what does IDF weighting have to do with this?
week4_allquestions,4.3,4 - 3 - 3.3 Query Likelihood Retrieval Function (00-12-07).csv, 06'40",07'50",400,470, I am confused as to what is meant by "sampling words from a doc model". May you clarify?
week4_allquestions,4.2,4 - 2 - 3.2 Statistical Language Models (00-17-53).csv, 1'45",2'11",105,131,$ if the probability of one word is too low, will it underfloat to 0? If yes, how to deal with this type of question?$
week4_allquestions,4.7,4 - 7 - 3.5 Smoothing Methods Part - 2 (00-13-17).csv, 11'00",13'00",660,780,$ Which one is better, JM smoothing or Dirichlet Prior smoothing?$
week4_allquestions,4.2,4 - 2 - 3.2 Statistical Language Models (00-17-53).csv, 7'36'',10'20'',456,620,Confused about ML Estimator.
week4_allquestions,4.7,4 - 7 - 3.5 Smoothing Methods Part - 2 (00-13-17).csv, 5’00”,6’36”,300,396, What’s the difference between Jelinek-Mercer smoothing and Dirichlet prior smoothing since they have very similar forms?
week4_allquestions,4.5,4 - 5 - 3.4 Smoothing of Language Model - Part 2 (00-09-36).csv, 4'55",5'40",295,340, What exactly does alpha_d mean in English?
week4_allquestions,4.3,4 - 3 - 3.3 Query Likelihood Retrieval Function (00-12-07).csv, 9'04",9'10",544,550, Why do we need to log the probabilities in this formula?
week4_allquestions,4.5,4 - 5 - 3.4 Smoothing of Language Model - Part 2 (00-09-36).csv,3'00'',6'20'',180,380,What if we don't ignore the last part of ranking?
week4_allquestions,4.4,4 - 4 - 3.4 Smoothing of Language Model - Part 1 (00-12-15).csv, 6'32",10'47",392,647, Which part of the formula for the ranking function with smoothing actually implements smoothing?
week4_allquestions,4.4,4 - 4 - 3.4 Smoothing of Language Model - Part 1 (00-12-15).csv, 7’18”,7’38”,438,458, Why do we assume that in our smoothing method each word that aren't ovserved would have a different form of probability?
week4_allquestions,4.7,4 - 7 - 3.5 Smoothing Methods Part - 2 (00-13-17).csv, 9'44",11'44",584,704, Can we have examples on how to rank using the smoothed ranking functions?
week4_allquestions,4.4,4 - 4 - 3.4 Smoothing of Language Model - Part 1 (00-12-15).csv, 6'47",9'46",407,586, Why even care about query words not matched in d? These sums are getting confusing.
week4_allquestions,4.2,4 - 2 - 3.2 Statistical Language Models (00-17-53).csv, 4'53",7'55",293,475, Is any normalization necessary for the Unigram language model?
week4_allquestions,4.5,4 - 5 - 3.4 Smoothing of Language Model - Part 2 (00-09-36).csv, 3'03",3'45",183,225,$ Why P(Wi|d) represent TF weighting? How does this probability add a limit/bound to the score of a certain word? In week3, one of the TF weighting was log(x+1), so why TF here is P(Wi|d) instead of log(P(Wi|d)) ?$
week4_allquestions,4.5,4 - 5 - 3.4 Smoothing of Language Model - Part 2 (00-09-36).csv, 4'43" , 4'50",283,290,$ I don't agree with the statement that "if the document is long, we need less smoothing". What if a docment is long but in a bad way, meaning that it has many repeated words (less unique words)? In this case, the probability of being able to observe all/more words won't increase.$
week4_allquestions,4.6,4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).csv, 5'22",5'36",322,336, Why do we say that the smoothing variable 'mu' is dynamic in this case? When exactly is the variable changing and why? How does it behave differently than 'lambda'?
week4_allquestions,4.4,4 - 4 - 3.4 Smoothing of Language Model - Part 1 (00-12-15).csv, 2'30",3'30",150,210,Why taking away the probabilitiy mass from observered words help us assign probability to words not seen in the document?
week4_allquestions,4.2,4 - 2 - 3.2 Statistical Language Models (00-17-53).csv, 5'33",5'35",333,335, When is Unigram LM used
week4_allquestions,4.1,4 - 1 - 3.1 Probabilistic Retrieval Model- Basic Idea (00-12-44).csv, 8'35",11'30",515,690,$ What is the conditional probalility mean? Does it mean given the document guess the query which returns this document? And then we compute the probability of guessed query and the given document? What does it mean to say a user likes a document, if the query is unknown? (How do the user click the document if he didn't enter a query?)$
week4_allquestions,4.2,4 - 2 - 3.2 Statistical Language Models (00-17-53).csv, 4'41",4'57",281,297, Why do we have N-1 parameters? Don't we sum up the probabilities from w1 to wN?
week4_allquestions,4.3,4 - 3 - 3.3 Query Likelihood Retrieval Function (00-12-07).csv, 2'04",4'08",124,248,$ Since not every query can be drawn from the documents, does that mean that query generation and query likelihood eventually become better (improving doc model) as query inputs increase?$
week4_allquestions,4.3,4 - 3 - 3.3 Query Likelihood Retrieval Function (00-12-07).csv, 4'30",4'50",270,290, When doing query likelihood are there cases were the user writes the query using words that depend on previous words?
week4_allquestions,4.1,4 - 1 - 3.1 Probabilistic Retrieval Model- Basic Idea (00-12-44).csv, 9'15",9'25",555,565, What is the assumption we make while calculating the query likelihood.
week4_allquestions,4.7,4 - 7 - 3.5 Smoothing Methods Part - 2 (00-13-17).csv, 5'10",8'47",310,527,$ Poor explanation of how to derive the pseen/alpha varible, need mathematical explanation.$
week4_allquestions,4.6,4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).csv, 5'46",6'22",346,382, Does the smaller coefficient for longer documents i.e. lesser smoothing make it harder for models to come up with more accurate retrieval through probabilistic techniques?
week4_allquestions,4.7,4 - 7 - 3.5 Smoothing Methods Part - 2 (00-13-17).csv, 6'00",9'45",360,585,$ I am not quite sure whether increasing mu would increase the overall likelihood of a certain word, or reduce it. It seems that mu occurs in both the denominator and the numerator of the entire equation.$
week4_allquestions,4.3,4 - 3 - 3.3 Query Likelihood Retrieval Function (00-12-07).csv, 7'00" , 9'00",420,540, Can you go into more detail as to how the query likelihood retrieval function works?
week4_allquestions,4.1,4 - 1 - 3.1 Probabilistic Retrieval Model- Basic Idea (00-12-44).csv, 4'40",4'45",280,285,$ I don't get the notation used in the model. I understand R is the contraint, but what are d and q?$
week4_allquestions,4.3,4 - 3 - 3.3 Query Likelihood Retrieval Function (00-12-07).csv, 2'23",3'26",143,206, Why would we assume each word in the query is chosen independently when users often chose entire phrases for a query at once?
week4_allquestions,4.4,4 - 4 - 3.4 Smoothing of Language Model - Part 1 (00-12-15).csv, 10'30",11'30",630,690, What's the meaning of rewriting the ranking function with smoothing?
week4_allquestions,5.3,MISSING, 16’18’’,17’21’’,978,1041,$ Why is lamda higher makes the common words disappear? According to the formula, higher lamda results more involvement of background model. Wouldn’t this favors the occurrence of more common words?$
week4_allquestions,4.1,4 - 1 - 3.1 Probabilistic Retrieval Model- Basic Idea (00-12-44).csv, 9'04",10'10",544,610, Does this assumption still hold in production? It seems like we have to use this assumption to calculate the probability.
week4_allquestions,4.1,4 - 1 - 3.1 Probabilistic Retrieval Model- Basic Idea (00-12-44).csv, 08'20",10'24",500,624, Is it possible to glean meaningful information about for probabilistic retrieval models from something other than clickthrough data?
week4_allquestions,4.2,4 - 2 - 3.2 Statistical Language Models (00-17-53).csv, 11'30",14'10",690,850, How does one choose which language model to use for a specific set of text? Are some LMs better for some types of texts than others?
week4_allquestions,4.7,4 - 7 - 3.5 Smoothing Methods Part - 2 (00-13-17).csv, 1'40",1'47",100,107, How does lambda affect p(w|d)?
week4_allquestions,4.1,4 - 1 - 3.1 Probabilistic Retrieval Model- Basic Idea (00-12-44).csv, 7'51",8'20",471,500, Are there any shortcomings of our assumption that a user formulates a query based on an imaginary relevant document?
week4_allquestions,4.2,4 - 2 - 3.2 Statistical Language Models (00-17-53).csv, 00'44",02'28",44,148,$ Since context is what somehow gives meaning to a word, and helps with ambiguity.How do we consider the meaning of words with respect to their context? I might be understading wrong there is not way to deal with ambiguity so far.$
week4_allquestions,4.6,4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).csv, 3'20",4'00",200,240, Can you please explain how you get the network and mining values using the equation?
week4_allquestions,4.1,4 - 1 - 3.1 Probabilistic Retrieval Model- Basic Idea (00-12-44).csv, 1'19",1'35",79,95, why you said that "The classic probabilistic model has led to the BM25 retrieval function"? What is the probabilistic interpretation of BM25?
week4_allquestions,4.4,4 - 4 - 3.4 Smoothing of Language Model - Part 1 (00-12-15).csv, 6'11 , 6'30,371,390, how to smooth a LM?
week4_allquestions,4.7,4 - 7 - 3.5 Smoothing Methods Part - 2 (00-13-17).csv, 8'32 , 8'50,512,530, why to deduce the function of Fdir?
week4_allquestions,4.6,4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).csv, 6'38",8'30",398,510, How do we choose the coefficient of pseudo-counts? On the slides it just says it should be positive.
week4_allquestions,4.3,4 - 3 - 3.3 Query Likelihood Retrieval Function (00-12-07).csv, 0'10",1'00,10,60, Why do we need to predict the likelihood of the query? Or what's the usage for that?
week4_allquestions,4.7,4 - 7 - 3.5 Smoothing Methods Part - 2 (00-13-17).csv, 11'13'',11'25'',673,685, What is the benefit of a less heristic retrieval function?
week4_allquestions,4.3,4 - 3 - 3.3 Query Likelihood Retrieval Function (00-12-07).csv, 7'50'',7'55'',470,475,$ For the improved query likelihood with language model, do we apply background model to all query likelihood or should we only apply specific topic model? If we apply topic model for each document, how can we compare the query likelihood if the users actually have a cross-topic document in mind?$
week4_allquestions,4.4,4 - 4 - 3.4 Smoothing of Language Model - Part 1 (00-12-15).csv, 7'33",8'40",453,520,$ What's the function of alpha here, and how to choose its value?$
